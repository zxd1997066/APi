W0916 01:37:54.441000 1536538 site-packages/torch/distributed/run.py:814] 
W0916 01:37:54.441000 1536538 site-packages/torch/distributed/run.py:814] *****************************************
W0916 01:37:54.441000 1536538 site-packages/torch/distributed/run.py:814] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 01:37:54.441000 1536538 site-packages/torch/distributed/run.py:814] *****************************************
Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_8B/dora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_8B/dora/logs
model:
  _component_: torchtune.models.llama3.lora_llama3_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_rank: 8
  use_dora: true
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_8B/dora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_8B/dora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 256
  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model

Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_8B/dora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_8B/dora/logs
model:
  _component_: torchtune.models.llama3.lora_llama3_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_rank: 8
  use_dora: true
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_8B/dora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_8B/dora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 256
  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model

[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 11 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 11

Writing logs to /tmp/torchtune/llama3_8B/dora/logs/log_1757986678.txt
FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...
2025:09:16-01:37:59:1536612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:16-01:37:59:1536612 |CCL_WARN| value of CCL_RECV changed to be direct (default:)
2025:09:16-01:37:59:1536612 |CCL_WARN| value of CCL_SEND changed to be direct (default:)
2025:09:16-01:37:59:1536612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:16-01:37:59:1536613 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:16-01:37:59:1536613 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[rank1]:W0916 01:37:59.942000 1536613 site-packages/torch/distributed/tensor/_random.py:208] DTensor is synchronizing RNG states of every rank with the state from rank 0. This behavior is deprecated. Please call `torch.manual_seed()` on every rank that participates in SPMD DTensor Operations with the same seed. If using Pipeline Parallelism, each pipeling state would use a different seed, but all ranks belonging to one pipeline stage would use the same seed.
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py:535: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Instantiating model and loading checkpoint took 12.28 secs
Memory stats after model init:
	XPU peak memory active: 8.55 GiB
	XPU peak memory alloc: 8.55 GiB
	XPU peak memory reserved: 8.67 GiB
Optimizer is initialized.
Loss is initialized.
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   1%|          | 319/51760 [00:00<00:16, 3177.13it/s]Packing dataset:   1%|          | 637/51760 [00:00<00:16, 3171.46it/s]Packing dataset:   2%|▏         | 955/51760 [00:00<00:16, 3113.66it/s]Packing dataset:   2%|▏         | 1281/51760 [00:00<00:15, 3167.01it/s]Packing dataset:   3%|▎         | 1598/51760 [00:00<00:15, 3136.66it/s]Packing dataset:   4%|▎         | 1912/51760 [00:00<00:16, 3053.04it/s]Packing dataset:   4%|▍         | 2233/51760 [00:00<00:15, 3102.75it/s]Packing dataset:   5%|▍         | 2556/51760 [00:00<00:15, 3142.38it/s]Packing dataset:   6%|▌         | 2887/51760 [00:00<00:15, 3190.69it/s]Packing dataset:   6%|▌         | 3211/51760 [00:01<00:15, 3205.61it/s]Packing dataset:   7%|▋         | 3532/51760 [00:01<00:15, 3163.74it/s]Packing dataset:   7%|▋         | 3849/51760 [00:01<00:15, 3160.34it/s]Packing dataset:   8%|▊         | 4166/51760 [00:01<00:15, 3061.97it/s]Packing dataset:   9%|▊         | 4483/51760 [00:01<00:15, 3093.01it/s]Packing dataset:   9%|▉         | 4795/51760 [00:01<00:15, 3099.14it/s]Packing dataset:  10%|▉         | 5110/51760 [00:01<00:14, 3112.96it/s]Packing dataset:  10%|█         | 5422/51760 [00:01<00:14, 3103.67it/s]Packing dataset:  11%|█         | 5754/51760 [00:01<00:14, 3166.96it/s]Packing dataset:  12%|█▏        | 6071/51760 [00:01<00:14, 3117.63it/s]Packing dataset:  12%|█▏        | 6393/51760 [00:02<00:14, 3143.96it/s]Packing dataset:  13%|█▎        | 6708/51760 [00:02<00:14, 3074.61it/s]Packing dataset:  14%|█▎        | 7019/51760 [00:02<00:14, 3083.62it/s]Packing dataset:  14%|█▍        | 7328/51760 [00:02<00:14, 3079.95it/s]Packing dataset:  15%|█▍        | 7650/51760 [00:02<00:14, 3120.51it/s]Packing dataset:  15%|█▌        | 7963/51760 [00:02<00:14, 3104.78it/s]Packing dataset:  16%|█▌        | 8274/51760 [00:02<00:14, 3060.71it/s]Packing dataset:  17%|█▋        | 8587/51760 [00:02<00:14, 3080.60it/s]Packing dataset:  17%|█▋        | 8911/51760 [00:02<00:13, 3124.70it/s]Packing dataset:  18%|█▊        | 9224/51760 [00:02<00:13, 3115.78it/s]Packing dataset:  18%|█▊        | 9536/51760 [00:03<00:13, 3100.84it/s]Packing dataset:  19%|█▉        | 9847/51760 [00:03<00:13, 3058.28it/s]Packing dataset:  20%|█▉        | 10163/51760 [00:03<00:13, 3088.00it/s]Packing dataset:  20%|██        | 10473/51760 [00:03<00:13, 3087.73it/s]Packing dataset:  21%|██        | 10795/51760 [00:03<00:13, 3126.98it/s]Packing dataset:  21%|██▏       | 11108/51760 [00:03<00:13, 3106.55it/s]Packing dataset:  22%|██▏       | 11419/51760 [00:03<00:13, 3086.38it/s]Packing dataset:  23%|██▎       | 11728/51760 [00:03<00:13, 3069.74it/s]Packing dataset:  23%|██▎       | 12036/51760 [00:03<00:12, 3067.90it/s]Packing dataset:  24%|██▍       | 12343/51760 [00:03<00:12, 3065.30it/s]Packing dataset:  24%|██▍       | 12652/51760 [00:04<00:12, 3069.83it/s]Packing dataset:  25%|██▌       | 12960/51760 [00:04<00:12, 3056.41it/s]Packing dataset:  26%|██▌       | 13271/51760 [00:04<00:12, 3069.90it/s]Packing dataset:  26%|██▌       | 13579/51760 [00:04<00:12, 3065.32it/s]Packing dataset:  27%|██▋       | 13886/51760 [00:04<00:12, 3064.97it/s]Packing dataset:  27%|██▋       | 14193/51760 [00:04<00:12, 3041.24it/s]Packing dataset:  28%|██▊       | 14514/51760 [00:04<00:12, 3090.32it/s]Packing dataset:  29%|██▊       | 14824/51760 [00:04<00:11, 3085.06it/s]Packing dataset:  29%|██▉       | 15133/51760 [00:04<00:11, 3082.79it/s]Packing dataset:  30%|██▉       | 15447/51760 [00:04<00:11, 3099.10it/s]Packing dataset:  30%|███       | 15757/51760 [00:05<00:11, 3095.37it/s]Packing dataset:  31%|███       | 16080/51760 [00:05<00:11, 3133.32it/s]Packing dataset:  32%|███▏      | 16394/51760 [00:05<00:11, 3070.93it/s]Packing dataset:  32%|███▏      | 16705/51760 [00:05<00:11, 3079.03it/s]Packing dataset:  33%|███▎      | 17014/51760 [00:05<00:11, 3051.98it/s]Packing dataset:  33%|███▎      | 17320/51760 [00:05<00:11, 3038.19it/s]Packing dataset:  34%|███▍      | 17624/51760 [00:05<00:11, 3036.66it/s]Packing dataset:  35%|███▍      | 17928/51760 [00:05<00:11, 3008.50it/s]Packing dataset:  35%|███▌      | 18229/51760 [00:05<00:11, 2996.42it/s]Packing dataset:  36%|███▌      | 18529/51760 [00:06<00:11, 2953.82it/s]Packing dataset:  36%|███▋      | 18847/51760 [00:06<00:10, 3019.74it/s]Packing dataset:  37%|███▋      | 19153/51760 [00:06<00:10, 3030.31it/s]Packing dataset:  38%|███▊      | 19457/51760 [00:06<00:10, 3009.17it/s]Packing dataset:  38%|███▊      | 19759/51760 [00:06<00:10, 2986.47it/s]Packing dataset:  39%|███▉      | 20065/51760 [00:06<00:10, 3005.74it/s]Packing dataset:  39%|███▉      | 20372/51760 [00:06<00:10, 3022.81it/s]Packing dataset:  40%|███▉      | 20675/51760 [00:06<00:10, 3008.67it/s]Packing dataset:  41%|████      | 20994/51760 [00:06<00:10, 3062.24it/s]Packing dataset:  41%|████      | 21301/51760 [00:06<00:09, 3063.73it/s]Packing dataset:  42%|████▏     | 21608/51760 [00:07<00:09, 3057.38it/s]Packing dataset:  42%|████▏     | 21914/51760 [00:07<00:09, 3055.40it/s]Packing dataset:  43%|████▎     | 22220/51760 [00:07<00:09, 3015.70it/s]Packing dataset:  44%|████▎     | 22534/51760 [00:07<00:09, 3047.85it/s]Packing dataset:  44%|████▍     | 22839/51760 [00:07<00:15, 1847.20it/s]Packing dataset:  45%|████▍     | 23137/51760 [00:07<00:13, 2078.63it/s]Packing dataset:  45%|████▌     | 23441/51760 [00:07<00:12, 2295.52it/s]Packing dataset:  46%|████▌     | 23748/51760 [00:07<00:11, 2484.17it/s]Packing dataset:  46%|████▋     | 24048/51760 [00:08<00:10, 2617.05it/s]Packing dataset:  47%|████▋     | 24356/51760 [00:08<00:09, 2740.52it/s]Packing dataset:  48%|████▊     | 24672/51760 [00:08<00:09, 2854.82it/s]Packing dataset:  48%|████▊     | 24973/51760 [00:08<00:09, 2879.89it/s]Packing dataset:  49%|████▉     | 25273/51760 [00:08<00:09, 2911.78it/s]Packing dataset:  49%|████▉     | 25575/51760 [00:08<00:08, 2942.31it/s]Packing dataset:  50%|████▉     | 25875/51760 [00:08<00:08, 2933.61it/s]Packing dataset:  51%|█████     | 26190/51760 [00:08<00:08, 2995.09it/s]Packing dataset:  51%|█████     | 26493/51760 [00:08<00:08, 2984.73it/s]Packing dataset:  52%|█████▏    | 26794/51760 [00:08<00:08, 2951.57it/s]Packing dataset:  52%|█████▏    | 27117/51760 [00:09<00:08, 3031.23it/s]Packing dataset:  53%|█████▎    | 27422/51760 [00:09<00:08, 3025.93it/s]Packing dataset:  54%|█████▎    | 27726/51760 [00:09<00:08, 3002.68it/s]Packing dataset:  54%|█████▍    | 28027/51760 [00:09<00:08, 2962.89it/s]Packing dataset:  55%|█████▍    | 28324/51760 [00:09<00:07, 2954.68it/s]Packing dataset:  55%|█████▌    | 28620/51760 [00:09<00:07, 2933.42it/s]Packing dataset:  56%|█████▌    | 28921/51760 [00:09<00:07, 2954.53it/s]Packing dataset:  56%|█████▋    | 29217/51760 [00:09<00:07, 2946.79it/s]Packing dataset:  57%|█████▋    | 29520/51760 [00:09<00:07, 2968.56it/s]Packing dataset:  58%|█████▊    | 29817/51760 [00:09<00:07, 2926.44it/s]Packing dataset:  58%|█████▊    | 30110/51760 [00:10<00:07, 2908.01it/s]Packing dataset:  59%|█████▊    | 30406/51760 [00:10<00:07, 2922.82it/s]Packing dataset:  59%|█████▉    | 30701/51760 [00:10<00:07, 2930.42it/s]Packing dataset:  60%|█████▉    | 30995/51760 [00:10<00:07, 2927.82it/s]Packing dataset:  60%|██████    | 31293/51760 [00:10<00:06, 2942.30it/s]Packing dataset:  61%|██████    | 31607/51760 [00:10<00:06, 2998.45it/s]Packing dataset:  62%|██████▏   | 31920/51760 [00:10<00:06, 3036.34it/s]Packing dataset:  62%|██████▏   | 32224/51760 [00:10<00:06, 2983.98it/s]Packing dataset:  63%|██████▎   | 32530/51760 [00:10<00:06, 3004.92it/s]Packing dataset:  63%|██████▎   | 32832/51760 [00:10<00:06, 3009.06it/s]Packing dataset:  64%|██████▍   | 33134/51760 [00:11<00:06, 3012.27it/s]Packing dataset:  65%|██████▍   | 33449/51760 [00:11<00:05, 3052.06it/s]Packing dataset:  65%|██████▌   | 33755/51760 [00:11<00:05, 3041.69it/s]Packing dataset:  66%|██████▌   | 34060/51760 [00:11<00:05, 3033.64it/s]Packing dataset:  66%|██████▋   | 34364/51760 [00:11<00:05, 3008.46it/s]Packing dataset:  67%|██████▋   | 34671/51760 [00:11<00:05, 3026.71it/s]Packing dataset:  68%|██████▊   | 34974/51760 [00:11<00:05, 3024.70it/s]Packing dataset:  68%|██████▊   | 35284/51760 [00:11<00:05, 3045.36it/s]Packing dataset:  69%|██████▉   | 35590/51760 [00:11<00:05, 3048.88it/s]Packing dataset:  69%|██████▉   | 35895/51760 [00:11<00:05, 2986.73it/s]Packing dataset:  70%|██████▉   | 36194/51760 [00:12<00:05, 2932.75it/s]Packing dataset:  71%|███████   | 36508/51760 [00:12<00:05, 2988.67it/s]Packing dataset:  71%|███████   | 36820/51760 [00:12<00:04, 3026.74it/s]Packing dataset:  72%|███████▏  | 37124/51760 [00:12<00:04, 3030.33it/s]Packing dataset:  72%|███████▏  | 37428/51760 [00:12<00:04, 3022.46it/s]Packing dataset:  73%|███████▎  | 37731/51760 [00:12<00:04, 3018.21it/s]Packing dataset:  73%|███████▎  | 38037/51760 [00:12<00:04, 3028.40it/s]Packing dataset:  74%|███████▍  | 38343/51760 [00:12<00:04, 3035.34it/s]Packing dataset:  75%|███████▍  | 38647/51760 [00:12<00:04, 3002.55it/s]Packing dataset:  75%|███████▌  | 38948/51760 [00:13<00:04, 2990.93it/s]Packing dataset:  76%|███████▌  | 39267/51760 [00:13<00:04, 3047.04it/s]Packing dataset:  76%|███████▋  | 39572/51760 [00:13<00:04, 3016.28it/s]Packing dataset:  77%|███████▋  | 39880/51760 [00:13<00:03, 3033.38it/s]Packing dataset:  78%|███████▊  | 40184/51760 [00:13<00:03, 3009.97it/s]Packing dataset:  78%|███████▊  | 40493/51760 [00:13<00:03, 3032.24it/s]Packing dataset:  79%|███████▉  | 40803/51760 [00:13<00:03, 3052.36it/s]Packing dataset:  79%|███████▉  | 41110/51760 [00:13<00:03, 3057.57it/s]Packing dataset:  80%|████████  | 41416/51760 [00:13<00:03, 3038.16it/s]Packing dataset:  81%|████████  | 41720/51760 [00:13<00:03, 3026.70it/s]Packing dataset:  81%|████████  | 42023/51760 [00:14<00:03, 2989.93it/s]Packing dataset:  82%|████████▏ | 42325/51760 [00:14<00:03, 2997.47it/s]Packing dataset:  82%|████████▏ | 42625/51760 [00:14<00:03, 2976.52it/s]Packing dataset:  83%|████████▎ | 42924/51760 [00:14<00:02, 2980.42it/s]Packing dataset:  84%|████████▎ | 43223/51760 [00:14<00:02, 2942.16it/s]Packing dataset:  84%|████████▍ | 43530/51760 [00:14<00:02, 2976.93it/s]Packing dataset:  85%|████████▍ | 43832/51760 [00:14<00:02, 2988.99it/s]Packing dataset:  85%|████████▌ | 44132/51760 [00:14<00:02, 2965.18it/s]Packing dataset:  86%|████████▌ | 44444/51760 [00:14<00:02, 3010.16it/s]Packing dataset:  86%|████████▋ | 44750/51760 [00:14<00:02, 3021.52it/s]Packing dataset:  87%|████████▋ | 45053/51760 [00:15<00:02, 3003.94it/s]Packing dataset:  88%|████████▊ | 45367/51760 [00:15<00:02, 3041.44it/s]Packing dataset:  88%|████████▊ | 45672/51760 [00:15<00:02, 3030.64it/s]Packing dataset:  89%|████████▉ | 45976/51760 [00:15<00:01, 2979.88it/s]Packing dataset:  89%|████████▉ | 46283/51760 [00:15<00:01, 3002.02it/s]Packing dataset:  90%|█████████ | 46584/51760 [00:15<00:01, 2991.88it/s]Packing dataset:  91%|█████████ | 46895/51760 [00:15<00:01, 3025.71it/s]Packing dataset:  91%|█████████ | 47198/51760 [00:15<00:01, 2993.10it/s]Packing dataset:  92%|█████████▏| 47503/51760 [00:15<00:01, 3005.60it/s]Packing dataset:  92%|█████████▏| 47804/51760 [00:15<00:01, 3001.33it/s]Packing dataset:  93%|█████████▎| 48105/51760 [00:16<00:01, 2963.36it/s]Packing dataset:  94%|█████████▎| 48409/51760 [00:16<00:01, 2985.64it/s]Packing dataset:  94%|█████████▍| 48720/51760 [00:16<00:01, 3019.44it/s]Packing dataset:  95%|█████████▍| 49023/51760 [00:16<00:00, 2961.18it/s]Packing dataset:  95%|█████████▌| 49324/51760 [00:16<00:00, 2975.16it/s]Packing dataset:  96%|█████████▌| 49627/51760 [00:16<00:00, 2991.11it/s]Packing dataset:  96%|█████████▋| 49927/51760 [00:16<00:00, 2987.18it/s]Packing dataset:  97%|█████████▋| 50226/51760 [00:16<00:00, 2974.85it/s]Packing dataset:  98%|█████████▊| 50526/51760 [00:16<00:00, 2980.40it/s]Packing dataset:  98%|█████████▊| 50836/51760 [00:16<00:00, 3014.38it/s]Packing dataset:  99%|█████████▉| 51139/51760 [00:17<00:00, 3016.80it/s]Packing dataset:  99%|█████████▉| 51441/51760 [00:17<00:00, 3011.74it/s]Packing dataset: 100%|█████████▉| 51750/51760 [00:17<00:00, 3034.25it/s]Packing dataset: 100%|██████████| 51760/51760 [00:17<00:00, 2997.11it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py:724: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, self.world_size / num_tokens)
 10%|█         | 1/10 [00:17<02:33, 17.05s/it]1|1|Loss: 2.1070616245269775:  10%|█         | 1/10 [00:17<02:33, 17.05s/it]iteration:  1 tokens:  755 time:  17.05356872396078 tokens_per_second:  44.27
1|1|Loss: 2.1070616245269775:  20%|██        | 2/10 [00:18<01:03,  7.94s/it]1|2|Loss: 2.382875442504883:  20%|██        | 2/10 [00:18<01:03,  7.94s/it] iteration:  2 tokens:  770 time:  1.5474120610160753 tokens_per_second:  497.61
1|2|Loss: 2.382875442504883:  30%|███       | 3/10 [00:20<00:34,  4.99s/it]1|3|Loss: 2.4784016609191895:  30%|███       | 3/10 [00:20<00:34,  4.99s/it]iteration:  3 tokens:  743 time:  1.4760169599903747 tokens_per_second:  503.38
1|3|Loss: 2.4784016609191895:  40%|████      | 4/10 [00:21<00:21,  3.60s/it]1|4|Loss: 2.7015202045440674:  40%|████      | 4/10 [00:21<00:21,  3.60s/it]iteration:  4 tokens:  733 time:  1.463137841958087 tokens_per_second:  500.98
1|4|Loss: 2.7015202045440674:  50%|█████     | 5/10 [00:23<00:14,  2.83s/it]1|5|Loss: 2.4363083839416504:  50%|█████     | 5/10 [00:23<00:14,  2.83s/it]iteration:  5 tokens:  836 time:  1.4607263899524696 tokens_per_second:  572.32
1|5|Loss: 2.4363083839416504:  60%|██████    | 6/10 [00:24<00:09,  2.37s/it]1|6|Loss: 2.708160638809204:  60%|██████    | 6/10 [00:24<00:09,  2.37s/it] iteration:  6 tokens:  786 time:  1.4723250480019487 tokens_per_second:  533.85
1|6|Loss: 2.708160638809204:  70%|███████   | 7/10 [00:25<00:06,  2.08s/it]1|7|Loss: 1.8790479898452759:  70%|███████   | 7/10 [00:25<00:06,  2.08s/it]iteration:  7 tokens:  1016 time:  1.4760246189543977 tokens_per_second:  688.34
1|7|Loss: 1.8790479898452759:  80%|████████  | 8/10 [00:27<00:03,  1.88s/it]1|8|Loss: 2.186150312423706:  80%|████████  | 8/10 [00:27<00:03,  1.88s/it] iteration:  8 tokens:  909 time:  1.4763255040161312 tokens_per_second:  615.72
1|8|Loss: 2.186150312423706:  90%|█████████ | 9/10 [00:28<00:01,  1.75s/it]1|9|Loss: 2.267975330352783:  90%|█████████ | 9/10 [00:28<00:01,  1.75s/it]iteration:  9 tokens:  749 time:  1.468386197986547 tokens_per_second:  510.08
1|9|Loss: 2.267975330352783: 100%|██████████| 10/10 [00:30<00:00,  1.66s/it]1|10|Loss: 2.3271751403808594: 100%|██████████| 10/10 [00:30<00:00,  1.66s/it]iteration:  10 tokens:  785 time:  1.4584215079667047 tokens_per_second:  538.25
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 912, in <module>
[rank1]:     sys.exit(recipe_main())
[rank1]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank1]:     sys.exit(recipe_main(conf))
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 907, in recipe_main
[rank1]:     recipe.train()
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 808, in train
[rank1]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank1]: ZeroDivisionError: division by zero
avg tokens_per_second:  557.98
1|10|Loss: 2.3271751403808594: 100%|██████████| 10/10 [00:30<00:00,  3.04s/it]
[rank0]:[W916 01:39:00.941048807 ProcessGroup.hpp:941] Warning: No backend of type 0 found for Process Group with name undefined. Assuming no hooks are registered. (function hasHooks)
[rank1]:[W916 01:39:01.814100299 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0916 01:39:01.978000 1536538 site-packages/torch/distributed/elastic/multiprocessing/api.py:932] Sending process 1536612 closing signal SIGTERM
E0916 01:39:02.092000 1536538 site-packages/torch/distributed/elastic/multiprocessing/api.py:906] failed (exitcode: 1) local_rank: 1 (pid: 1536613) of binary: /home/jenkins/.conda/envs/xpu_op_/bin/python3.10
Running with torchrun...
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/bin/tune", line 7, in <module>
    sys.exit(main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 212, in _run_cmd
    self._run_distributed(args, is_builtin=is_builtin)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 101, in _run_distributed
    run(args)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/run.py", line 939, in run
    elastic_launch(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_01:39:01
  host      : dut7358
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1536613)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W916 01:39:02.804061051 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
