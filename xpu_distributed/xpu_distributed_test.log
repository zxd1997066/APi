         GPU 0/0  GPU 1/0  GPU 2/0  GPU 3/0  GPU 4/0  GPU 5/0  GPU 6/0  GPU 7/0  CPU Affinity
GPU 0/0  S        XL8      XL8      XL8      SYS      SYS      SYS      SYS      0-47,96-143
GPU 1/0  XL8      S        XL8      XL8      SYS      SYS      SYS      SYS      0-47,96-143
GPU 2/0  XL8      XL8      S        XL8      SYS      SYS      SYS      SYS      0-47,96-143
GPU 3/0  XL8      XL8      XL8      S        SYS      SYS      SYS      SYS      0-47,96-143
GPU 4/0  SYS      SYS      SYS      SYS      S        XL8      XL8      XL8      48-95,144-191
GPU 5/0  SYS      SYS      SYS      SYS      XL8      S        XL8      XL8      48-95,144-191
GPU 6/0  SYS      SYS      SYS      SYS      XL8      XL8      S        XL8      48-95,144-191
GPU 7/0  SYS      SYS      SYS      SYS      XL8      XL8      XL8      S        48-95,144-191
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 07:52:30.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_apply_activation_checkpointing PASSED [0.2283s] [ 12%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_checkpoint_wrapper_args_kwargs PASSED [0.0012s] [ 25%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_checkpoint_wrapper_cpu_offload PASSED [0.0708s] [ 37%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_checkpoint_wrapper_kwarg_support PASSED [0.0038s] [ 50%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_checkpoint_wrapper_parity SKIPPED [0.0002s] [ 62%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_forward_missing_attributes PASSED [0.0008s] [ 75%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_fqn PASSED [0.0005s] [ 87%]
../../../../test/distributed/fsdp/test_checkpoint_wrapper.py::CheckpointWrapperTest::test_load_activation_checkpointed_module PASSED [0.0012s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_checkpoint_wrapper.py.xml -
========================= 7 passed, 1 skipped in 2.17s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 07:52:34.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_distributed_checkpoint.py::TestDistributedCheckpointXPU::test_distributed_checkpoint_state_dict_type0_xpu [2025-09-19 07:52:36.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:52:36.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:52:36:1774154 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:52:36:1774154 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:52:36:1774155 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:52:36:1774155 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
Using temp directory: /tmp/tmp4vkxmqjb
dist init r=1, world=2
PASSED [15.5289s] [ 50%]
../../../../test/distributed/fsdp/test_distributed_checkpoint.py::TestDistributedCheckpointXPU::test_distributed_checkpoint_state_dict_type1_xpu [2025-09-19 07:52:51.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:52:51.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:52:52:1774310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:52:52:1774310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:52:52:1774311 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:52:52:1774311 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
Using temp directory: /tmp/tmpc8pwn4cx
dist init r=1, world=2
PASSED [15.5285s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_distributed_checkpoint.py.xml -
============================== 2 passed in 33.14s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 07:53:08.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/fsdp/test_fsdp_apply.py::TestApplyXPU::test_apply_in_summon_raises_error_xpu [2025-09-19 07:53:10.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:53:10.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:53:10:1774537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:10:1774537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:53:10:1774536 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:10:1774536 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
Asserting FSDP instance is: FullyShardedDataParallel(
  (_fsdp_wrapped_module): TransformerWithSharedParams(
    (embed_tokens): Embedding(23, 16)
    (transformer): Transformer(
      (encoder): TransformerEncoder(
        (layers): ModuleList(
          (0-1): 2 x FullyShardedDataParallel(
            (_fsdp_wrapped_module): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
              )
              (linear1): Linear(in_features=16, out_features=8, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=16, bias=True)
              (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-1): 2 x FullyShardedDataParallel(
            (_fsdp_wrapped_module): TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
              )
              (linear1): Linear(in_features=16, out_features=8, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=8, out_features=16, bias=True)
              (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_proj): Linear(in_features=16, out_features=23, bias=True)
    (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
ERROR: expected to be in states [<TrainingState.IDLE: 1>] but current state is TrainingState.SUMMON_FULL_PARAMS
PASSED [15.0269s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_apply.py::TestApplyXPU::test_nested_module_apply_xpu [2025-09-19 07:53:25.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:53:25.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:53:25:1774688 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:25:1774688 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:53:25:1774687 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:25:1774687 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.6265s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_apply.py::TestApplyXPU::test_transformer_module_apply_xpu [2025-09-19 07:53:41.012] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:53:41.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:53:41:1774839 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:41:1774839 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:53:41:1774838 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:41:1774838 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.6265s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_apply.py.xml -
============================== 3 passed in 48.33s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 07:53:57.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/fsdp/test_fsdp_backward_prefetch.py::TestBackwardPrefetch::test_backward_prefetch [2025-09-19 07:53:59.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:53:59.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:53:59:1775064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:59:1775064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:53:59:1775065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:53:59:1775065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [36.8276s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_backward_prefetch.py.xml -
============================== 1 passed in 38.70s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 07:54:37.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 17 items
Running 17 items in this shard

../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload0_offload_activations_False_use_orig_params_False [2025-09-19 07:54:39.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:54:39.301] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:54:39.321] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:54:39.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:54:39:1775300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:54:39:1775300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:54:39:1775299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:54:39:1775299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:54:39:1775302 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:54:39:1775302 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:54:39:1775301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:54:39:1775301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.5546s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload0_offload_activations_False_use_orig_params_True [2025-09-19 07:55:10.881] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:10.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:10.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:10.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:55:11:1775622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:11:1775622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:11:1775621 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:11:1775621 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:11:1775623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:11:1775623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:11:1775620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:11:1775620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.3554s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload0_offload_activations_True_use_orig_params_False [2025-09-19 07:55:42.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:42.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:42.257] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:55:42.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:55:42:1775943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:42:1775943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:42:1775941 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:42:1775941 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:42:1775942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:42:1775942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:55:42:1775940 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:55:42:1775940 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.3550s] [ 17%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload0_offload_activations_True_use_orig_params_True [2025-09-19 07:56:13.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:13.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:13.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:13.584] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:56:13:1776264 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:13:1776264 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:13:1776262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:13:1776262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:13:1776263 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:13:1776263 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:14:1776261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:14:1776261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.3550s] [ 23%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload1_offload_activations_False_use_orig_params_False [2025-09-19 07:56:44.909] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:44.916] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:44.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:56:44.935] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:56:45:1776580 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:45:1776580 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:45:1776582 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:45:1776582 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:45:1776583 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:45:1776583 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:56:45:1776581 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:56:45:1776581 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.2550s] [ 29%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload1_offload_activations_False_use_orig_params_True [2025-09-19 07:57:16.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:16.217] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:16.217] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:16.217] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:57:16:1776898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:16:1776898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:16:1776901 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:16:1776901 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:16:1776899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:16:1776899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:16:1776900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:16:1776900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.4597s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload1_offload_activations_True_use_orig_params_False [2025-09-19 07:57:47.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:47.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:47.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:57:47.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:57:47:1777220 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:47:1777220 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:47:1777222 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:47:1777222 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:48:1777221 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:48:1777221 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:57:48:1777219 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:57:48:1777219 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.1524s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_basic_checkpoint_end_to_end_cpu_offload1_offload_activations_True_use_orig_params_True [2025-09-19 07:58:18.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:18.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:18.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:18.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:58:19:1777544 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:19:1777544 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:19:1777543 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:19:1777543 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:19:1777541 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:19:1777541 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:19:1777542 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:19:1777542 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.2549s] [ 47%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload0_offload_activations_False_use_orig_params_False [2025-09-19 07:58:50.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:50.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:50.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:58:50.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:58:50:1777860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:50:1777860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:50:1777861 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:50:1777861 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:50:1777863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:50:1777863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:58:50:1777862 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:58:50:1777862 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.3552s] [ 52%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload0_offload_activations_False_use_orig_params_True [2025-09-19 07:59:21.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:21.424] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:21.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:21.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:59:21:1778180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:21:1778180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:21:1778177 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:21:1778177 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:21:1778179 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:21:1778179 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:21:1778178 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:21:1778178 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [30.9524s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload0_offload_activations_True_use_orig_params_False [2025-09-19 07:59:52.356] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:52.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:52.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 07:59:52.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-07:59:52:1778499 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:52:1778499 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:52:1778498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:52:1778498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:52:1778497 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:52:1778497 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-07:59:52:1778496 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-07:59:52:1778496 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.2551s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload0_offload_activations_True_use_orig_params_True [2025-09-19 08:00:23.607] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:23.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:23.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:23.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:00:23:1778816 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:23:1778816 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:23:1778815 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:23:1778815 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:23:1778817 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:23:1778817 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:24:1778818 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:24:1778818 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.6554s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload1_offload_activations_False_use_orig_params_False [2025-09-19 08:00:55.325] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:55.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:55.380] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:00:55.381] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:00:55:1779137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:55:1779137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:55:1779136 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:55:1779136 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:55:1779138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:55:1779138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:00:55:1779135 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:00:55:1779135 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.4559s] [ 76%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload1_offload_activations_False_use_orig_params_True [2025-09-19 08:01:26.731] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:26.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:26.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:26.760] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:01:27:1779457 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:27:1779457 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:27:1779456 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:27:1779456 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:27:1779455 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:27:1779455 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:27:1779454 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:27:1779454 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.4548s] [ 82%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload1_offload_activations_True_use_orig_params_False [2025-09-19 08:01:58.166] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:58.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:58.219] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:01:58.219] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:01:58:1779773 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:58:1779773 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:58:1779775 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:58:1779775 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:58:1779776 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:58:1779776 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:01:58:1779774 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:01:58:1779774 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.2540s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpoint::test_checkpoint_fsdp_wrapping_cpu_offload1_offload_activations_True_use_orig_params_True [2025-09-19 08:02:29.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:02:29.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:02:29.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:02:29.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:02:29:1780094 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:02:29:1780094 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:02:29:1780096 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:02:29:1780096 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:02:29:1780095 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:02:29:1780095 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:02:29:1780093 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:02:29:1780093 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.0516s] [ 94%]
../../../../test/distributed/fsdp/test_fsdp_checkpoint.py::TestFSDPCheckpointSubmoduleXPU::test_checkpoint_submodule_use_reentrant_False_xpu [2025-09-19 08:03:00.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:00.488] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:00.489] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:00.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:03:00:1780413 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:00:1780413 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:00:1780412 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:00:1780412 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:00:1780415 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:00:1780415 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:00:1780414 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:00:1780414 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [17.1238s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_checkpoint.py.xml -
======================== 17 passed in 520.38s (0:08:40) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:03:18.535] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/fsdp/test_fsdp_clip_grad_norm.py::TestClipGradNormXPU::test_ddp_parity_xpu [2025-09-19 08:03:20.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:20.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:20.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:20.753] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:03:21:1780803 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:21:1780803 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:21:1780804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:21:1780804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:21:1780805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:21:1780805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:21:1780806 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:21:1780806 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
FAILED [31.6410s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_clip_grad_norm.py::TestClipGradNormXPU::test_low_precision_grads_xpu [2025-09-19 08:03:52.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:52.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:52.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:03:52.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:03:52:1781121 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:52:1781121 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:52:1781122 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:52:1781122 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:52:1781123 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:52:1781123 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:03:52:1781124 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:03:52:1781124 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [16.5301s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_clip_grad_norm.py::TestClipGradNormXPU::test_no_gradients_xpu [2025-09-19 08:04:08.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:08.892] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:08.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:08.913] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:04:09:1781439 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:09:1781439 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:09:1781442 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:09:1781442 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:09:1781441 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:09:1781441 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:09:1781440 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:09:1781440 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [16.0294s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_clip_grad_norm.py::TestClipGradNormXPU::test_non_root_xpu [2025-09-19 08:04:24.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:24.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:24.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:24.963] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:04:25:1781742 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:25:1781742 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:25:1781739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:25:1781739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:25:1781740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:25:1781740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:25:1781741 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:25:1781741 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [30.8514s] [100%]

=================================== FAILURES ===================================
___________________ TestClipGradNormXPU.test_ddp_parity_xpu ____________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:03:18.847000 1780730 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1780803
I0919 08:03:18.848000 1780730 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1780804
I0919 08:03:18.848000 1780730 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1780805
I0919 08:03:18.849000 1780730 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1780806
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_clip_grad_norm.py.xml -
=========================== short test summary info ============================
FAILED [31.6410s] ../../../../test/distributed/fsdp/test_fsdp_clip_grad_norm.py::TestClipGradNormXPU::test_ddp_parity_xpu - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 78, in test_ddp_parity
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_clip_grad_norm.py", line 236, in _test_ddp_parity
    self.assertEqual(ddp_total_norm, fsdp_total_norm)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 23674.564453125 but got 23686.31640625.
Absolute difference: 11.751953125 (up to 1e-05 allowed)
Relative difference: 0.0004963957477768408 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_clip_grad_norm.py TestClipGradNormXPU.test_ddp_parity_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
==================== 1 failed, 3 passed in 97.23s (0:01:37) ====================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:04:56.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 10 items
Running 10 items in this shard

../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_False_use_no_sync_False_sharding_strategy0_xpu [2025-09-19 08:04:58.804] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:58.804] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:58.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:04:58.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:04:59:1782138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:59:1782138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:59:1782135 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:59:1782135 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:59:1782137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:59:1782137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:04:59:1782136 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:04:59:1782136 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [16.2316s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_False_use_no_sync_False_sharding_strategy1_xpu [2025-09-19 08:05:15.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:15.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:15.081] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:15.083] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:05:15:1782452 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:15:1782452 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:15:1782453 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:15:1782453 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:15:1782451 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:15:1782451 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:15:1782454 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:15:1782454 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [16.1293s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_False_use_no_sync_True_sharding_strategy0_xpu [2025-09-19 08:05:31.163] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:31.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:31.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:31.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:05:31:1782769 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:31:1782769 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:31:1782770 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:31:1782770 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:31:1782771 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:31:1782771 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:31:1782768 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:31:1782768 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [16.5276s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_False_use_no_sync_True_sharding_strategy1_xpu [2025-09-19 08:05:47.676] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:47.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:47.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:05:47.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:05:48:1783085 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:48:1783085 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:48:1783086 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:48:1783086 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:48:1783084 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:48:1783084 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:05:48:1783087 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:05:48:1783087 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [16.5307s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_True_use_no_sync_False_sharding_strategy0_xpu [2025-09-19 08:06:04.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:04.268] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:04.268] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:04.268] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:06:04:1783403 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:04:1783403 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:04:1783402 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:04:1783402 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:04:1783404 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:04:1783404 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:04:1783401 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:04:1783401 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.9285s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_True_use_no_sync_False_sharding_strategy1_xpu [2025-09-19 08:06:20.200] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:20.212] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:20.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:20.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:06:20:1783720 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:20:1783720 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:20:1783719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:20:1783719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:20:1783721 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:20:1783721 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:20:1783722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:20:1783722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.8297s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_True_use_no_sync_True_sharding_strategy0_xpu [2025-09-19 08:06:35.988] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:35.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:36.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:36.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:06:36:1784035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:36:1784035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:36:1784037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:36:1784037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:36:1784036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:36:1784036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:36:1784038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:36:1784038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [16.1281s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestCommunicationXPU::test_communication_nested_model_True_use_no_sync_True_sharding_strategy1_xpu [2025-09-19 08:06:52.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:52.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:52.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:06:52.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:06:52:1784354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:52:1784354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:52:1784353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:52:1784353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:52:1784355 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:52:1784355 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:06:52:1784352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:06:52:1784352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [16.1299s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestExplicitUnshardXPU::test_unshard_async_use_orig_params_False_xpu [2025-09-19 08:07:08.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:07:08.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:07:08:1784670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:07:08:1784670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:07:08:1784669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:07:08:1784669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.7489s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_comm.py::TestExplicitUnshardXPU::test_unshard_async_use_orig_params_True_xpu [2025-09-19 08:07:38.975] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:07:38.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:07:39:1784828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:07:39:1784828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:07:39:1784827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:07:39:1784827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.4396s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_comm.py.xml -
======================== 10 passed in 192.70s (0:03:12) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:08:10.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 28 items
Running 28 items in this shard

../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_False_sharding_strategy0 [2025-09-19 08:08:12.613] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:12.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:12.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:12.701] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:08:12:1785064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:12:1785064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:12:1785062 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:12:1785062 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:12:1785063 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:12:1785063 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:12:1785065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:12:1785065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [16.2313s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_False_sharding_strategy1 [2025-09-19 08:08:28.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:28.643] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:28.645] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:28.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:08:28:1785380 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:28:1785380 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:28:1785381 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:28:1785381 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:28:1785378 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:28:1785378 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:28:1785379 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:28:1785379 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [27.4457s] [  7%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_False_sharding_strategy2 [2025-09-19 08:08:56.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:56.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:56.097] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:08:56.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:08:56:1785699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:56:1785699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:56:1785697 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:56:1785697 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:56:1785696 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:56:1785696 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:08:56:1785698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:08:56:1785698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [27.8508s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_True_sharding_strategy0 [2025-09-19 08:09:23.912] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:23.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:23.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:23.937] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:09:24:1786019 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:24:1786019 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:24:1786016 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:24:1786016 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:24:1786018 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:24:1786018 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:24:1786017 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:24:1786017 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [15.9297s] [ 14%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_True_sharding_strategy1 [2025-09-19 08:09:39.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:39.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:39.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:09:39.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:09:40:1786334 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:40:1786334 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:40:1786333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:40:1786333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:40:1786335 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:40:1786335 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:09:40:1786336 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:09:40:1786336 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [42.5740s] [ 17%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_bf16_hook_has_wrapping_True_sharding_strategy2 [2025-09-19 08:10:22.427] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:10:22.427] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:10:22.437] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:10:22.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:10:22:1786654 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:10:22:1786654 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:10:22:1786655 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:10:22:1786655 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:10:22:1786653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:10:22:1786653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:10:22:1786652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:10:22:1786652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [42.5732s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_behavior_sharding_strategy0 [2025-09-19 08:11:04.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:05.004] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:05.007] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:05.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:11:05:1786972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:05:1786972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:05:1786970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:05:1786970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:05:1786973 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:05:1786973 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:05:1786971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:05:1786971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.6294s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_behavior_sharding_strategy1 [2025-09-19 08:11:20.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:20.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:20.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:20.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:11:20:1787290 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:20:1787290 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:20:1787289 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:20:1787289 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:20:1787288 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:20:1787288 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:20:1787287 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:20:1787287 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [30.7513s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_behavior_sharding_strategy2 [2025-09-19 08:11:51.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:51.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:51.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:11:51.405] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:11:51:1787605 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:51:1787605 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:51:1787604 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:51:1787604 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:51:1787607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:51:1787607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:11:51:1787606 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:11:51:1787606 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.1549s] [ 32%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_False_sharding_strategy0 [2025-09-19 08:12:22.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:22.568] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:22.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:22.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:12:22:1787924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:22:1787924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:22:1787925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:22:1787925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:22:1787923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:22:1787923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:22:1787922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:22:1787922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [15.4292s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_False_sharding_strategy1 [2025-09-19 08:12:37.933] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:37.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:37.975] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:37.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:12:38:1788223 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:38:1788223 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:38:1788222 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:38:1788222 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:38:1788225 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:38:1788225 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:38:1788224 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:38:1788224 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [15.5289s] [ 39%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_False_sharding_strategy2 [2025-09-19 08:12:53.491] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:53.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:53.528] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:12:53.528] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:12:53:1788526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:53:1788526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:53:1788527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:53:1788527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:53:1788528 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:53:1788528 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:12:53:1788525 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:12:53:1788525 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.5219s] [ 42%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_True_sharding_strategy0 [2025-09-19 08:13:09.008] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:09.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:09.023] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:09.029] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:13:09:1788828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:09:1788828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:09:1788827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:09:1788827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:09:1788826 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:09:1788826 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:09:1788825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:09:1788825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [15.5268s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_True_sharding_strategy1 [2025-09-19 08:13:24.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:24.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:24.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:24.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:13:24:1789125 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:24:1789125 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:24:1789126 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:24:1789126 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:24:1789128 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:24:1789128 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:24:1789127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:24:1789127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [15.5292s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_default_communication_hook_initialization_has_wrapping_True_sharding_strategy2 [2025-09-19 08:13:40.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:40.088] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:40.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:40.109] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:13:40:1789429 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:40:1789429 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:40:1789428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:40:1789428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:40:1789430 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:40:1789430 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:40:1789427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:40:1789427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [15.5283s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_False_sharding_strategy0 [2025-09-19 08:13:55.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:55.640] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:55.640] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:13:55.640] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:13:55:1789727 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:55:1789727 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:55:1789729 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:55:1789729 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:55:1789730 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:55:1789730 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:13:55:1789728 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:13:55:1789728 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.9215s] [ 57%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_False_sharding_strategy1 [2025-09-19 08:14:11.519] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:11.521] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:11.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:11.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:14:11:1790043 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:11:1790043 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:11:1790045 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:11:1790045 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:11:1790044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:11:1790044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:11:1790046 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:11:1790046 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [16.0300s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_False_sharding_strategy2 [2025-09-19 08:14:27.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:27.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:27.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:27.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:14:27:1790363 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:27:1790363 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:27:1790362 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:27:1790362 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:27:1790361 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:27:1790361 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:27:1790364 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:27:1790364 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [16.1306s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_True_sharding_strategy0 [2025-09-19 08:14:43.681] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:43.684] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:43.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:43.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:14:43:1790679 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:43:1790679 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:43:1790681 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:43:1790681 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:43:1790682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:43:1790682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:43:1790680 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:43:1790680 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [15.9279s] [ 67%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_True_sharding_strategy1 [2025-09-19 08:14:59.604] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:59.605] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:59.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:14:59.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:14:59:1790995 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:59:1790995 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:59:1790998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:59:1790998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:59:1790996 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:59:1790996 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:14:59:1790997 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:14:59:1790997 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [16.2303s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_fp16_hook_has_wrapping_True_sharding_strategy2 [2025-09-19 08:15:15.848] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:15.849] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:15.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:15.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:15:16:1791315 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:16:1791315 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:16:1791312 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:16:1791312 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:16:1791313 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:16:1791313 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:16:1791314 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:16:1791314 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.9290s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_hybrid_strategy [2025-09-19 08:15:31.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:31.811] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:31.811] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:31.811] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:15:32:1791629 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:32:1791629 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:32:1791630 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:32:1791630 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:32:1791631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:32:1791631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:32:1791632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:32:1791632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [15.5293s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_non_root_sharding_strategy0 [2025-09-19 08:15:47.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:47.340] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:47.347] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:15:47.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:15:47:1791947 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:47:1791947 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:47:1791949 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:47:1791949 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:47:1791950 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:47:1791950 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:15:47:1791948 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:15:47:1791948 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [15.6301s] [ 82%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_non_root_sharding_strategy1 [2025-09-19 08:16:02.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:02.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:02.967] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:02.969] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:16:03:1792249 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:03:1792249 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:03:1792252 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:03:1792252 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:03:1792251 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:03:1792251 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:03:1792250 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:03:1792250 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.6291s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_non_root_sharding_strategy2 [2025-09-19 08:16:18.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:18.617] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:18.617] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:18.617] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:16:18:1792553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:18:1792553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:18:1792550 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:18:1792550 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:18:1792552 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:18:1792552 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:18:1792551 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:18:1792551 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.4290s] [ 89%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_submodules_sharding_strategy0 [2025-09-19 08:16:33.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:33.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:34.009] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:34.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:16:34:1792852 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:34:1792852 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:34:1792850 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:34:1792850 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:34:1792853 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:34:1792853 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:34:1792851 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:34:1792851 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.4290s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_submodules_sharding_strategy1 [2025-09-19 08:16:49.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:49.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:49.459] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:16:49.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:16:49:1793152 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:49:1793152 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:49:1793151 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:49:1793151 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:49:1793150 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:49:1793150 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:16:49:1793153 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:16:49:1793153 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.8296s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_comm_hooks.py::TestCommunicationHooks::test_registering_hook_submodules_sharding_strategy2 [2025-09-19 08:17:05.274] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:05.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:05.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:05.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:17:05:1793454 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:05:1793454 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:05:1793457 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:05:1793457 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:05:1793456 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:05:1793456 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:05:1793455 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:05:1793455 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [15.7269s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_comm_hooks.py.xml -
======================== 28 passed in 550.59s (0:09:10) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:17:21.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 60 items
Running 60 items in this shard

../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_pre_backward_hook_registration_after_state_dict_xpu [2025-09-19 08:17:24.056] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:24.056] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:24.056] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:24.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:17:24:1793832 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:24:1793832 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:24:1793831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:24:1793831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:24:1793833 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:24:1793833 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:24:1793830 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:24:1793830 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.7437s] [  1%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_pre_backward_hook_registration_cuda_first_False_xpu [2025-09-19 08:17:55.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:55.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:55.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:17:55.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:17:56:1794149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:56:1794149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:56:1794151 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:56:1794151 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:56:1794148 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:56:1794148 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:17:56:1794150 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:17:56:1794150 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.3397s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_pre_backward_hook_registration_cuda_first_True_xpu [2025-09-19 08:18:27.237] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:27.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:27.257] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:27.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:18:27:1794467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:27:1794467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:27:1794469 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:27:1794469 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:27:1794466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:27:1794466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:27:1794468 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:27:1794468 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.1528s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_register_functions_called_cuda_first_False_mixed_precision_False_xpu [2025-09-19 08:18:58.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:58.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:58.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:18:58.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:18:58:1794784 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:58:1794784 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:58:1794787 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:58:1794787 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:58:1794786 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:58:1794786 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:18:58:1794785 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:18:58:1794785 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.1303s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_register_functions_called_cuda_first_False_mixed_precision_True_xpu [2025-09-19 08:19:14.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:14.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:14.448] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:14.449] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:19:14:1795086 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:14:1795086 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:14:1795087 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:14:1795087 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:14:1795084 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:14:1795084 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:14:1795085 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:14:1795085 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [16.0292s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_register_functions_called_cuda_first_True_mixed_precision_False_xpu [2025-09-19 08:19:30.485] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:30.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:30.525] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:30.533] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:19:30:1795386 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:30:1795386 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:30:1795387 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:30:1795387 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:30:1795385 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:30:1795385 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:30:1795384 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:30:1795384 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [16.2308s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestHooksXPU::test_register_functions_called_cuda_first_True_mixed_precision_True_xpu [2025-09-19 08:19:46.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:46.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:46.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:19:46.709] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:19:47:1795687 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:47:1795687 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:47:1795688 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:47:1795688 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:47:1795686 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:47:1795686 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:19:47:1795689 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:19:47:1795689 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.3299s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_false_no_shard_xpu [2025-09-19 08:20:03.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:03.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:03.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:03.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:20:03:1795989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:03:1795989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:03:1795986 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:03:1795986 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:03:1795987 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:03:1795987 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:03:1795988 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:03:1795988 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [41.0655s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_false_none_xpu [2025-09-19 08:20:44.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:44.121] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:44.121] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:20:44.121] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:20:44:1796306 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:44:1796306 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:44:1796303 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:44:1796303 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:44:1796304 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:44:1796304 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:20:44:1796305 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:20:44:1796305 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
FAILED [32.4567s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_false_shard_grad_op_xpu [2025-09-19 08:21:16.560] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:21:16.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:21:16.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:21:16.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:21:16:1796623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:21:16:1796623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:21:16:1796622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:21:16:1796622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:21:16:1796625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:21:16:1796625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:21:16:1796624 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:21:16:1796624 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [56.3920s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_true_no_shard_xpu [2025-09-19 08:22:12.948] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:22:12.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:22:12.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:22:12.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:22:13:1796943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:22:13:1796943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:22:13:1796942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:22:13:1796942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:22:13:1796945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:22:13:1796945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:22:13:1796944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:22:13:1796944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [47.3729s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_true_none_xpu [2025-09-19 08:23:00.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:00.335] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:00.399] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:00.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:23:00:1797263 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:00:1797263 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:00:1797262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:00:1797262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:00:1797264 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:00:1797264 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:00:1797261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:00:1797261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
FAILED [38.9655s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_true_shard_grad_op_xpu [2025-09-19 08:23:39.312] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:39.324] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:39.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:23:39.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:23:39:1797579 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:39:1797579 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:39:1797580 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:39:1797580 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:39:1797578 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:39:1797578 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:23:39:1797577 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:23:39:1797577 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [62.9999s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_false_no_shard_xpu [2025-09-19 08:24:42.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:42.288] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:42.303] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:42.304] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:24:42:1797898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:42:1797898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:42:1797899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:42:1797899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:42:1797901 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:42:1797901 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:42:1797900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:42:1797900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [17.1330s] [ 23%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_false_none_xpu [2025-09-19 08:24:59.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:59.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:59.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:24:59.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:24:59:1798217 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:59:1798217 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:59:1798219 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:59:1798219 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:59:1798218 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:59:1798218 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:24:59:1798216 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:24:59:1798216 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
FAILED [33.5583s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_false_shard_grad_op_xpu [2025-09-19 08:25:32.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:25:32.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:25:32.997] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:25:33.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:25:33:1798537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:25:33:1798537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:25:33:1798534 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:25:33:1798534 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:25:33:1798535 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:25:33:1798535 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:25:33:1798536 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:25:33:1798536 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [80.5229s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_true_no_shard_xpu [2025-09-19 08:26:53.496] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:26:53.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:26:53.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:26:53.528] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:26:53:1798860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:26:53:1798860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:26:53:1798862 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:26:53:1798862 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:26:53:1798861 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:26:53:1798861 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:26:53:1798859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:26:53:1798859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [17.3317s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_true_none_xpu [2025-09-19 08:27:10.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:10.819] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:10.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:10.843] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:27:11:1799177 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:11:1799177 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:11:1799180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:11:1799180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:11:1799178 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:11:1799178 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:11:1799179 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:11:1799179 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
FAILED [33.5583s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_true_shard_grad_op_xpu [2025-09-19 08:27:44.395] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:44.397] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:44.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:27:44.409] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:27:44:1799494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:44:1799494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:44:1799493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:44:1799493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:44:1799496 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:44:1799496 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:27:44:1799495 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:27:44:1799495 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [80.8243s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_false_no_shard_xpu [2025-09-19 08:29:05.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:05.251] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:05.251] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:05.253] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:29:05:1799813 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:05:1799813 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:05:1799814 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:05:1799814 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:05:1799815 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:05:1799815 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:05:1799816 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:05:1799816 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:18:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:18:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:19:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:20:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:21:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:22:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:23:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:24:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800114:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800127:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800120:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:25:1800121:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [23.1407s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_false_none_xpu [2025-09-19 08:29:28.364] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:28.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:28.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:29:28.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:29:28:1800324 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:28:1800324 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:28:1800326 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:28:1800326 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:28:1800325 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:28:1800325 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:28:1800327 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:29:28:1800327 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:29:41:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:41:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:41:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:41:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:57:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:58:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:29:59:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:00:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:01:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:02:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:03:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:04:1800640:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:04:1800629:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:04:1800634:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:04:1800627:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [38.4657s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_false_shard_grad_op_xpu [2025-09-19 08:30:06.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:06.835] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:06.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:06.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:30:07:1800837 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:07:1800837 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:07:1800835 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:07:1800835 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:07:1800834 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:07:1800834 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:07:1800836 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:07:1800836 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:20:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:20:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:20:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:20:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:35:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:36:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:37:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:38:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:39:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:40:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:41:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:42:1801142:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:42:1801136:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:42:1801149:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:42:1801143:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [38.1619s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_true_no_shard_xpu [2025-09-19 08:30:44.977] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:44.977] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:44.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:30:45.012] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:30:45:1801345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:45:1801345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:45:1801346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:45:1801346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:45:1801348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:45:1801348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:45:1801347 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:30:45:1801347 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:30:58:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:58:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:58:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:58:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:30:59:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:00:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:01:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:02:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:03:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:04:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801661:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801656:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:05:1801650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [23.4405s] [ 38%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_true_none_xpu [2025-09-19 08:31:08.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:08.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:08.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:08.467] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:31:08:1801902 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:08:1801902 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:08:1801905 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:08:1801905 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:08:1801903 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:08:1801903 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:08:1801904 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:08:1801904 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:22:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:22:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:22:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:22:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:37:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:37:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:37:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:37:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:38:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:39:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:40:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:41:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:42:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:43:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802214:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802217:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:31:44:1802211:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [38.7471s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_offload_true_shard_grad_op_xpu [2025-09-19 08:31:47.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:47.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:47.211] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:31:47.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:31:47:1802462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:47:1802462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:47:1802461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:47:1802461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:47:1802463 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:47:1802463 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:31:47:1802464 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:31:47:1802464 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:32:00:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:00:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:00:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:00:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:16:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:17:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:18:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:19:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:20:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:21:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:22:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802769:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802764:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:23:1802770:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [38.9668s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_false_no_shard_xpu [2025-09-19 08:32:26.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:32:26.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:32:26.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:32:26.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:32:26:1803021 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:32:26:1803021 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:32:26:1803022 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:32:26:1803022 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:32:26:1803024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:32:26:1803024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:32:26:1803023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:32:26:1803023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:32:39:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:39:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:39:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:39:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:41:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:41:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:41:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:41:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:42:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:42:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:42:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:42:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:43:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:43:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:43:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:43:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:45:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:45:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:45:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:45:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:46:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:46:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:46:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:46:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:47:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:47:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:47:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:47:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:49:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:49:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:49:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:49:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:50:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:50:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:50:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:50:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:51:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:51:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:51:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:51:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:53:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:53:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:53:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:53:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:54:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:54:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:54:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:54:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:55:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:55:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:55:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:55:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:57:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:57:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:57:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:57:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:58:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:58:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:58:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:58:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:59:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:59:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:59:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:32:59:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:00:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:00:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:00:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:00:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:02:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:02:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:02:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:02:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:03:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:03:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:03:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:03:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:04:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:04:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:04:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:04:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:06:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:06:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:06:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:06:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:07:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:07:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:07:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:07:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:08:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:08:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:08:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:08:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:10:1803331:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:10:1803324:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:10:1803326:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:10:1803333:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [47.1740s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_false_none_xpu [2025-09-19 08:33:13.305] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:33:13.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:33:13.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:33:13.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:33:13:1803532 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:33:13:1803532 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:33:13:1803534 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:33:13:1803534 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:33:13:1803531 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:33:13:1803531 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:33:13:1803533 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:33:13:1803533 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:33:27:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:27:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:27:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:27:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:43:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:43:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:43:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:43:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:45:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:45:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:45:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:45:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:46:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:46:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:46:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:46:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:47:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:47:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:47:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:47:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:49:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:49:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:49:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:49:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:50:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:50:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:50:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:50:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:51:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:51:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:51:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:51:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:53:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:53:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:53:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:53:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:54:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:54:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:54:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:54:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:55:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:55:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:55:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:55:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:57:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:57:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:57:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:57:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:58:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:58:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:58:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:58:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:59:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:59:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:59:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:33:59:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:00:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:00:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:00:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:00:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:02:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:02:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:02:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:02:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:03:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:03:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:03:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:03:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:04:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:04:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:04:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:04:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:06:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:06:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:06:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:06:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:07:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:07:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:07:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:07:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:08:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:08:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:08:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:08:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:10:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:10:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:10:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:10:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:11:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:11:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:11:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:11:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:12:1803837:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:12:1803842:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:12:1803844:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:12:1803835:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [62.5982s] [ 45%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_false_shard_grad_op_xpu [2025-09-19 08:34:15.921] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:34:15.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:34:15.941] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:34:15.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:34:16:1804045 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:34:16:1804045 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:34:16:1804047 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:34:16:1804047 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:34:16:1804044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:34:16:1804044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:34:16:1804046 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:34:16:1804046 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:34:29:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:29:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:29:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:29:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:46:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:46:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:46:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:46:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:47:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:47:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:47:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:47:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:48:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:48:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:48:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:48:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:50:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:50:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:50:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:50:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:51:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:51:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:51:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:51:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:53:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:53:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:53:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:53:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:54:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:54:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:54:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:54:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:55:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:55:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:55:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:55:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:56:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:56:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:56:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:56:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:58:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:58:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:58:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:58:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:59:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:59:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:59:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:34:59:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:00:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:00:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:00:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:00:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:02:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:02:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:02:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:02:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:03:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:03:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:03:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:03:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:04:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:04:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:04:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:04:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:06:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:06:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:06:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:06:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:07:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:07:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:07:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:07:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:08:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:08:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:08:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:08:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:09:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:09:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:09:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:09:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:11:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:11:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:11:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:11:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:12:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:12:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:12:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:12:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:13:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:13:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:13:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:13:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:15:1804346:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:15:1804359:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:15:1804348:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:15:1804353:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [62.4985s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_true_no_shard_xpu [2025-09-19 08:35:18.443] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:35:18.443] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:35:18.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:35:18.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:35:18:1804557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:35:18:1804557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:35:18:1804558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:35:18:1804558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:35:18:1804556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:35:18:1804556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:35:18:1804555 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:35:18:1804555 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:35:32:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:32:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:32:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:32:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:34:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:34:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:34:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:34:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:35:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:35:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:35:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:35:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:36:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:36:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:36:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:36:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:37:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:37:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:37:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:37:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:39:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:39:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:39:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:39:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:40:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:40:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:40:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:40:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:41:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:41:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:41:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:41:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:43:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:43:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:43:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:43:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:44:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:44:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:44:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:44:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:45:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:45:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:45:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:45:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:47:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:47:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:47:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:47:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:48:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:48:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:48:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:48:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:49:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:49:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:49:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:49:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:50:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:50:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:50:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:50:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:52:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:52:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:52:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:52:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:53:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:53:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:53:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:53:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:54:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:54:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:54:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:54:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:56:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:56:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:56:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:56:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:57:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:57:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:57:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:57:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:58:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:58:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:58:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:35:58:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:00:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:00:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:00:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:00:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:01:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:01:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:01:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:01:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:02:1804871:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:02:1804862:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:02:1804865:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:02:1804856:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [47.4741s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_true_none_xpu [2025-09-19 08:36:05.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:36:05.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:36:05.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:36:05.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:36:06:1805114 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:36:06:1805114 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:36:06:1805115 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:36:06:1805115 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:36:06:1805116 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:36:06:1805116 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:36:06:1805117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:36:06:1805117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:36:20:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:20:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:20:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:20:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:36:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:36:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:36:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:36:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:38:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:38:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:38:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:38:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:39:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:39:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:39:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:39:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:40:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:40:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:40:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:40:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:42:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:42:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:42:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:42:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:43:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:43:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:43:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:43:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:44:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:44:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:44:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:44:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:45:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:45:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:45:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:45:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:47:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:47:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:47:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:47:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:48:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:48:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:48:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:48:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:49:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:49:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:49:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:49:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:51:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:51:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:51:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:51:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:52:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:52:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:52:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:52:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:53:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:53:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:53:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:53:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:55:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:55:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:55:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:55:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:56:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:56:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:56:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:56:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:57:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:57:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:57:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:57:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:59:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:59:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:59:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:36:59:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:00:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:00:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:00:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:00:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:01:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:01:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:01:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:01:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:02:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:02:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:02:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:02:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:04:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:04:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:04:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:04:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:05:1805424:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:05:1805419:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:05:1805430:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:05:1805417:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [62.9995s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_mixture_of_experts_with_delay_before_free_offload_true_shard_grad_op_xpu [2025-09-19 08:37:08.940] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:37:08.947] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:37:08.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:37:08.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:37:09:1805676 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:37:09:1805676 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:37:09:1805674 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:37:09:1805674 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:37:09:1805673 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:37:09:1805673 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:37:09:1805675 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:37:09:1805675 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:37:23:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:23:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:23:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:23:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:39:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:39:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:39:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:39:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:41:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:41:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:41:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:41:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:42:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:42:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:42:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:42:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:43:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:43:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:43:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:43:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:44:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:44:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:44:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:44:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:46:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:46:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:46:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:46:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:47:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:47:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:47:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:47:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:48:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:48:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:48:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:48:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:50:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:50:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:50:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:50:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:51:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:51:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:51:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:51:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:52:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:52:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:52:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:52:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:54:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:54:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:54:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:54:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:55:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:55:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:55:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:55:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:56:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:56:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:56:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:56:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:58:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:58:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:58:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:58:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:59:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:59:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:59:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:37:59:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:00:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:00:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:00:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:00:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:01:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:01:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:01:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:01:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:03:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:03:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:03:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:03:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:04:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:04:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:04:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:04:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:05:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:05:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:05:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:05:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:07:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:07:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:07:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:07:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:08:1805977:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:08:1805975:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:08:1805988:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-08:38:08:1805982:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [63.0008s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_false_no_shard_xpu [2025-09-19 08:38:11.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:11.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:11.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:11.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:38:12:1806234 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:12:1806234 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:12:1806233 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:12:1806233 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:12:1806235 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:12:1806235 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:12:1806232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:12:1806232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [17.1304s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_false_none_xpu [2025-09-19 08:38:29.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:29.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:29.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:38:29.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:38:29:1806552 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:29:1806552 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:29:1806553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:29:1806553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:29:1806554 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:29:1806554 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:38:29:1806551 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:38:29:1806551 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [32.8575s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_false_shard_grad_op_xpu [2025-09-19 08:39:01.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:01.925] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:01.925] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:01.927] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:39:02:1806872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:02:1806872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:02:1806870 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:02:1806870 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:02:1806869 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:02:1806869 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:02:1806871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:02:1806871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [32.7577s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_true_no_shard_xpu [2025-09-19 08:39:34.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:34.665] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:34.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:34.731] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:39:34:1807190 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:34:1807190 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:35:1807192 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:35:1807192 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:35:1807191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:35:1807191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:35:1807189 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:35:1807189 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [17.5318s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_true_none_xpu [2025-09-19 08:39:52.253] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:52.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:52.263] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:39:52.280] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:39:52:1807507 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:52:1807507 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:52:1807508 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:52:1807508 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:52:1807506 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:52:1807506 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:39:52:1807505 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:39:52:1807505 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [32.6546s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_always_wrap_model_offload_true_shard_grad_op_xpu [2025-09-19 08:40:24.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:24.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:24.860] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:24.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:40:25:1807826 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:25:1807826 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:25:1807827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:25:1807827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:25:1807825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:25:1807825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:25:1807824 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:25:1807824 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [33.0535s] [ 61%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_false_no_shard_xpu [2025-09-19 08:40:57.892] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:57.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:57.916] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:40:57.923] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:40:58:1808142 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:58:1808142 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:58:1808144 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:58:1808144 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:58:1808145 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:58:1808145 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:40:58:1808143 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:40:58:1808143 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [16.9298s] [ 63%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_false_none_xpu [2025-09-19 08:41:14.841] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:14.844] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:14.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:14.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:41:15:1808460 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:15:1808460 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:15:1808462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:15:1808462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:15:1808461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:15:1808461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:15:1808459 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:15:1808459 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
FAILED [31.1525s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_false_shard_grad_op_xpu [2025-09-19 08:41:45.964] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:45.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:45.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:41:46.005] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:41:46:1808778 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:46:1808778 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:46:1808776 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:46:1808776 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:46:1808779 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:46:1808779 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:41:46:1808777 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:41:46:1808777 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [32.3566s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_true_no_shard_xpu [2025-09-19 08:42:18.360] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:18.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:18.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:18.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:42:18:1809097 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:18:1809097 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:18:1809095 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:18:1809095 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:18:1809098 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:18:1809098 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:18:1809096 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:18:1809096 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [17.2324s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_true_none_xpu [2025-09-19 08:42:35.543] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:35.565] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:35.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:42:35.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:42:35:1809416 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:35:1809416 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:35:1809414 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:35:1809414 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:35:1809417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:35:1809417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:42:35:1809415 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:42:35:1809415 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
FAILED [31.6559s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_true_shard_grad_op_xpu [2025-09-19 08:43:07.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:07.224] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:07.224] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:07.226] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:43:07:1809732 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:07:1809732 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:07:1809735 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:07:1809735 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:07:1809733 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:07:1809733 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:07:1809734 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:07:1809734 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [32.6575s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_false_no_shard_xpu [2025-09-19 08:43:39.845] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:39.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:39.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:39.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:43:40:1810050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:40:1810050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:40:1810049 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:40:1810049 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:40:1810051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:40:1810051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:40:1810052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:40:1810052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [16.6231s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_false_none_xpu [2025-09-19 08:43:56.475] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:56.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:56.497] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:43:56.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:43:56:1810366 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:56:1810366 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:56:1810368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:56:1810368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:56:1810367 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:56:1810367 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:43:56:1810369 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:43:56:1810369 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [16.7258s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_false_shard_grad_op_xpu [2025-09-19 08:44:13.175] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:13.237] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:13.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:13.264] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:44:13:1810682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:13:1810682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:13:1810685 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:13:1810684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:13:1810685 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:13:1810684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:13:1810683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:13:1810683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [16.7313s] [ 76%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_true_no_shard_xpu [2025-09-19 08:44:29.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:29.953] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:29.965] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:29.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:44:30:1811000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:30:1811000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:30:1810999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:30:1810999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:30:1811001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:30:1811001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:30:1811002 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:30:1811002 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [16.8313s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_true_none_xpu [2025-09-19 08:44:46.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:46.836] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:46.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:44:46.867] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:44:47:1811318 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:47:1811318 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:47:1811317 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:47:1811317 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:47:1811316 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:47:1811316 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:44:47:1811319 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:44:47:1811319 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [17.1314s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_single_iteration_mixed_precision_offload_true_shard_grad_op_xpu [2025-09-19 08:45:03.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:03.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:03.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:03.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:45:04:1811633 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:04:1811633 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:04:1811635 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:04:1811635 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:04:1811634 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:04:1811634 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:04:1811636 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:04:1811636 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [17.1317s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_false_no_shard_xpu [2025-09-19 08:45:21.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:21.072] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:21.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:21.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:45:21:1811950 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:21:1811950 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:21:1811952 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:21:1811952 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:21:1811951 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:21:1811951 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:21:1811949 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:21:1811949 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [19.2340s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_false_none_xpu [2025-09-19 08:45:40.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:40.289] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:40.300] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:45:40.303] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:45:40:1812268 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:40:1812268 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:40:1812269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:40:1812269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:40:1812266 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:40:1812266 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:45:40:1812267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:45:40:1812267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
FAILED [31.7558s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_false_shard_grad_op_xpu [2025-09-19 08:46:12.035] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:12.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:12.064] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:12.088] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:46:12:1812584 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:12:1812584 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:12:1812587 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:12:1812587 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:12:1812586 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:12:1812586 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:12:1812585 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:12:1812585 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [34.8604s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_true_no_shard_xpu [2025-09-19 08:46:46.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:46.952] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:46.953] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:46:46.953] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:46:47:1812904 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:47:1812904 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:47:1812903 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:47:1812903 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:47:1812906 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:47:1812906 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:46:47:1812905 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:46:47:1812905 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [20.0357s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_true_none_xpu [2025-09-19 08:47:06.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:06.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:06.996] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:07.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:47:07:1813223 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:07:1813223 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:07:1813221 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:07:1813221 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:07:1813220 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:07:1813220 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:07:1813222 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:07:1813222 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
FAILED [32.5567s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_true_shard_grad_op_xpu [2025-09-19 08:47:39.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:39.569] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:39.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:47:39.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:47:39:1813537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:39:1813537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:39:1813540 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:39:1813540 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:39:1813538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:39:1813538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:47:39:1813539 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:47:39:1813539 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [36.0647s] [ 91%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestNoGradXPU::test_transformer_no_grad_mixed_precision_False_xpu [2025-09-19 08:48:15.549] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:15.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:15.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:15.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:48:15:1813855 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:15:1813855 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:15:1813856 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:15:1813856 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:15:1813858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:15:1813858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:15:1813857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:15:1813857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.7587s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestNoGradXPU::test_transformer_no_grad_mixed_precision_True_xpu [2025-09-19 08:48:47.333] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:47.356] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:47.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:48:47.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:48:47:1814174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:47:1814174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:47:1814176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:47:1814176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:47:1814175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:47:1814175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:48:47:1814173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:48:47:1814173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
FAILED [32.0588s] [ 95%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParamInitXPU::test_param_change_after_init_mixed_precision_False_xpu [2025-09-19 08:49:19.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:19.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:19.432] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:19.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:49:19:1814489 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:19:1814489 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:19:1814492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:19:1814492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:19:1814490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:19:1814490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:19:1814491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:19:1814491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [16.4240s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestParamInitXPU::test_param_change_after_init_mixed_precision_True_xpu [2025-09-19 08:49:35.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:35.803] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:35.909] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:35.937] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:49:36:1814793 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:36:1814793 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:36:1814792 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:36:1814792 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:36:1814794 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:36:1814794 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:36:1814791 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:36:1814791 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.5306s] [ 98%]
../../../../test/distributed/fsdp/test_fsdp_core.py::TestAutogradXPU::test_unshard_params_as_tensors_xpu [2025-09-19 08:49:52.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:52.440] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:52.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:49:52.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:49:52:1815094 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:52:1815094 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:52:1815091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:52:1815091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:52:1815092 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:52:1815092 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:49:52:1815093 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:49:52:1815093 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [32.2535s] [100%]

=================================== FAILURES ===================================
_____ TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu ______
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:20:42.207000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1796303
I0919 08:20:42.208000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1796304
I0919 08:20:42.208000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1796305
I0919 08:20:42.209000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1796306
______ TestParityWithDDPXPU.test_delayed_optim_step_offload_true_none_xpu ______
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: 0.002280794084072113 at index (1, 0) (up to 1e-05 allowed)
Greatest relative difference: 0.16635674238204956 at index (1, 7) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:22:58.432000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1797261
I0919 08:22:58.433000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1797262
I0919 08:22:58.433000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1797263
I0919 08:22:58.434000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1797264
___ TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_false_none_xpu ____
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 210, in test_delayed_reduce_scatter
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:24:57.534000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1798216
I0919 08:24:57.535000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1798217
I0919 08:24:57.535000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1798218
I0919 08:24:57.536000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1798219
____ TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_true_none_xpu ____
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 210, in test_delayed_reduce_scatter
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 31 / 32 (96.9%)
Greatest absolute difference: 0.00503598153591156 at index (1, 1) (up to 1e-05 allowed)
Greatest relative difference: 0.3581630289554596 at index (1, 7) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:27:08.951000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1799177
I0919 08:27:08.951000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1799178
I0919 08:27:08.952000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1799179
I0919 08:27:08.952000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1799180
____ TestParityWithDDPXPU.test_nested_wrapped_model_offload_false_none_xpu _____
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 113, in test_nested_wrapped_model
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_nested_wrapped_model_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:41:12.939000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1808459
I0919 08:41:12.940000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1808460
I0919 08:41:12.940000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1808461
I0919 08:41:12.941000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1808462
_____ TestParityWithDDPXPU.test_nested_wrapped_model_offload_true_none_xpu _____
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 113, in test_nested_wrapped_model
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_nested_wrapped_model_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:42:33.686000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1809414
I0919 08:42:33.686000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1809415
I0919 08:42:33.687000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1809416
I0919 08:42:33.687000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1809417
_________ TestParityWithDDPXPU.test_transformer_offload_false_none_xpu _________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.723793029785156 but got 17.822208404541016.
Absolute difference: 0.09841537475585938 (up to 1e-05 allowed)
Relative difference: 0.0055527264728532175 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.723793029785156 but got 17.822208404541016.
Absolute difference: 0.09841537475585938 (up to 1e-05 allowed)
Relative difference: 0.0055527264728532175 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:45:38.418000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1812266
I0919 08:45:38.419000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1812267
I0919 08:45:38.419000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1812268
I0919 08:45:38.419000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1812269
_________ TestParityWithDDPXPU.test_transformer_offload_true_none_xpu __________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 205 / 368 (55.7%)
Greatest absolute difference: 0.000621795654296875 at index (0, 1) (up to 1e-05 allowed)
Greatest relative difference: 0.022012708708643913 at index (5, 3) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:47:05.073000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1813220
I0919 08:47:05.074000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1813221
I0919 08:47:05.074000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1813222
I0919 08:47:05.075000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1813223
_______ TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu ________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 08:48:45.458000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1814173
I0919 08:48:45.458000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1814174
I0919 08:48:45.459000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1814175
I0919 08:48:45.460000 1793756 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1814176
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_core.py.xml -
=========================== short test summary info ============================
FAILED [32.4567s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_false_none_xpu - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [38.9655s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_optim_step_offload_true_none_xpu - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 189, in test_delayed_optim_step
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: 0.002280794084072113 at index (1, 0) (up to 1e-05 allowed)
Greatest relative difference: 0.16635674238204956 at index (1, 7) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_optim_step_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [33.5583s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_false_none_xpu - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 210, in test_delayed_reduce_scatter
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [33.5583s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_delayed_reduce_scatter_offload_true_none_xpu - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 210, in test_delayed_reduce_scatter
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 31 / 32 (96.9%)
Greatest absolute difference: 0.00503598153591156 at index (1, 1) (up to 1e-05 allowed)
Greatest relative difference: 0.3581630289554596 at index (1, 7) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_delayed_reduce_scatter_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [31.1525s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_false_none_xpu - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 113, in test_nested_wrapped_model
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_nested_wrapped_model_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [31.6559s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_nested_wrapped_model_offload_true_none_xpu - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 113, in test_nested_wrapped_model
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_nested_wrapped_model_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [31.7558s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_false_none_xpu - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.723793029785156 but got 17.822208404541016.
Absolute difference: 0.09841537475585938 (up to 1e-05 allowed)
Relative difference: 0.0055527264728532175 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.723793029785156 but got 17.822208404541016.
Absolute difference: 0.09841537475585938 (up to 1e-05 allowed)
Relative difference: 0.0055527264728532175 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_false_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [32.5567s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestParityWithDDPXPU::test_transformer_offload_true_none_xpu - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1484, in _test_fsdp_parity
    torch.testing.assert_close(ref_loss, fsdp_loss, check_dtype=False)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 17.804584503173828 but got 17.822208404541016.
Absolute difference: 0.0176239013671875 (up to 1e-05 allowed)
Relative difference: 0.0009898518757371665 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 168, in test_transformer
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 205 / 368 (55.7%)
Greatest absolute difference: 0.000621795654296875 at index (0, 1) (up to 1e-05 allowed)
Greatest relative difference: 0.022012708708643913 at index (5, 3) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestParityWithDDPXPU.test_transformer_offload_true_none_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [32.0588s] ../../../../test/distributed/fsdp/test_fsdp_core.py::TestNoGradXPU::test_transformer_no_grad_mixed_precision_True_xpu - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py", line 426, in instantiated_test
    result = test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_core.py", line 434, in test_transformer_no_grad
    self.assertEqual(ref_output, no_grad_output)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 68 / 184 (37.0%)
Greatest absolute difference: 0.01171875 at index (0, 1, 12) (up to 1e-05 allowed)
Greatest relative difference: 0.08544921875 at index (3, 0, 14) (up to 0.001 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_core.py TestNoGradXPU.test_transformer_no_grad_mixed_precision_True_xpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
================== 9 failed, 51 passed in 1982.63s (0:33:02) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:50:25.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 15 items
Running 15 items in this shard

../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_False_is_even_sharded_model_False_xpu [2025-09-19 08:50:27.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:27.905] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:27.911] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:27.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:50:28:1815493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:50:28:1815493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:50:28:1815491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:50:28:1815491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:50:28:1815492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:50:28:1815492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:50:28:1815494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:50:28:1815494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [32.0408s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_False_is_even_sharded_model_True_xpu [2025-09-19 08:50:59.831] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:59.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:59.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:50:59.875] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:51:00:1815811 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:00:1815811 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:00:1815809 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:00:1815809 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:00:1815810 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:00:1815810 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:00:1815808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:00:1815808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.7559s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_True_is_even_sharded_model_False_xpu [2025-09-19 08:51:31.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:51:31.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:51:31.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:51:31.703] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:51:31:1816126 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:31:1816126 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:32:1816129 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:32:1816129 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:32:1816127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:32:1816127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:51:32:1816128 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:51:32:1816128 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.8503s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_True_is_even_sharded_model_True_xpu [2025-09-19 08:52:03.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:03.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:03.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:03.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:52:03:1816447 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:03:1816447 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:03:1816445 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:03:1816445 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:03:1816446 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:03:1816446 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:03:1816444 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:03:1816444 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5558s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_False_is_even_sharded_model_False_xpu [2025-09-19 08:52:35.079] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:35.132] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:35.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:52:35.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:52:35:1816762 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:35:1816762 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:35:1816764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:35:1816764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:35:1816765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:35:1816765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:52:35:1816763 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:52:35:1816763 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.7565s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_False_is_even_sharded_model_True_xpu [2025-09-19 08:53:06.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:06.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:06.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:06.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:53:07:1817080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:07:1817080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:07:1817082 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:07:1817082 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:07:1817081 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:07:1817081 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:07:1817083 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:07:1817083 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.7572s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_True_is_even_sharded_model_False_xpu [2025-09-19 08:53:38.535] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:38.549] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:38.551] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:53:38.551] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:53:38:1817398 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:38:1817398 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:38:1817400 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:38:1817400 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:38:1817399 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:38:1817399 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:53:38:1817397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:53:38:1817397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5558s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_True_is_even_sharded_model_True_xpu [2025-09-19 08:54:10.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:10.111] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:10.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:10.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:54:10:1817717 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:10:1817717 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:10:1817716 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:10:1817716 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:10:1817715 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:10:1817715 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:10:1817718 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:10:1817718 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5551s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_False_is_even_sharded_model_False_xpu [2025-09-19 08:54:41.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:41.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:41.677] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:54:41.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:54:41:1818033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:41:1818033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:42:1818034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:42:1818034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:42:1818035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:42:1818035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:54:42:1818036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:54:42:1818036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.6559s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_False_is_even_sharded_model_True_xpu [2025-09-19 08:55:13.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:13.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:13.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:13.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:55:13:1818354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:13:1818354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:13:1818351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:13:1818351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:13:1818352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:13:1818352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:13:1818353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:13:1818353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.6546s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_True_is_even_sharded_model_False_xpu [2025-09-19 08:55:44.989] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:44.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:44.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:55:45.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:55:45:1818671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:45:1818671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:45:1818672 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:45:1818672 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:45:1818674 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:45:1818674 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:55:45:1818673 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:55:45:1818673 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5553s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_True_is_even_sharded_model_True_xpu [2025-09-19 08:56:16.519] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:16.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:16.538] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:16.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:56:16:1818989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:16:1818989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:16:1818991 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:16:1818991 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:16:1818990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:16:1818990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:16:1818992 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:16:1818992 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.4553s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_fsdp_init_with_device_mesh_is_even_sharded_model_False_xpu [2025-09-19 08:56:47.959] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:47.977] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:47.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:56:47.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:56:48:1819310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:48:1819310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:48:1819308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:48:1819308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:48:1819309 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:48:1819309 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:56:48:1819307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:56:48:1819307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5556s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_fsdp_init_with_device_mesh_is_even_sharded_model_True_xpu [2025-09-19 08:57:19.565] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:19.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:19.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:19.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:57:19:1819625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:19:1819625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:19:1819627 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:19:1819627 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:19:1819628 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:19:1819628 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:20:1819626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:20:1819626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.3537s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_dtensor_state_dict.py::TestFSDPWithDeviceMeshAndDTensorXPU::test_raises_warning_or_errors_xpu [2025-09-19 08:57:50.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:50.927] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:50.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:57:50.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:57:51:1819943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:51:1819943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:51:1819942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:51:1819942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:51:1819944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:51:1819944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:57:51:1819945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:57:51:1819945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.5566s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_dtensor_state_dict.py.xml -
======================== 15 passed in 476.83s (0:07:56) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 08:58:23.399] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_first_iter_order_sharding_strategy0_xpu [2025-09-19 08:58:25.538] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:25.604] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:25.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:25.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:58:25:1820335 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:25:1820335 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:25:1820337 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:25:1820337 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:25:1820336 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:25:1820336 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:26:1820338 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:26:1820338 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [16.0316s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_first_iter_order_sharding_strategy1_xpu [2025-09-19 08:58:41.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:41.669] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:41.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:41.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:58:41:1820638 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:41:1820638 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:41:1820635 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:41:1820635 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:42:1820636 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:42:1820636 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:42:1820637 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:42:1820637 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [15.9292s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_later_iter_order_sharding_strategy0_iters_before_path_change_1_xpu [2025-09-19 08:58:57.495] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:57.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:57.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:58:57.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:58:57:1820939 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:57:1820939 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:57:1820936 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:57:1820936 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:57:1820938 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:57:1820938 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:58:58:1820937 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:58:58:1820937 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.2384s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_later_iter_order_sharding_strategy0_iters_before_path_change_3_xpu [2025-09-19 08:59:28.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:59:28.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:59:28.791] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:59:28.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-08:59:29:1821257 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:59:29:1821257 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:59:29:1821256 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:59:29:1821256 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:59:29:1821254 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:59:29:1821254 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-08:59:29:1821255 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-08:59:29:1821255 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.1548s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_later_iter_order_sharding_strategy1_iters_before_path_change_1_xpu [2025-09-19 08:59:59.967] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 08:59:59.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:00:00.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:00:00.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:00:00:1821572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:00:1821572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:00:1821571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:00:1821571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:00:1821573 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:00:1821573 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:00:1821574 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:00:1821574 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.2379s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_invalid_later_iter_order_sharding_strategy1_iters_before_path_change_3_xpu [2025-09-19 09:00:31.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:00:31.201] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:00:31.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:00:31.232] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:00:31:1821892 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:31:1821892 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:31:1821889 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:31:1821889 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:31:1821891 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:31:1821891 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:00:31:1821890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:00:31:1821890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.5563s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_train_eval_sharding_strategy0_xpu [2025-09-19 09:01:02.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:02.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:02.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:02.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:01:03:1822210 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:03:1822210 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:03:1822208 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:03:1822208 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:03:1822209 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:03:1822209 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:03:1822211 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:03:1822211 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.0544s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_exec_order.py::TestFSDPExecOrderXPU::test_train_eval_sharding_strategy1_xpu [2025-09-19 09:01:33.765] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:33.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:33.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:01:33.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:01:34:1822526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:34:1822526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:34:1822529 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:34:1822529 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:34:1822527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:34:1822527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:01:34:1822528 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:01:34:1822528 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.0546s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_exec_order.py.xml -
======================== 8 passed in 221.45s (0:03:41) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:02:05.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/fsdp/test_fsdp_fine_tune.py::TestFSDPFineTuneXPU::test_backward_reshard_hooks_xpu [2025-09-19 09:02:07.935] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:02:07.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:02:08:1822919 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:02:08:1822919 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:02:08:1822918 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:02:08:1822918 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.7379s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_fine_tune.py::TestFSDPFineTuneXPU::test_hooks_multi_traversal_xpu [2025-09-19 09:02:38.680] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:02:38.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:02:39:1823078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:02:39:1823078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:02:39:1823077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:02:39:1823077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [33.5446s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_fine_tune.py::TestFSDPFineTuneXPU::test_parity_with_ddp_xpu [2025-09-19 09:03:12.228] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:03:12.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:03:12:1823237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:03:12:1823236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:03:12:1823237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:03:12:1823236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.8491s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_fine_tune.py::TestFSDPFineTuneXPU::test_parity_with_non_frozen_fsdp_xpu [2025-09-19 09:03:43.110] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:03:43.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:03:43:1823396 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:03:43:1823396 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:03:43:1823395 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:03:43:1823395 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [33.0500s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_fine_tune.py.xml -
======================== 4 passed in 130.37s (0:02:10) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:04:17.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 14 items
Running 14 items in this shard

../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_empty_module [2025-09-19 09:04:19.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:19:1823629 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:19:1823629 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.2921s] [  7%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flat_param_shard_metadata_aligned_full_precision [2025-09-19 09:04:22.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:22:1823703 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:22:1823703 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.0060s] [ 14%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flat_param_shard_metadata_aligned_mixed_precision [2025-09-19 09:04:25.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:25:1823777 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:25:1823777 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [2.9065s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flat_param_shard_metadata_unaligned [2025-09-19 09:04:28.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:28:1823851 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:28:1823851 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.0065s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flat_param_shard_metadata_with_memory_format_memory_format0 [2025-09-19 09:04:31.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:31:1823925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:31:1823925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1066s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flat_param_shard_metadata_with_memory_format_memory_format1 [2025-09-19 09:04:34.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:34:1823999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:34:1823999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1066s] [ 42%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_flatten_nothing [2025-09-19 09:04:37.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:37:1824075 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:37:1824075 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1062s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_numel_with_shared_params [2025-09-19 09:04:40.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:40:1824149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:40:1824149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1065s] [ 57%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_numel_without_shared_params [2025-09-19 09:04:43.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:43:1824223 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:43:1824223 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1064s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_output_with_shared_params [2025-09-19 09:04:46.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:47:1824297 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:47:1824297 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.5069s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_output_without_shared_params [2025-09-19 09:04:50.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:50:1824371 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:50:1824371 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.5072s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_partial_flattening [2025-09-19 09:04:53.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:53:1824445 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:53:1824445 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.0062s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_pnorm_after_step_with_shared_params [2025-09-19 09:04:56.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:04:57:1824519 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:04:57:1824519 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.9076s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_flatten_params.py::TestFlattenParams::test_writeback_orig_params_no_shard [2025-09-19 09:05:00.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:05:00:1824598 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:00:1824598 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.1067s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_flatten_params.py.xml -
============================= 14 passed in 46.79s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:05:04.835] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 32 items
Running 32 items in this shard

../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_False [2025-09-19 09:05:07.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:07.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:07.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:07.036] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:05:07:1824749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:07:1824749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:07:1824748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:07:1824748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:07:1824747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:07:1824747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:07:1824746 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:07:1824746 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.8451s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_True [2025-09-19 09:05:38.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:38.664] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:38.669] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:05:38.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:05:38:1825065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:38:1825065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:38:1825067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:38:1825067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:38:1825064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:38:1825064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:05:38:1825066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:05:38:1825066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.2551s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_False [2025-09-19 09:06:09.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:09.957] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:09.977] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:09.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:06:10:1825383 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:10:1825383 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:10:1825382 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:10:1825382 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:10:1825384 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:10:1825384 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:10:1825385 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:10:1825385 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.4563s] [  9%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_True [2025-09-19 09:06:41.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:41.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:41.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:06:41.425] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:06:41:1825703 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:41:1825703 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:41:1825702 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:41:1825702 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:41:1825704 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:41:1825704 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:06:41:1825705 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:06:41:1825705 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.2550s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_False [2025-09-19 09:07:12.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:12.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:12.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:12.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:07:12:1826026 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:12:1826026 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:12:1826025 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:12:1826025 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:12:1826027 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:12:1826027 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:12:1826028 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:12:1826028 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.2553s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_True [2025-09-19 09:07:43.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:43.927] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:43.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:07:43.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:07:44:1826345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:44:1826345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:44:1826346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:44:1826346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:44:1826344 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:44:1826344 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:07:44:1826343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:07:44:1826343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.6568s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_False [2025-09-19 09:08:15.569] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:15.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:15.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:15.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:08:15:1826661 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:15:1826661 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:15:1826662 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:15:1826662 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:15:1826663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:15:1826663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:15:1826664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:15:1826664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.3556s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_True [2025-09-19 09:08:46.984] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:47.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:47.008] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:08:47.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:08:47:1826979 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:47:1826979 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:47:1826978 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:47:1826978 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:47:1826980 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:47:1826980 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:08:47:1826981 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:08:47:1826981 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.5567s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_False [2025-09-19 09:09:18.545] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:18.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:18.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:18.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:09:18:1827299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:18:1827299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:18:1827296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:18:1827296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:18:1827298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:18:1827298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:18:1827297 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:18:1827297 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.5567s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_True [2025-09-19 09:09:50.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:50.039] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:50.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:09:50.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:09:50:1827617 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:50:1827617 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:50:1827616 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:50:1827616 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:50:1827615 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:50:1827615 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:09:50:1827614 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:09:50:1827614 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.5564s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_False [2025-09-19 09:10:21.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:21.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:21.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:21.605] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:10:21:1827933 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:21:1827933 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:21:1827934 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:21:1827934 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:21:1827931 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:21:1827931 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:21:1827932 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:21:1827932 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.5379s] [ 34%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_True [2025-09-19 09:10:53.117] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:53.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:53.159] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:10:53.196] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:10:53:1828252 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:53:1828252 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:53:1828251 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:53:1828251 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:53:1828253 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:53:1828253 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:10:53:1828250 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:10:53:1828250 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.4562s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_False [2025-09-19 09:11:24.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:24.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:24.724] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:24.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:11:24:1828572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:24:1828572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:24:1828570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:24:1828570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:24:1828569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:24:1828569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:24:1828571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:24:1828571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.1522s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_True [2025-09-19 09:11:55.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:55.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:55.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:11:55.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:11:55:1828887 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:55:1828887 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:55:1828889 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:55:1828889 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:55:1828888 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:55:1828888 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:11:55:1828890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:11:55:1828890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.1553s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_False [2025-09-19 09:12:26.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:26.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:26.987] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:26.995] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:12:27:1829207 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:27:1829207 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:27:1829204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:27:1829204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:27:1829205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:27:1829205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:27:1829206 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:27:1829206 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.4561s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_False_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_True [2025-09-19 09:12:58.363] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:58.379] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:58.399] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:12:58.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:12:58:1829525 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:58:1829525 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:58:1829526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:58:1829526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:58:1829524 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:58:1829524 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:12:58:1829527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:12:58:1829527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.3557s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_False [2025-09-19 09:13:29.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:13:29.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:13:29.755] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:13:29.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:13:29:1829841 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:13:29:1829841 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:13:29:1829840 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:13:29:1829840 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:13:29:1829842 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:13:29:1829842 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:13:29:1829843 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:13:29:1829843 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.8577s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_True [2025-09-19 09:14:01.557] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:01.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:01.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:01.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:14:01:1830159 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:01:1830159 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:01:1830160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:01:1830160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:01:1830158 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:01:1830158 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:01:1830157 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:01:1830157 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.7572s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_False [2025-09-19 09:14:33.283] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:33.351] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:33.472] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:14:33.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:14:33:1830477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:33:1830477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:33:1830475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:33:1830475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:33:1830474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:33:1830474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:14:33:1830476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:14:33:1830476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.6564s] [ 59%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_True [2025-09-19 09:15:04.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:05.021] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:05.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:05.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:15:05:1830792 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:05:1830792 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:05:1830795 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:05:1830795 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:05:1830794 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:05:1830794 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:05:1830793 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:05:1830793 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.5389s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_False [2025-09-19 09:15:36.511] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:36.544] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:36.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:15:36.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:15:36:1831114 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:36:1831114 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:36:1831113 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:36:1831113 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:36:1831112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:36:1831112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:15:36:1831111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:15:36:1831111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.9434s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_True [2025-09-19 09:16:08.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:08.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:08.507] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:08.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:16:08:1831433 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:08:1831433 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:08:1831432 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:08:1831430 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:08:1831432 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:08:1831430 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:08:1831431 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:08:1831431 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.7388s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_False [2025-09-19 09:16:40.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:40.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:40.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:16:40.385] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:16:40:1831749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:40:1831749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:40:1831748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:40:1831748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:40:1831750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:40:1831750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:16:40:1831751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:16:40:1831751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6565s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_GradToNone_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_True [2025-09-19 09:17:11.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:11.861] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:11.883] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:11.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:17:12:1832072 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:12:1832072 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:12:1832074 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:12:1832074 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:12:1832073 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:12:1832073 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:12:1832071 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:12:1832071 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.5509s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_False [2025-09-19 09:17:43.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:43.457] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:43.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:17:43.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:17:43:1832389 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:43:1832389 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:43:1832388 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:43:1832388 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:43:1832390 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:43:1832390 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:17:43:1832391 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:17:43:1832391 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.1552s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_False_forward_prefetch_True [2025-09-19 09:18:14.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:14.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:14.592] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:14.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:18:14:1832708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:14:1832708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:14:1832710 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:14:1832710 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:14:1832707 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:14:1832707 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:14:1832709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:14:1832709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6573s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_False [2025-09-19 09:18:46.199] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:46.234] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:46.241] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:18:46.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:18:46:1833024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:46:1833024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:46:1833027 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:46:1833027 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:46:1833025 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:46:1833025 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:18:46:1833026 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:18:46:1833026 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.5569s] [ 84%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_False_disable_autograd_True_forward_prefetch_True [2025-09-19 09:19:17.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:17.797] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:17.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:17.821] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:19:17:1833343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:17:1833343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:17:1833342 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:17:1833342 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:18:1833344 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:18:1833344 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:18:1833345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:18:1833345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.1551s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_False [2025-09-19 09:19:48.929] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:48.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:48.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:19:48.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:19:49:1833664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:49:1833664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:49:1833662 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:49:1833662 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:49:1833661 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:49:1833661 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:19:49:1833663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:19:49:1833663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.5565s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_False_forward_prefetch_True [2025-09-19 09:20:20.537] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:20.560] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:20.561] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:20.584] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:20:20:1833978 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:20:1833978 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:20:1833979 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:20:1833979 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:20:1833981 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:20:1833981 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:20:1833980 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:20:1833980 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.5566s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_False [2025-09-19 09:20:52.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:52.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:52.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:20:52.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:20:52:1834298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:52:1834298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:52:1834297 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:52:1834297 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:52:1834296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:52:1834296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:20:52:1834299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:20:52:1834299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.5571s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_freezing_weights.py::TestFreezingWeights::test_freezing_weights_with_nested_trunk_True_freezing_method_FreezingMethod_RequiresGrad_freeze_after_wrap_fsdp_True_disable_autograd_True_forward_prefetch_True [2025-09-19 09:21:23.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:21:23.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:21:23.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:21:23.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:21:23:1834615 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:21:23:1834615 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:21:23:1834617 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:21:23:1834617 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:21:23:1834618 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:21:23:1834618 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:21:23:1834616 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:21:23:1834616 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.3560s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_freezing_weights.py.xml -
======================= 32 passed in 1010.15s (0:16:50) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:21:55.967] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/fsdp/test_fsdp_fx.py::TestSymbolicTracingXPU::test_symbolic_tracing_outputs_xpu PASSED [0.0140s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_fx.py.xml -
============================== 1 passed in 2.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:21:58.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 6 items
Running 6 items in this shard

../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs0_use_orig_params_False [2025-09-19 09:22:01.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:22:01.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:22:01:1835077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:22:01:1835077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:22:01:1835078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:22:01:1835078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [30.7200s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs0_use_orig_params_True [2025-09-19 09:22:31.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:22:31.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:22:31:1835237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:22:31:1835237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:22:31:1835238 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:22:31:1835238 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
FAILED [30.5475s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs1_use_orig_params_False [2025-09-19 09:23:02.231] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:23:02.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:23:02:1835397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:23:02:1835397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:23:02:1835398 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:23:02:1835398 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [31.8494s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs1_use_orig_params_True [2025-09-19 09:23:34.067] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:23:34.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:23:34:1835557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:23:34:1835557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:23:34:1835556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:23:34:1835556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
FAILED [32.2373s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_cpu_offload_use_orig_params_False [2025-09-19 09:24:06.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:24:06.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:24:06:1835715 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:24:06:1835716 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:24:06:1835716 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:24:06:1835715 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [30.4349s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_cpu_offload_use_orig_params_True [2025-09-19 09:24:36.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:24:36.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:24:36:1835875 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:24:36:1835875 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:24:37:1835876 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:24:37:1835876 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [30.3480s] [100%]

=================================== FAILURES ===================================
___________ TestGradAcc.test_grad_acc_configs0_use_orig_params_False ___________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.655092716217041 at index (126,) (up to 1e-05 allowed)
Greatest relative difference: 5.704293727874756 at index (112,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 228 (14.0%)
Greatest absolute difference: 0.8042259216308594 at index (144,) (up to 1e-05 allowed)
Greatest relative difference: 0.7871339917182922 at index (159,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:21:59.277000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835077
I0919 09:21:59.278000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835078
___________ TestGradAcc.test_grad_acc_configs0_use_orig_params_True ____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 52 / 52 (100.0%)
Greatest absolute difference: 1.697052001953125 at index (43,) (up to 1e-05 allowed)
Greatest relative difference: 0.09329049289226532 at index (16,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:22:29.826000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835237
I0919 09:22:29.826000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835238
___________ TestGradAcc.test_grad_acc_configs1_use_orig_params_False ___________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.8396000862121582 at index (129,) (up to 1e-05 allowed)
Greatest relative difference: 23.024791717529297 at index (186,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 668 / 716 (93.3%)
Greatest absolute difference: 3.7239112854003906 at index (519,) (up to 1e-05 allowed)
Greatest relative difference: 33.97804260253906 at index (440,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:23:00.376000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835397
I0919 09:23:00.376000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835398
___________ TestGradAcc.test_grad_acc_configs1_use_orig_params_True ____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.7380075454711914 at index (102,) (up to 1e-05 allowed)
Greatest relative difference: 19.133033752441406 at index (49,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 52 / 52 (100.0%)
Greatest absolute difference: 2.3607215881347656 at index (43,) (up to 1e-05 allowed)
Greatest relative difference: 0.0874820202589035 at index (0,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:23:32.226000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835556
I0919 09:23:32.226000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835557
_________ TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_False __________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.6550931930541992 at index (126,) (up to 1e-05 allowed)
Greatest relative difference: 15.473516464233398 at index (74,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 228 (14.0%)
Greatest absolute difference: 0.8042201995849609 at index (144,) (up to 1e-05 allowed)
Greatest relative difference: 1.0529930591583252 at index (159,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:24:04.464000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835715
I0919 09:24:04.465000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835716
__________ TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_True __________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.49391043186187744 at index (105,) (up to 1e-05 allowed)
Greatest relative difference: 4.816555500030518 at index (119,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 09:24:34.901000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1835875
I0919 09:24:34.901000 1835005 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1835876
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_grad_acc.py.xml -
=========================== short test summary info ============================
FAILED [30.7200s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs0_use_orig_params_False - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.655092716217041 at index (126,) (up to 1e-05 allowed)
Greatest relative difference: 5.704293727874756 at index (112,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 228 (14.0%)
Greatest absolute difference: 0.8042259216308594 at index (144,) (up to 1e-05 allowed)
Greatest relative difference: 0.7871339917182922 at index (159,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [30.5475s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs0_use_orig_params_True - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 52 / 52 (100.0%)
Greatest absolute difference: 1.697052001953125 at index (43,) (up to 1e-05 allowed)
Greatest relative difference: 0.09329049289226532 at index (16,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs0_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [31.8494s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs1_use_orig_params_False - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.8396000862121582 at index (129,) (up to 1e-05 allowed)
Greatest relative difference: 23.024791717529297 at index (186,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 668 / 716 (93.3%)
Greatest absolute difference: 3.7239112854003906 at index (519,) (up to 1e-05 allowed)
Greatest relative difference: 33.97804260253906 at index (440,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [32.2373s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_configs1_use_orig_params_True - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.7380075454711914 at index (102,) (up to 1e-05 allowed)
Greatest relative difference: 19.133033752441406 at index (49,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 269, in test_grad_acc
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 52 / 52 (100.0%)
Greatest absolute difference: 2.3607215881347656 at index (43,) (up to 1e-05 allowed)
Greatest relative difference: 0.0874820202589035 at index (0,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_configs1_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [30.4349s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_cpu_offload_use_orig_params_False - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.6550931930541992 at index (126,) (up to 1e-05 allowed)
Greatest relative difference: 15.473516464233398 at index (74,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 228 (14.0%)
Greatest absolute difference: 0.8042201995849609 at index (144,) (up to 1e-05 allowed)
Greatest relative difference: 1.0529930591583252 at index (159,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [30.3480s] ../../../../test/distributed/fsdp/test_fsdp_grad_acc.py::TestGradAcc::test_grad_acc_cpu_offload_use_orig_params_True - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 294, in test_grad_acc_cpu_offload
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_grad_acc.py", line 212, in _test_grad_acc
    torch.testing.assert_close(ref_grad, acc_grad)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 192 / 228 (84.2%)
Greatest absolute difference: 0.49391043186187744 at index (105,) (up to 1e-05 allowed)
Greatest relative difference: 4.816555500030518 at index (119,) (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_grad_acc.py TestGradAcc.test_grad_acc_cpu_offload_use_orig_params_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
======================== 6 failed in 188.05s (0:03:08) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:25:07.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 6 items
Running 6 items in this shard

../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_fsdp_hybrid_shard_basic_setup [2025-09-19 09:25:10.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:10.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:10.223] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:10.231] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:25:10:1836117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:10:1836117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:10:1836116 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:10:1836116 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:10:1836114 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:10:1836114 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:10:1836115 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:10:1836115 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:23:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:23:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:23:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:23:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:39:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:40:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:41:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:42:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:43:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:44:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:45:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:46:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:47:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:48:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:49:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836114:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836116:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836115:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836117:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836441:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836435:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836432:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:25:50:1836426:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [43.3725s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_fsdp_hybrid_shard_parity [2025-09-19 09:25:53.291] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:53.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:53.368] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:25:53.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:25:53:1836688 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:53:1836688 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:53:1836687 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:53:1836687 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:53:1836691 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:53:1836691 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:25:53:1836690 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:25:53:1836690 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:26:22:1836687:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1836688:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1836690:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1836691:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1837010:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1837005:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1837004:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:22:1836999:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1836687:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1836688:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1836690:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1836691:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1837010:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1837005:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1837004:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:23:1836999:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1836687:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1836690:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1836688:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1836691:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1837010:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1837005:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1837004:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:26:24:1836999:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
FAILED [300.0246s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_hsdp_save_load_state_dict [2025-09-19 09:30:53.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:30:53.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:30:53.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:30:53.463] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:30:53:1837064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:30:53:1837064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:30:53:1837063 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:30:53:1837063 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:30:53:1837065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:30:53:1837065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:30:53:1837066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:30:53:1837066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:06:1837064:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:06:1837063:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:06:1837065:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:06:1837066:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:21:1837392:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:21:1837386:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:22:1837383:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:22:1837381:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.8563s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_hsdp_sync_module_state [2025-09-19 09:31:25.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:25.277] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:25.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:25.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:31:25:1837399 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:25:1837399 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:25:1837402 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:25:1837402 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:25:1837400 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:25:1837400 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:25:1837401 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:25:1837401 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:38:1837399:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837400:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837402:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837401:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837400:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837402:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837399:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-09:31:38:1837401:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [16.1271s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_invalid_pg_specification_raises [2025-09-19 09:31:41.347] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:41.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:41.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:41.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:31:41:1837720 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:41:1837720 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:41:1837717 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:41:1837717 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:41:1837719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:41:1837719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:41:1837718 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:41:1837718 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [15.7284s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_raises_manual_wrap_hybrid_shard_when_none_policy [2025-09-19 09:31:57.089] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:57.097] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:57.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:31:57.108] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:31:57:1838022 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:57:1838022 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:57:1838023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:57:1838023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:57:1838021 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:57:1838021 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:31:57:1838024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:31:57:1838024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [15.5294s] [100%]

=================================== FAILURES ===================================
______________ TestFSDPHybridShard.test_fsdp_hybrid_shard_parity _______________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.02004051208496 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 09:25:51.481000 1836042 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1836687
I0919 09:25:51.482000 1836042 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1836688
I0919 09:25:51.482000 1836042 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1836690
I0919 09:25:51.483000 1836042 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1836691
E0919 09:30:51.498000 1836042 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fc3b2ffd640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fc3b37fe640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fc3b3fff640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fc3bcebf640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007fc3bffff640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fc4f7204e00 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/xpu/streams.py", line 163 in synchronize
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 299 in _unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 417 in _pre_forward_unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 382 in _pre_forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 837 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 628 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 280 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 314 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 851 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 375 in _test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 341 in test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1237 in _run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Process 2 timed out with traceback: 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f639d3dd640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f639cbdc640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f639dbde640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f639e3df640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f63a5cbf640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f64dc729e00 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/xpu/streams.py", line 163 in synchronize
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 299 in _unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 417 in _pre_forward_unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 382 in _pre_forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 837 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 628 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 280 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 314 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 851 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 375 in _test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 341 in test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1237 in _run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Process 3 timed out with traceback: 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f67811dd640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f67809dc640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f67819de640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f67821df640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f6791abf640 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f68c4507e00 (most recent call first):
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/xpu/streams.py", line 163 in synchronize
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 299 in _unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 417 in _pre_forward_unshard
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_runtime_utils.py", line 382 in _pre_forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 837 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 628 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 280 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 314 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 851 in forward
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1786 in _call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1775 in _wrapped_call_impl
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 375 in _test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188 in run_subtests
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_hybrid_shard.py", line 341 in test_fsdp_hybrid_shard_parity
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1237 in _run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 09:30:51.501000 1836042 site-packages/torch/testing/_internal/common_distributed.py:965] 
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_hybrid_shard.py.xml -
=========================== short test summary info ============================
FAILED [300.0246s] ../../../../test/distributed/fsdp/test_fsdp_hybrid_shard.py::TestFSDPHybridShard::test_fsdp_hybrid_shard_parity - RuntimeError: Process 0 terminated or timed out after 300.02004051208496 seconds
=================== 1 failed, 5 passed in 424.54s (0:07:04) ====================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:32:13.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_diff_ignored_modules_across_ranks [2025-09-19 09:32:15.656] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:32:15.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:32:15:1838397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:32:15:1838397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:32:15:1838398 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:32:15:1838398 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.3453s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_modules_invalid [2025-09-19 09:32:45.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:32:45.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:32:45:1838556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:32:45:1838556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:32:45:1838557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:32:45:1838557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8248s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_modules_nested [2025-09-19 09:33:00.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:33:00.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:33:00:1838709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:33:00:1838709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:33:00:1838708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:33:00:1838708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1502s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_modules_not_under_wrapped_root_ignore_modules_False [2025-09-19 09:33:30.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:33:30.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:33:30:1838869 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:33:30:1838869 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:33:30:1838868 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:33:30:1838868 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.0477s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_modules_not_under_wrapped_root_ignore_modules_True [2025-09-19 09:34:00.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:34:00.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:34:00:1839028 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:34:00:1839028 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:34:01:1839029 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:34:01:1839029 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2478s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_modules_transformer [2025-09-19 09:34:31.072] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:34:31.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:34:31:1839187 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:34:31:1839187 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:34:31:1839186 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:34:31:1839186 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.7494s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_states_auto_wrap [2025-09-19 09:35:01.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:35:01.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:35:02:1839347 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:02:1839347 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:35:02:1839348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:02:1839348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8250s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_ignored_modules.py::TestFSDPIgnoredModules::test_ignored_states_check [2025-09-19 09:35:16.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:35:16.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:35:16:1839499 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:16:1839499 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:35:16:1839498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:16:1839498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8250s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_ignored_modules.py.xml -
======================== 8 passed in 197.93s (0:03:17) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:35:32.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_fsdp_input.py::TestInputXPU::test_input_type_dict_xpu [2025-09-19 09:35:34.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:35:34:1839723 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:34:1839723 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.4084s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_input.py::TestInputXPU::test_input_type_list_xpu [2025-09-19 09:35:37.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:35:38:1839801 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:38:1839801 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.3067s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_input.py.xml -
============================== 2 passed in 8.82s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:35:42.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_fsdp_memory.py::TestFSDPMemory::test_fsdp_memory_ckpt_ckpt [2025-09-19 09:35:44.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:35:44.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:35:44:1839953 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:44:1839953 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:35:44:1839954 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:35:44:1839954 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [36.7673s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_memory.py::TestFSDPMemory::test_fsdp_memory_ckpt_no_ckpt [2025-09-19 09:36:20.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:36:21.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:36:21:1840304 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:36:21:1840304 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:36:21:1840303 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:36:21:1840303 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [36.5576s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_memory.py.xml -
========================= 2 passed in 75.31s (0:01:15) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:36:58.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 15 items
Running 15 items in this shard

../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_bad_arg_meta [2025-09-19 09:37:00.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:37:00.523] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:37:00:1840727 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:00:1840727 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:37:00:1840728 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:00:1840728 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0214s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_bad_arg_torchdistx SKIPPED [0.0003s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_meta_device_with_mixed_precision [2025-09-19 09:37:15.325] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:37:15.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:37:15:1840877 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:15:1840877 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:37:15:1840878 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:15:1840878 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8253s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_meta_device_default_init_auto_wrap_False [2025-09-19 09:37:30.199] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:37:30.220] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:37:30:1841028 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:30:1841028 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:37:30:1841027 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:37:30:1841027 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2484s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_meta_device_default_init_auto_wrap_True [2025-09-19 09:38:00.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:38:00.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:38:00:1841186 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:38:00:1841186 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:38:00:1841187 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:38:00:1841187 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2488s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_meta_device_reset_params_auto_wrap_False [2025-09-19 09:38:30.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:38:30.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:38:30:1841346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:38:30:1841346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:38:30:1841345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:38:30:1841345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1399s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_meta_device_reset_params_auto_wrap_True [2025-09-19 09:39:00.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:39:00.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:39:01:1841505 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:39:01:1841505 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:39:01:1841506 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:39:01:1841506 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1472s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_torchdistX_default_init_auto_wrap_False SKIPPED [0.0002s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_torchdistX_default_init_auto_wrap_True SKIPPED [0.0003s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_torchdistX_init_fn_auto_wrap_False SKIPPED [0.0001s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_nested_model_with_torchdistX_init_fn_auto_wrap_True SKIPPED [0.0001s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_simple_model_with_meta_device_default_init [2025-09-19 09:39:30.952] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:39:30.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:39:31:1841664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:39:31:1841664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:39:31:1841665 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:39:31:1841665 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2357s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_simple_model_with_meta_device_reset_params [2025-09-19 09:40:01.196] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:40:01.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:40:01:1841824 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:01:1841824 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:40:01:1841825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:01:1841825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2484s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_simple_model_with_torchdistX_default_init SKIPPED [0.0002s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_meta.py::TestFSDPWithMetaDevice::test_simple_model_with_torchdistX_init_fn SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_meta.py.xml -
=================== 8 passed, 7 skipped in 213.03s (0:03:33) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:40:32.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 28 items / 1 deselected / 27 selected
Running 27 items in this shard

../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_cpu_init_with_sync_module_states [2025-09-19 09:40:34.823] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:40:34.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:40:35:1842060 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:35:1842060 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:40:35:1842059 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:35:1842059 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2037s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_cpu_init_stays_on_cpu [2025-09-19 09:40:49.793] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:40:49.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:40:50:1842210 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:50:1842210 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:40:50:1842209 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:40:50:1842209 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [29.9446s] [  7%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_cpu_training [2025-09-19 09:41:19.800] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:41:19.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:41:20:1842368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:20:1842368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:41:20:1842369 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:20:1842369 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1255s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_device_id_use_index_False [2025-09-19 09:41:34.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:41:34.899] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:41:35:1842535 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:35:1842535 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:41:35:1842534 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:35:1842534 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0256s] [ 14%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_device_id_use_index_True [2025-09-19 09:41:49.987] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:41:49.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:41:50:1842684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:50:1842685 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:41:50:1842684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:41:50:1842685 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1245s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_module_no_compute_grad_use_second_layer_False_sharding_strategy0 [2025-09-19 09:42:05.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:42:05.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:42:05:1842836 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:05:1842836 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:42:05:1842835 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:05:1842835 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1252s] [ 22%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_module_no_compute_grad_use_second_layer_False_sharding_strategy1 [2025-09-19 09:42:20.239] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:42:20.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:42:20:1842994 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:20:1842993 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:20:1842993 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:42:20:1842994 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1469s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_module_no_compute_grad_use_second_layer_True_sharding_strategy0 [2025-09-19 09:42:50.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:42:50.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:42:50:1843153 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:50:1843153 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:42:50:1843154 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:42:50:1843154 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1254s] [ 29%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_module_no_compute_grad_use_second_layer_True_sharding_strategy1 [2025-09-19 09:43:05.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:43:05.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:43:05:1843313 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:43:05:1843313 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:43:05:1843312 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:43:05:1843312 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.0476s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_not_all_outputs_used_in_loss [2025-09-19 09:43:35.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:43:35.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:43:35:1843471 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:43:35:1843471 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:43:35:1843472 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:43:35:1843472 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2481s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_optim_overlap_no_use_orig_params_error [2025-09-19 09:44:05.803] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:44:05.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:44:06:1843632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:44:06:1843632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:44:06:1843631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:44:06:1843631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0222s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiProcess::test_fsdp_zero2_eval_with_prefetch [2025-09-19 09:44:20.755] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:44:20.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:44:20:1843782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:44:20:1843782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:44:20:1843783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:44:20:1843783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [66.6019s] [ 44%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_cpu_gpu_module SKIPPED [0.0003s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_device_id_auto_wrap SKIPPED [0.0001s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_device_id_cpu_offload SKIPPED [0.0012s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_device_id_no_move_ignored_params_and_bufs SKIPPED [0.0001s] [ 59%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_ignored_module_meta SKIPPED [0.0001s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_namedtuple SKIPPED [0.0001s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_same_model_across_ranks SKIPPED [0.0001s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_fsdp_unsupported_module_cls SKIPPED [0.0001s] [ 74%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_homogeneous_attributes SKIPPED [0.0001s] [ 77%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_module_device_mismatches_device_id SKIPPED [0.0001s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_multigpu_module SKIPPED [0.0002s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscMultiThread::test_no_params SKIPPED [0.0001s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscWorldSize1::test_training_device_mismatch_errors SKIPPED [0.0001s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscWorldSize1::test_unsafe_setattr SKIPPED [0.0001s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_misc.py::TestFSDPMiscWorldSize1::test_world_size_1_sharding_strategy_warning SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_misc.py.xml -
=========== 12 passed, 15 skipped, 1 deselected in 294.83s (0:04:54) ===========
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 09:45:28.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 58 items
Running 58 items in this shard

../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_buffer_dtype_no_root_handle [2025-09-19 09:45:30.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:45:30.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:45:30:1844017 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:45:30:1844017 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:45:30:1844016 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:45:30:1844016 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.7942s] [  1%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_eval_root_cast_inputs [2025-09-19 09:45:46.071] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:45:46.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:45:46:1844175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:45:46:1844175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:45:46:1844176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:45:46:1844176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.4424s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_full_precision_in_eval [2025-09-19 09:46:16.501] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:46:16.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:46:16:1844333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:46:16:1844333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:46:16:1844334 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:46:16:1844334 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.9491s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_full_precision_in_eval_buffers [2025-09-19 09:46:47.371] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:46:47.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:46:47:1844493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:46:47:1844493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:46:47:1844494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:46:47:1844494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1247s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_full_precision_in_eval_comm [2025-09-19 09:47:02.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:47:02.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:47:02:1844646 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:02:1844646 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:47:02:1844645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:02:1844645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.4485s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_grads_reduced_precision [2025-09-19 09:47:32.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:47:32.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:47:33:1844804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:33:1844804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:47:33:1844805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:33:1844805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2262s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_input_grads_with_param_mixed_precision [2025-09-19 09:47:48.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:47:48.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:47:48:1844963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:48:1844963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:47:48:1844964 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:47:48:1844964 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [38.7603s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_false_fp32_enable_sharded_grad_scaler [2025-09-19 09:48:26.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:48:26.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:48:27:1845124 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:27:1845124 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:48:27:1845123 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:27:1845123 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.7261s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_false_fp32_none [2025-09-19 09:48:42.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:48:42.712] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:48:43:1845281 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:43:1845282 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:43:1845281 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:48:43:1845282 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.7260s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_false_fp64_enable_sharded_grad_scaler [2025-09-19 09:48:58.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:48:58.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:48:58:1845443 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:58:1845443 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:48:58:1845442 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:48:58:1845442 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3254s] [ 17%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_false_fp64_none [2025-09-19 09:49:13.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:49:13.727] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:49:14:1845600 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:14:1845601 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:14:1845600 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:49:14:1845601 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4194s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_true_fp32_enable_sharded_grad_scaler [2025-09-19 09:49:29.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:49:29.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:49:29:1845761 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:29:1845761 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:49:29:1845760 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:29:1845760 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5260s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_true_fp32_none [2025-09-19 09:49:44.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:49:44.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:49:45:1845918 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:45:1845918 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:49:45:1845919 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:49:45:1845919 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.6262s] [ 22%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_true_fp64_enable_sharded_grad_scaler [2025-09-19 09:50:00.313] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:50:00.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:50:00:1846078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:00:1846078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:50:00:1846077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:00:1846077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3258s] [ 24%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_fp16_offload_true_fp64_none [2025-09-19 09:50:15.621] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:50:15.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:50:15:1846237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:15:1846237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:50:15:1846236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:15:1846236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.4239s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_false_fp32_enable_sharded_grad_scaler [2025-09-19 09:50:31.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:50:31.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:50:31:1846397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:31:1846397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:50:31:1846396 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:50:31:1846396 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2433s] [ 27%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_false_fp32_none [2025-09-19 09:51:01.304] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:51:01.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:51:01:1846557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:01:1846557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:51:01:1846556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:01:1846556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2472s] [ 29%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_false_fp64_enable_sharded_grad_scaler [2025-09-19 09:51:31.507] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:51:31.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:51:31:1846718 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:31:1846718 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:51:31:1846717 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:31:1846717 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2260s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_false_fp64_none [2025-09-19 09:51:46.869] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:51:46.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:51:47:1846876 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:47:1846875 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:51:47:1846875 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:51:47:1846876 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.4261s] [ 32%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_true_fp32_enable_sharded_grad_scaler [2025-09-19 09:52:02.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:52:02.251] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:52:02:1847036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:52:02:1847036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:52:02:1847035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:52:02:1847035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2480s] [ 34%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_true_fp32_none [2025-09-19 09:52:32.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:52:32.448] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:52:32:1847196 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:52:32:1847196 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:52:32:1847195 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:52:32:1847195 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.3471s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_true_fp64_enable_sharded_grad_scaler [2025-09-19 09:53:02.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:53:02.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:53:03:1847355 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:03:1847355 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:53:03:1847356 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:03:1847356 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2255s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_no_mp_offload_true_fp64_none [2025-09-19 09:53:18.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:53:18.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:53:18:1847513 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:18:1847513 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:53:18:1847514 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:18:1847514 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2259s] [ 39%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_false_fp32_enable_sharded_grad_scaler [2025-09-19 09:53:33.245] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:53:33.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:53:33:1847673 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:33:1847673 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:53:33:1847672 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:33:1847672 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.7267s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_false_fp32_none [2025-09-19 09:53:48.991] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:53:49.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:53:49:1847831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:49:1847831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:53:49:1847830 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:53:49:1847830 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5268s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_false_fp64_enable_sharded_grad_scaler [2025-09-19 09:54:04.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:54:04.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:54:04:1847990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:04:1847990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:54:04:1847991 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:04:1847991 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5262s] [ 44%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_false_fp64_none [2025-09-19 09:54:20.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:54:20.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:54:20:1848148 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:20:1848148 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:54:20:1848149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:20:1848149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5260s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_true_fp32_enable_sharded_grad_scaler [2025-09-19 09:54:35.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:54:35.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:54:35:1848307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:35:1848307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:54:35:1848306 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:35:1848306 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.6261s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_true_fp32_none [2025-09-19 09:54:51.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:54:51.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:54:51:1848465 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:51:1848466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:54:51:1848465 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:54:51:1848466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5253s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_true_fp64_enable_sharded_grad_scaler [2025-09-19 09:55:06.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:55:06.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:55:07:1848624 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:07:1848624 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:55:07:1848625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:07:1848625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5254s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_param_and_buf_offload_true_fp64_none [2025-09-19 09:55:22.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:55:22.227] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:55:22:1848783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:22:1848783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:55:22:1848782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:22:1848782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3254s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_false_fp32_enable_sharded_grad_scaler [2025-09-19 09:55:37.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:55:37.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:55:37:1848941 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:37:1848941 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:55:37:1848942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:37:1848942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5262s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_false_fp32_none [2025-09-19 09:55:53.097] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:55:53.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:55:53:1849099 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:53:1849099 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:55:53:1849100 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:55:53:1849100 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5265s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_false_fp64_enable_sharded_grad_scaler [2025-09-19 09:56:08.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:56:08.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:56:08:1849259 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:08:1849259 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:56:08:1849258 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:08:1849258 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3257s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_false_fp64_none [2025-09-19 09:56:24.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:56:24.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:56:24:1849417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:24:1849417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:56:24:1849418 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:24:1849418 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5260s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_true_fp32_enable_sharded_grad_scaler [2025-09-19 09:56:39.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:56:39.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:56:39:1849579 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:39:1849579 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:56:39:1849578 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:39:1849578 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5257s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_true_fp32_none [2025-09-19 09:56:55.011] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:56:55.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:56:55:1849736 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:55:1849737 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:56:55:1849736 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:56:55:1849737 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5261s] [ 63%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_true_fp64_enable_sharded_grad_scaler [2025-09-19 09:57:10.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:57:10.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:57:10:1849897 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:10:1849896 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:10:1849897 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:57:10:1849896 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4248s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_e2e_full_shard_mp_only_reduce_offload_true_fp64_none [2025-09-19 09:57:26.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:57:26.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:57:26:1850055 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:26:1850055 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:57:26:1850056 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:26:1850056 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.5264s] [ 67%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_no_reshard_after_forward [2025-09-19 09:57:41.463] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:57:41.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:57:41:1850214 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:41:1850214 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:57:41:1850215 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:41:1850215 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2257s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mixed_precision_resnet [2025-09-19 09:57:56.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:57:56.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:57:57:1850374 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:57:1850374 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:57:57:1850373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:57:57:1850373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [35.3551s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_batchnorm_convert_sync_bn_False [2025-09-19 09:58:32.035] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:58:32.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:58:32:1850532 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:58:32:1850532 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:58:32:1850533 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:58:32:1850533 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2487s] [ 72%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_batchnorm_convert_sync_bn_True [2025-09-19 09:59:02.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:59:02.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:59:02:1850693 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:02:1850693 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:59:02:1850692 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:02:1850692 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.5489s] [ 74%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_embedding_default [2025-09-19 09:59:32.865] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:59:32.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:59:33:1850851 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:33:1850851 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:59:33:1850852 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:33:1850852 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.8267s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_embedding_only_params_and_bufs [2025-09-19 09:59:48.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 09:59:48.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-09:59:49:1851011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:49:1851011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-09:59:49:1851012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-09:59:49:1851012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.8266s] [ 77%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_embedding_params_and_reduce_diff [2025-09-19 10:00:04.515] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:00:04.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:04:1851169 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:04:1851169 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:00:04:1851170 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:04:1851170 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [26.9433s] [ 79%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionSharded::test_mp_embedding_reduce [2025-09-19 10:00:31.423] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:00:31.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:31:1851328 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:31:1851328 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:00:31:1851327 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:31:1851327 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.4196s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionUnsharded::test_grads_reduced_precision [2025-09-19 10:00:46.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:47:1851487 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:47:1851487 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.4068s] [ 82%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionUnsharded::test_mixed_precision_e2e_full_shard [2025-09-19 10:00:50.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:50:1851565 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:50:1851565 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.4069s] [ 84%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionUnsharded::test_mixed_precision_no_reshard_after_forward [2025-09-19 10:00:53.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:54:1851643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:54:1851643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.4071s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPMixedPrecisionIgnoredModules::test_mixed_precision_with_ignored_module [2025-09-19 10:00:57.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:00:57:1851721 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:00:57:1851721 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.2057s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_float16_on_one_submodule [2025-09-19 10:01:00.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:01:00.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:01:00:1851796 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:00:1851796 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:01:00:1851797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:00:1851797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [26.7424s] [ 89%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_float16_on_one_submodule_skip_inputs [2025-09-19 10:01:27.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:01:27.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:01:27:1851956 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:27:1851956 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:01:27:1851955 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:27:1851955 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [26.8427s] [ 91%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_float16_on_one_submodule_skip_inputs_error [2025-09-19 10:01:53.902] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:01:53.909] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:01:54:1852118 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:54:1852118 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:01:54:1852117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:01:54:1852117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1249s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_submodules_with_different_precisions [2025-09-19 10:02:09.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:02:09.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:02:09:1852269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:09:1852269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:02:09:1852270 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:09:1852270 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [26.7419s] [ 94%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_submodules_with_different_precisions_error [2025-09-19 10:02:35.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:02:35.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:02:36:1852429 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:36:1852429 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:02:36:1852428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:36:1852428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1259s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPDifferentSubmodulePrecision::test_submodules_with_external_inputs [2025-09-19 10:02:50.991] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:02:50.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:02:51:1852581 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:51:1852580 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:02:51:1852580 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:02:51:1852581 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.5252s] [ 98%]
../../../../test/distributed/fsdp/test_fsdp_mixed_precision.py::TestFSDPTrainEval::test_train_ema_eval_flow [2025-09-19 10:03:06.435] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:06.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:03:06:1852741 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:06:1852741 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:06:1852740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:06:1852740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
FullyShardedDataParallel(
  (_fsdp_wrapped_module): TransformerWithEMA(
    (module): FullyShardedDataParallel(
      (_fsdp_wrapped_module): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerDecoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (dropout3): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ema_module): AveragedModel(
      (module): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Transformer(
          (encoder): TransformerEncoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (decoder): TransformerDecoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerDecoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (multihead_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                  (dropout3): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): TransformerWithEMA(
    (module): FullyShardedDataParallel(
      (_fsdp_wrapped_module): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerDecoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (dropout3): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ema_module): AveragedModel(
      (module): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Transformer(
          (encoder): TransformerEncoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (decoder): TransformerDecoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerDecoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (multihead_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                  (dropout3): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
dist init r=1, world=2
FullyShardedDataParallel(
  (_fsdp_wrapped_module): TransformerWithEMA(
    (module): FullyShardedDataParallel(
      (_fsdp_wrapped_module): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-5): 6 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): TransformerDecoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (multihead_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                )
                (linear1): Linear(in_features=512, out_features=2048, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
                (linear2): Linear(in_features=2048, out_features=512, bias=True)
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (dropout3): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ema_module): AveragedModel(
      (module): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Transformer(
          (encoder): TransformerEncoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerEncoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (decoder): TransformerDecoder(
            (layers): ModuleList(
              (0-5): 6 x FullyShardedDataParallel(
                (_fsdp_wrapped_module): TransformerDecoderLayer(
                  (self_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (multihead_attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
                  )
                  (linear1): Linear(in_features=512, out_features=2048, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (linear2): Linear(in_features=2048, out_features=512, bias=True)
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (dropout1): Dropout(p=0.1, inplace=False)
                  (dropout2): Dropout(p=0.1, inplace=False)
                  (dropout3): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
)
PASSED [16.9284s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_mixed_precision.py.xml -
======================= 58 passed in 1075.17s (0:17:55) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:03:24.387] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_fsdp_multiple_forward.py::TestMultiForwardCPU::test_multi_forward_cpu [2025-09-19 10:03:26.543] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:26.552] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:26.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:26.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:03:26:1852976 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:26:1852976 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:26:1852974 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:26:1852974 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:26:1852977 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:26:1852977 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:26:1852975 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:26:1852975 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.2404s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_multiple_forward.py::TestMultiForwardXPU::test_multi_forward_xpu [2025-09-19 10:03:57.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:57.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:57.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:03:57.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:03:58:1853292 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:58:1853292 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:58:1853295 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:58:1853295 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:58:1853293 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:58:1853293 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:03:58:1853294 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:03:58:1853294 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.2391s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_multiple_forward.py.xml -
========================= 2 passed in 64.66s (0:01:04) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:04:29.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/fsdp/test_fsdp_multiple_wrapping.py::TestMultipleWrappingXPU::test_multiple_wrapping_xpu [2025-09-19 10:04:32.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:04:32.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:04:32.191] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:04:32.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:04:32:1853684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:04:32:1853684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:04:32:1853683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:04:32:1853683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:04:32:1853686 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:04:32:1853686 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:04:32:1853685 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:04:32:1853685 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.1555s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_multiple_wrapping.py.xml -
============================== 1 passed in 33.34s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:05:04.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 60 items
Running 60 items in this shard

../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_compatible_with_trec [2025-09-19 10:05:06.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:06.703] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:06.708] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:06.757] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:05:06:1854077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:06:1854077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:06:1854076 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:06:1854076 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:06:1854079 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:06:1854079 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:06:1854078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:06:1854078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.9376s] [  1%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_flatten_sharded_optim_state_dict_nested [2025-09-19 10:05:38.384] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:38.385] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:38.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:05:38.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:05:38:1854393 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:38:1854393 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:38:1854396 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:38:1854396 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:38:1854395 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:38:1854395 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:05:38:1854394 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:05:38:1854394 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.2555s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_flatten_sharded_optim_state_dict_transformer [2025-09-19 10:06:09.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:09.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:09.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:09.735] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:06:09:1854713 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:09:1854713 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:09:1854715 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:09:1854715 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:09:1854712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:09:1854712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:09:1854714 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:09:1854714 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [32.4566s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_full_optim_state_dict_keys [2025-09-19 10:06:42.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:42.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:42.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:06:42.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:06:42:1855034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:42:1855034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:42:1855033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:42:1855033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:42:1855036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:42:1855036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:06:42:1855035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:06:42:1855035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.4560s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_full_optim_state_dict_nested_invalid [2025-09-19 10:07:13.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:13.585] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:13.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:13.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:07:13:1855355 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:13:1855355 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:13:1855354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:13:1855354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:13:1855352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:13:1855352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:13:1855353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:13:1855353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.7489s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_interface_arguments [2025-09-19 10:07:45.299] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:45.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:45.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:07:45.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:07:45:1855669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:45:1855669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:45:1855671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:45:1855671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:45:1855672 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:45:1855672 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:07:45:1855670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:07:45:1855670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.7386s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_no_grad [2025-09-19 10:08:17.044] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:17.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:17.063] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:17.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:08:17:1855988 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:17:1855988 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:17:1855989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:17:1855989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:17:1855987 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:17:1855987 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:17:1855990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:17:1855990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.8389s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_input_warning [2025-09-19 10:08:48.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:48.908] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:48.908] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:08:48.908] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:08:49:1856308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:49:1856308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:49:1856306 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:49:1856306 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:49:1856307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:49:1856307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:08:49:1856305 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:08:49:1856305 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.6480s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_False_rank0_only_False_use_diff_optim_inputs_False [2025-09-19 10:09:20.519] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:20.552] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:20.553] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:20.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:09:20:1856626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:20:1856626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:20:1856624 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:20:1856624 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:20:1856625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:20:1856625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:20:1856623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:20:1856623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.1474s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_False_rank0_only_False_use_diff_optim_inputs_True [2025-09-19 10:09:51.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:51.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:51.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:09:51.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:09:51:1856943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:51:1856943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:51:1856941 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:51:1856941 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:51:1856944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:51:1856944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:09:52:1856942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:09:52:1856942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.7482s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_False_rank0_only_True_use_diff_optim_inputs_False [2025-09-19 10:10:23.399] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:23.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:23.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:23.519] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:10:23:1857260 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:23:1857260 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:23:1857261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:23:1857261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:23:1857258 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:23:1857258 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:23:1857259 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:23:1857259 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6476s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_False_rank0_only_True_use_diff_optim_inputs_True [2025-09-19 10:10:55.071] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:55.093] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:55.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:10:55.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:10:55:1857577 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:55:1857577 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:55:1857578 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:55:1857578 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:55:1857579 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:55:1857579 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:10:55:1857580 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:10:55:1857580 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.6473s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_True_rank0_only_False_use_diff_optim_inputs_False [2025-09-19 10:11:26.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:26.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:26.780] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:26.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:11:26:1857900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:26:1857900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:27:1857897 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:27:1857897 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:27:1857899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:27:1857899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:27:1857898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:27:1857898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.5456s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_True_rank0_only_False_use_diff_optim_inputs_True [2025-09-19 10:11:58.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:58.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:58.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:11:58.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:11:58:1858217 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:58:1858217 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:58:1858218 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:58:1858218 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:58:1858216 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:58:1858216 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:11:58:1858219 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:11:58:1858219 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.6390s] [ 23%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_True_rank0_only_True_use_diff_optim_inputs_False [2025-09-19 10:12:29.919] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:12:29.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:12:29.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:12:29.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:12:30:1858537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:12:30:1858537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:12:30:1858535 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:12:30:1858535 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:12:30:1858538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:12:30:1858538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:12:30:1858536 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:12:30:1858536 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.1477s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type0_use_multiple_param_groups_True_rank0_only_True_use_diff_optim_inputs_True [2025-09-19 10:13:01.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:01.119] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:01.140] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:01.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:13:01:1858856 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:01:1858856 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:01:1858857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:01:1858857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:01:1858855 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:01:1858855 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:01:1858858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:01:1858858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.5476s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_False_rank0_only_False_use_diff_optim_inputs_False [2025-09-19 10:13:32.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:32.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:32.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:13:32.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:13:32:1859172 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:32:1859172 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:32:1859175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:32:1859175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:32:1859174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:32:1859174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:13:32:1859173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:13:32:1859173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.7465s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_False_rank0_only_False_use_diff_optim_inputs_True [2025-09-19 10:14:04.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:04.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:04.379] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:04.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:14:04:1859492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:04:1859492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:04:1859491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:04:1859491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:04:1859493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:04:1859493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:04:1859490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:04:1859490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.7485s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_False_rank0_only_True_use_diff_optim_inputs_False [2025-09-19 10:14:36.096] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:36.096] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:36.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:36.137] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:14:36:1859809 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:36:1859809 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:36:1859811 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:36:1859811 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:36:1859808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:36:1859808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:36:1859810 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:36:1859810 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [15.8263s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_False_rank0_only_True_use_diff_optim_inputs_True [2025-09-19 10:14:51.939] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:51.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:51.955] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:14:51.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:14:52:1860111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:52:1860111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:52:1860112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:52:1860112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:52:1860109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:52:1860109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:14:52:1860110 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:14:52:1860110 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [15.7259s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_True_rank0_only_False_use_diff_optim_inputs_False [2025-09-19 10:15:07.672] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:07.689] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:07.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:07.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:15:07:1860412 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:07:1860412 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:07:1860413 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:07:1860413 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:07:1860415 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:07:1860415 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:07:1860414 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:07:1860414 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.5459s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_True_rank0_only_False_use_diff_optim_inputs_True [2025-09-19 10:15:39.223] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:39.224] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:39.363] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:15:39.369] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:15:39:1860730 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:39:1860730 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:39:1860729 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:39:1860729 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:39:1860728 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:39:1860728 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:15:39:1860731 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:15:39:1860731 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.2487s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_True_rank0_only_True_use_diff_optim_inputs_False [2025-09-19 10:16:10.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:10.505] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:10.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:10.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:16:10:1861048 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:10:1861048 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:10:1861049 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:10:1861049 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:10:1861050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:10:1861050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:10:1861047 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:10:1861047 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.7266s] [ 38%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_dict_nested_state_dict_type1_use_multiple_param_groups_True_rank0_only_True_use_diff_optim_inputs_True [2025-09-19 10:16:26.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:26.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:26.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:26.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:16:26:1861350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:26:1861350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:26:1861347 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:26:1861347 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:26:1861349 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:26:1861349 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:26:1861348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:26:1861348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [15.7296s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_optim_state_without_param_groups [2025-09-19 10:16:41.923] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:41.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:41.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:16:41.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:16:42:1861650 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:42:1861650 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:42:1861648 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:42:1861648 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:42:1861649 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:42:1861649 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:16:42:1861647 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:16:42:1861647 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.5536s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_rekey_optim_state_dict_to_ids_state_dict_type0_use_multiple_param_groups_False [2025-09-19 10:17:13.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:13.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:13.517] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:13.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:17:13:1861970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:13:1861970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:13:1861971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:13:1861971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:13:1861972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:13:1861972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:13:1861969 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:13:1861969 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6568s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_rekey_optim_state_dict_to_ids_state_dict_type0_use_multiple_param_groups_True [2025-09-19 10:17:45.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:45.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:45.156] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:17:45.156] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:17:45:1862290 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:45:1862290 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:45:1862287 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:45:1862287 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:45:1862288 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:45:1862288 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:17:45:1862289 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:17:45:1862289 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.2552s] [ 45%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_rekey_optim_state_dict_to_ids_state_dict_type1_use_multiple_param_groups_False [2025-09-19 10:18:16.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:16.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:16.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:16.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:18:16:1862607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:16:1862607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:16:1862608 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:16:1862608 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:16:1862606 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:16:1862606 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:16:1862605 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:16:1862605 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.6574s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_rekey_optim_state_dict_to_ids_state_dict_type1_use_multiple_param_groups_True [2025-09-19 10:18:48.060] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:48.060] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:48.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:18:48.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:18:48:1862923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:48:1862923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:48:1862926 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:48:1862926 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:48:1862924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:48:1862924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:18:48:1862925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:18:48:1862925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.7569s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_rekey_optim_state_dict_to_names [2025-09-19 10:19:19.815] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:19.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:19.823] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:19.829] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:19:20:1863243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:20:1863243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:20:1863245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:20:1863245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:20:1863242 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:20:1863242 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:20:1863244 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:20:1863244 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.7567s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_save_load_without_0th_param_state_state_dict_type0 [2025-09-19 10:19:51.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:51.621] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:51.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:19:51.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:19:51:1863559 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:51:1863559 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:51:1863561 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:51:1863561 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:51:1863558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:51:1863558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:19:51:1863560 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:19:51:1863560 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [30.8538s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_save_load_without_0th_param_state_state_dict_type1 [2025-09-19 10:20:22.496] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:22.496] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:22.507] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:22.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:20:22:1863882 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:22:1863882 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:22:1863880 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:22:1863880 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:22:1863881 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:22:1863881 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:22:1863883 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:22:1863883 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.1550s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_halve_world_size [2025-09-19 10:20:53.547] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:53.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:53.608] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:20:53.628] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:20:53:1864199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:53:1864199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:53:1864200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:53:1864200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:53:1864198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:53:1864198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:20:53:1864197 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:20:53:1864197 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:22:1864199:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:21:22:1864197:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:21:22:1864199:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:21:22:1864197:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:21:23:1864199:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:21:23:1864197:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [32.5564s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_False_use_diff_optim_inputs_False [2025-09-19 10:21:26.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:26.185] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:26.207] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:26.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:21:26:1864529 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:26:1864529 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:26:1864530 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:26:1864530 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:26:1864528 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:26:1864528 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:26:1864531 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:26:1864531 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.5552s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_False_use_diff_optim_inputs_True [2025-09-19 10:21:57.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:57.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:57.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:21:57.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:21:57:1864848 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:57:1864848 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:57:1864849 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:57:1864849 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:57:1864850 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:57:1864850 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:21:58:1864847 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:21:58:1864847 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [32.0581s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_True_use_diff_optim_inputs_False [2025-09-19 10:22:29.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:22:29.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:22:29.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:22:29.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:22:29:1865168 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:22:29:1865168 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:22:30:1865165 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:22:30:1865165 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:22:30:1865166 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:22:30:1865166 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:22:30:1865167 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:22:30:1865167 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.6551s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_True_use_diff_optim_inputs_True [2025-09-19 10:23:01.459] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:01.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:01.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:01.469] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:23:01:1865485 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:01:1865485 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:01:1865484 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:01:1865484 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:01:1865487 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:01:1865487 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:01:1865486 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:01:1865486 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.5557s] [ 61%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_False_use_diff_optim_inputs_False [2025-09-19 10:23:32.957] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:32.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:32.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:23:32.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:23:33:1865804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:33:1865804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:33:1865805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:33:1865805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:33:1865803 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:33:1865803 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:23:33:1865802 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:23:33:1865802 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.6558s] [ 63%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_False_use_diff_optim_inputs_True [2025-09-19 10:24:04.655] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:04.655] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:04.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:04.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:24:04:1866124 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:04:1866124 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:04:1866126 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:04:1866126 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:04:1866125 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:04:1866125 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:04:1866127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:04:1866127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [32.0568s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_True_use_diff_optim_inputs_False [2025-09-19 10:24:36.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:36.693] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:36.693] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:24:36.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:24:36:1866444 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:36:1866444 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:36:1866441 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:36:1866441 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:36:1866443 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:36:1866443 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:24:36:1866442 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:24:36:1866442 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.9577s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_True_use_diff_optim_inputs_True [2025-09-19 10:25:08.647] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:08.680] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:08.697] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:08.701] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:25:08:1866761 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:08:1866761 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:08:1866762 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:08:1866762 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:08:1866759 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:08:1866759 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:08:1866760 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:08:1866760 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.6559s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_scatter_full_optim_state_dict_transformer [2025-09-19 10:25:40.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:40.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:40.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:25:40.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:25:40:1867078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:40:1867078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:40:1867080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:40:1867080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:40:1867077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:40:1867077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:25:40:1867079 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:25:40:1867079 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:09:1867077:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:09:1867079:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:10:1867077:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:10:1867079:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:10:1867077:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:10:1867079:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [33.6595s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_halve_world_size [2025-09-19 10:26:13.959] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:13.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:13.975] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:13.984] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:26:14:1867407 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:14:1867407 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:14:1867408 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:14:1867408 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:14:1867409 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:14:1867409 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:14:1867410 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:14:1867410 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:42:1867409:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:42:1867407:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:43:1867407:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:43:1867409:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:43:1867407:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:26:43:1867409:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [32.4565s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_False_use_diff_optim_inputs_False [2025-09-19 10:26:46.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:46.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:46.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:26:46.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:26:46:1867737 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:46:1867737 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:46:1867740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:46:1867740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:46:1867738 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:46:1867738 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:26:46:1867739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:26:46:1867739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.8583s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_False_use_diff_optim_inputs_True [2025-09-19 10:27:18.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:18.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:18.305] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:18.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:27:18:1868057 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:18:1868057 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:18:1868055 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:18:1868055 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:18:1868058 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:18:1868058 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:18:1868056 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:18:1868056 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.8564s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_True_use_diff_optim_inputs_False [2025-09-19 10:27:50.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:50.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:50.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:27:50.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:27:50:1868376 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:50:1868376 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:50:1868375 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:50:1868375 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:50:1868374 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:50:1868374 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:27:50:1868373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:27:50:1868373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.2553s] [ 76%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_False_wrap_alt_True_use_diff_optim_inputs_True [2025-09-19 10:28:21.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:21.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:21.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:21.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:28:21:1868691 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:21:1868691 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:21:1868692 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:21:1868692 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:21:1868693 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:21:1868693 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:21:1868694 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:21:1868694 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.7556s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_False_use_diff_optim_inputs_False [2025-09-19 10:28:53.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:53.180] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:53.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:28:53.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:28:53:1869012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:53:1869012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:53:1869009 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:53:1869009 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:53:1869011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:53:1869011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:28:53:1869010 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:28:53:1869010 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.9573s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_False_use_diff_optim_inputs_True [2025-09-19 10:29:25.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:25.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:25.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:25.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:29:25:1869329 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:25:1869329 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:25:1869326 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:25:1869326 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:25:1869328 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:25:1869328 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:25:1869327 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:25:1869327 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.5552s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_True_use_diff_optim_inputs_False [2025-09-19 10:29:56.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:56.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:56.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:29:56.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:29:56:1869646 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:56:1869646 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:56:1869644 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:56:1869644 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:57:1869645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:57:1869645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:29:57:1869643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:29:57:1869643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.7577s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_nested_use_multiple_param_groups_True_wrap_alt_True_use_diff_optim_inputs_True [2025-09-19 10:30:28.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:30:28.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:30:28.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:30:28.504] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:30:28:1869962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:30:28:1869962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:30:28:1869963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:30:28:1869963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:30:28:1869961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:30:28:1869961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:30:28:1869964 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:30:28:1869964 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.5556s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_transformer [2025-09-19 10:31:00.051] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:00.100] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:00.125] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:00.155] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:31:00:1870279 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:00:1870279 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:00:1870281 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:00:1870281 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:00:1870282 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:00:1870282 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:00:1870280 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:00:1870280 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:29:1870279:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:31:29:1870281:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:31:29:1870279:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:31:29:1870281:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:31:30:1870279:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:31:30:1870281:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [33.4587s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_unmanaged_params_state_dict_type0_add_to_fsdp_module_False [2025-09-19 10:31:33.487] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:33.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:33.504] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:31:33.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:31:33:1870611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:33:1870611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:33:1870612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:33:1870612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:33:1870609 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:33:1870609 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:31:33:1870610 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:31:33:1870610 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.5573s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_unmanaged_params_state_dict_type0_add_to_fsdp_module_True [2025-09-19 10:32:04.992] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:05.001] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:05.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:05.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:32:05:1870928 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:05:1870928 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:05:1870931 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:05:1870931 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:05:1870929 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:05:1870929 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:05:1870930 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:05:1870930 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [31.2552s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_unmanaged_params_state_dict_type1_add_to_fsdp_module_False [2025-09-19 10:32:36.336] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:36.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:36.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:32:36.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:32:36:1871249 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:36:1871249 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:36:1871250 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:36:1871250 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:36:1871248 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:36:1871248 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:32:36:1871247 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:32:36:1871247 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.1551s] [ 91%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_shard_full_optim_state_dict_unmanaged_params_state_dict_type1_add_to_fsdp_module_True [2025-09-19 10:33:07.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:07.456] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:07.467] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:07.491] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:33:07:1871568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:07:1871568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:07:1871569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:07:1871569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:07:1871567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:07:1871567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:07:1871566 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:07:1871566 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.3552s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_state_dict_with_none_tensor_state [2025-09-19 10:33:38.769] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:38.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:38.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:33:38.808] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:33:38:1871885 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:38:1871885 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:38:1871888 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:38:1871888 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:39:1871886 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:39:1871886 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:33:39:1871887 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:33:39:1871887 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.6565s] [ 95%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_use_orig_params [2025-09-19 10:34:10.456] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:10.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:10.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:10.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:34:10:1872204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:10:1872204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:10:1872205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:10:1872205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:10:1872206 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:10:1872206 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:10:1872203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:10:1872203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:39:1872203:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:39:1872205:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:39:1872203:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:39:1872205:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:40:1872203:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:40:1872205:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:41:1872203:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-10:34:41:1872205:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [34.5605s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_with_empty_optimizer_state [2025-09-19 10:34:44.981] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:45.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:45.003] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:34:45.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:34:45:1872536 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:45:1872536 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:45:1872537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:45:1872537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:45:1872538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:45:1872538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:34:45:1872539 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:34:45:1872539 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.9299s] [ 98%]
../../../../test/distributed/fsdp/test_fsdp_optim_state.py::TestFSDPOptimState::test_with_no_shard [2025-09-19 10:35:00.900] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:00.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:00.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:00.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:35:01:1872837 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:01:1872837 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:01:1872838 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:01:1872838 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:01:1872839 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:01:1872839 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:01:1872840 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:01:1872840 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [16.4308s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_optim_state.py.xml -
======================= 60 passed in 1812.97s (0:30:12) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:35:18.199] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_fsdp_pure_fp16.py::TestPureFP16XPU::test_fp16_dtypes_xpu [2025-09-19 10:35:20.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:20.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:20.444] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:20.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:35:20:1873228 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:20:1873228 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:20:1873230 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:20:1873230 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:20:1873229 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:20:1873229 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:20:1873231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:20:1873231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.9581s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_pure_fp16.py::TestPureFP16XPU::test_pure_fp16_training_xpu [2025-09-19 10:35:52.339] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:52.352] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:52.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:35:52.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:35:52:1873546 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:52:1873546 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:52:1873548 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:52:1873548 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:52:1873547 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:52:1873547 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:35:52:1873545 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:35:52:1873545 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [16.0300s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_pure_fp16.py.xml -
============================== 2 passed in 50.19s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:36:09.319] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 20 items
Running 20 items in this shard

../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardGradScaler::test_grad_scaling PASSED [0.1908s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardGradScaler::test_inf_gradients_skip_optim_step PASSED [0.0020s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardGradScaler::test_scaling_unscaling_sparse PASSED [0.0050s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_mixed_precision_none [2025-09-19 10:36:11.516] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:11.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:11.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:11.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:36:11:1874032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:11:1874032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:11:1874034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:11:1874034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:11:1874031 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:11:1874031 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:11:1874033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:11:1874033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.0264s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_mixed_precision_use_orig_params [2025-09-19 10:36:27.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:27.583] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:27.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:27.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:36:27:1874350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:27:1874350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:27:1874349 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:27:1874349 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:27:1874351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:27:1874351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:27:1874348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:27:1874348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [16.1254s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none [2025-09-19 10:36:43.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:43.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:43.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:36:43.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:36:43:1874665 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:43:1874665 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:43:1874667 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:43:1874667 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:43:1874666 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:43:1874666 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:36:43:1874664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:36:43:1874664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
FAILED [31.5483s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_use_orig_params [2025-09-19 10:37:15.207] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:15.315] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:15.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:15.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:37:15:1874983 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:15:1874983 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:15:1874981 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:15:1874981 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:15:1874982 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:15:1874982 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:15:1874984 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:15:1874984 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6484s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_shard_grad_op_mixed_precision_none [2025-09-19 10:37:46.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:46.911] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:46.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:37:46.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:37:47:1875302 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:47:1875302 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:47:1875301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:47:1875301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:47:1875299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:47:1875299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:37:47:1875300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:37:47:1875300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [16.2266s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_shard_grad_op_mixed_precision_use_orig_params [2025-09-19 10:38:03.115] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:03.124] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:03.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:03.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:38:03:1875617 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:03:1875617 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:03:1875619 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:03:1875619 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:03:1875620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:03:1875620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:03:1875618 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:03:1875618 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [16.0261s] [ 45%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_shard_grad_op_none_none [2025-09-19 10:38:19.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:19.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:19.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:19.143] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:38:19:1875936 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:19:1875936 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:19:1875933 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:19:1875933 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:19:1875935 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:19:1875935 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:19:1875934 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:19:1875934 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.2492s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_shard_grad_op_none_use_orig_params [2025-09-19 10:38:50.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:50.392] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:50.392] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:38:50.396] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:38:50:1876252 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:50:1876252 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:50:1876254 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:50:1876254 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:50:1876253 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:50:1876253 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:38:50:1876251 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:38:50:1876251 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.5475s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_mixed_precision_none [2025-09-19 10:39:21.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:21.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:21.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:22.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:39:22:1876571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:22:1876571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:22:1876569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:22:1876569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:22:1876568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:22:1876568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:22:1876570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:22:1876570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [16.1267s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_mixed_precision_use_orig_params [2025-09-19 10:39:38.043] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:38.059] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:38.059] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:38.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:39:38:1876887 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:38:1876887 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:38:1876890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:38:1876890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:38:1876888 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:38:1876888 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:38:1876889 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:38:1876889 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.1273s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_none [2025-09-19 10:39:54.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:54.209] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:54.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:39:54.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:39:54:1877206 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:54:1877206 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:54:1877203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:54:1877203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:54:1877204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:54:1877204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:39:54:1877205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:39:54:1877205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
FAILED [31.4488s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_use_orig_params [2025-09-19 10:40:25.652] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:25.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:25.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:25.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:40:25:1877522 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:25:1877522 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:25:1877523 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:25:1877523 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:25:1877521 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:25:1877521 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:25:1877524 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:25:1877524 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.2495s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_shard_grad_op_mixed_precision_none [2025-09-19 10:40:56.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:56.895] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:56.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:40:56.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:40:57:1877838 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:57:1877838 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:57:1877839 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:57:1877839 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:57:1877841 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:57:1877841 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:40:57:1877840 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:40:57:1877840 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [16.0268s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_shard_grad_op_mixed_precision_use_orig_params [2025-09-19 10:41:12.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:12.924] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:12.928] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:12.928] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:41:13:1878156 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:13:1878156 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:13:1878157 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:13:1878157 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:13:1878155 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:13:1878155 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:13:1878158 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:13:1878158 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [16.1270s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_shard_grad_op_none_none [2025-09-19 10:41:29.035] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:29.053] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:29.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:41:29.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:41:29:1878471 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:29:1878471 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:29:1878473 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:29:1878473 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:29:1878474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:29:1878474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:41:29:1878472 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:41:29:1878472 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.6486s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_shard_grad_op_none_use_orig_params [2025-09-19 10:42:00.680] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:00.702] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:00.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:00.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:42:00:1878789 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:00:1878789 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:00:1878790 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:00:1878790 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:00:1878792 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:00:1878792 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:00:1878791 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:00:1878791 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.5483s] [ 95%]
../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_sharded_grad_scaler_found_inf [2025-09-19 10:42:32.232] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:32.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:32.263] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:42:32.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:42:32:1879110 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:32:1879110 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:32:1879111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:32:1879111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:32:1879112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:32:1879112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:42:32:1879109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:42:32:1879109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [50.9724s] [100%]

=================================== FAILURES ===================================
_ TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 10:36:41.820000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1874664
I0919 10:36:41.821000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1874665
I0919 10:36:41.821000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1874666
I0919 10:36:41.822000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1874667
_ TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_none _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 10:39:52.331000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1877203
I0919 10:39:52.331000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1877204
I0919 10:39:52.332000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 1877205
I0919 10:39:52.332000 1873863 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 1877206
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_sharded_grad_scaler.py.xml -
=========================== short test summary info ============================
FAILED [31.5483s] ../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_false_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [31.4488s] ../../../../test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py::TestShardedGradScalerParityWithDDP::test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_none - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py", line 205, in test_fsdp_ddp_parity_with_grad_scaler
    self._test_fsdp_parity(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1493, in _test_fsdp_parity
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Tensor-likes are not close!

Mismatched elements: 32 / 32 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 1e-05 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1.3e-06 allowed)

The failure occurred for item [0]
FSDP did not match DDP

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py TestShardedGradScalerParityWithDDP.test_fsdp_ddp_parity_with_grad_scaler_offload_true_none_none_none

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
=================== 2 failed, 18 passed in 433.91s (0:07:13) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 10:43:24.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 179 items
Running 179 items in this shard

../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:43:26.556] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:43:26.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:43:26:1879504 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:43:26:1879504 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:43:26:1879505 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:43:26:1879505 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.6317s] [  0%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:43:56.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:43:56.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:43:57:1879664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:43:57:1879664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:43:57:1879663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:43:57:1879663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8250s] [  1%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:44:11.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:44:11.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:44:11:1879815 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:11:1879815 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:44:11:1879816 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:11:1879816 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8187s] [  1%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:44:26.587] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:44:26.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:44:26:1879966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:26:1879966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:44:26:1879965 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:26:1879965 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8190s] [  2%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:44:41.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:44:41.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:44:41:1880115 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:41:1880115 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:44:41:1880116 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:41:1880116 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.6202s] [  2%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:44:57.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:44:57.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:44:57:1880273 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:57:1880273 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:44:57:1880274 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:44:57:1880274 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8244s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:45:11.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:45:11.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:45:12:1880425 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:12:1880424 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:12:1880425 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:45:12:1880424 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9248s] [  3%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:45:26.789] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:45:26.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:45:27:1880574 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:27:1880574 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:45:27:1880575 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:27:1880575 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8243s] [  4%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:45:41.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:45:41.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:45:41:1880725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:41:1880725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:45:41:1880724 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:45:41:1880724 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.0484s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:46:11.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:46:11.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:46:11:1880883 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:11:1880883 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:46:11:1880884 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:11:1880884 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.7249s] [  5%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:46:26.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:46:26.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:46:26:1881050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:26:1881050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:46:26:1881049 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:26:1881049 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8188s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:46:41.297] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:46:41.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:46:41:1881199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:41:1881199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:46:41:1881200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:41:1881200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8185s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:46:56.083] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:46:56.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:46:56:1881350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:56:1881350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:46:56:1881351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:46:56:1881351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.6262s] [  7%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:47:11.659] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:47:11.676] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:47:11:1881510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:11:1881510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:47:11:1881509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:11:1881509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8250s] [  7%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:47:26.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:47:26.513] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:47:26:1881659 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:26:1881659 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:47:26:1881660 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:26:1881660 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.6247s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:47:41.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:47:41.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:47:41:1881810 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:41:1881810 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:47:41:1881811 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:41:1881811 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9253s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:47:56.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:47:56.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:47:56:1881960 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:56:1881960 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:47:56:1881961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:47:56:1881961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2481s] [  9%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:48:26.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:48:26.377] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:48:26:1882120 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:48:26:1882120 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:48:26:1882119 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:48:26:1882119 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1480s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:48:56.443] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:48:56.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:48:56:1882280 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:48:56:1882280 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:48:56:1882279 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:48:56:1882279 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.7248s] [ 10%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:49:11.155] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:49:11.166] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:49:11:1882432 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:11:1882432 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:49:11:1882431 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:11:1882431 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8252s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:49:25.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:49:25.987] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:49:26:1882581 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:26:1882582 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:26:1882582 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:49:26:1882581 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4252s] [ 11%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:49:41.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:49:41.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:49:41:1882740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:41:1882740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:49:41:1882739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:41:1882739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2259s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:49:56.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:49:56.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:49:56:1882898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:56:1882898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:49:56:1882899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:49:56:1882899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.7248s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:50:11.384] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:50:11.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:50:11:1883050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:11:1883050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:50:11:1883049 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:11:1883049 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9197s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:50:26.295] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:50:26.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:50:26:1883199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:26:1883200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:26:1883199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:50:26:1883200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.0479s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:50:56.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:50:56.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:50:56:1883358 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:56:1883358 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:50:56:1883357 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:50:56:1883357 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2469s] [ 14%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:51:26.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:51:26.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:51:26:1883517 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:26:1883517 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:51:26:1883518 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:26:1883518 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8249s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:51:41.425] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:51:41.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:51:41:1883668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:41:1883668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:51:41:1883669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:41:1883669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7246s] [ 15%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:51:56.143] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:51:56.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:51:56:1883820 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:56:1883820 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:51:56:1883819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:51:56:1883819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2258s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:52:11.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:52:11.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:52:11:1883977 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:11:1883977 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:52:11:1883978 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:11:1883978 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4257s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:52:26.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:52:26.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:52:26:1884137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:26:1884137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:52:26:1884136 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:26:1884136 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.6246s] [ 17%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:52:41.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:52:41.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:52:41:1884289 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:41:1884289 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:52:41:1884290 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:41:1884290 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9247s] [ 17%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:52:56.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:52:56.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:52:56:1884440 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:56:1884440 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:52:56:1884439 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:52:56:1884439 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2475s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:53:26.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:53:26.661] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:53:26:1884598 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:53:26:1884599 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:53:26:1884599 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:53:26:1884598 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1348s] [ 18%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:53:56.732] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:53:56.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:53:56:1884760 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:53:56:1884760 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:53:56:1884759 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:53:56:1884759 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.0478s] [ 19%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:54:26.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:54:26.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:54:26:1884918 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:54:26:1884918 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:54:26:1884919 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:54:26:1884919 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.2478s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:54:56.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:54:56.997] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:54:57:1885077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:54:57:1885077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:54:57:1885076 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:54:57:1885076 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3197s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:55:12.419] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:55:12.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:55:12:1885237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:12:1885237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:55:12:1885236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:12:1885236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3252s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:55:27.676] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:55:27.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:55:27:1885395 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:27:1885395 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:55:27:1885394 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:27:1885394 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4259s] [ 21%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:55:43.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:55:43.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:55:43:1885553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:43:1885553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:55:43:1885552 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:43:1885552 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.4261s] [ 22%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:55:58.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:55:58.538] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:55:58:1885711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:58:1885711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:55:58:1885710 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:55:58:1885710 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2468s] [ 22%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:56:28.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:56:28.771] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:56:28:1885870 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:56:28:1885870 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:56:28:1885871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:56:28:1885871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1352s] [ 23%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:56:58.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:56:59.007] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:56:59:1886029 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:56:59:1886029 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:56:59:1886030 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:56:59:1886030 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1384s] [ 24%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:57:29.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:57:29.020] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:57:29:1886189 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:57:29:1886189 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:57:29:1886188 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:57:29:1886188 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.4487s] [ 24%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:57:59.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:57:59.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:57:59:1886349 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:57:59:1886349 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:57:59:1886350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:57:59:1886350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2257s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:58:14.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:58:14.791] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:58:15:1886511 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:15:1886511 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:58:15:1886510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:15:1886510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3260s] [ 25%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:58:30.065] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:58:30.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:58:30:1886669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:30:1886668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:30:1886668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:58:30:1886669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3260s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_basic_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_fp16_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:58:45.369] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:58:45.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:58:45:1886827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:45:1886827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:58:45:1886828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:58:45:1886828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3262s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 10:59:00.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:59:00.696] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:59:00:1886985 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:00:1886985 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:59:00:1886986 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:00:1886986 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3257s] [ 27%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 10:59:16.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:59:16.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:59:16:1887138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:16:1887138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:59:16:1887137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:16:1887137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7246s] [ 27%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 10:59:30.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:59:30.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:59:30:1887289 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:30:1887288 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:30:1887288 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:59:30:1887289 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0252s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 10:59:45.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 10:59:45.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-10:59:45:1887439 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:45:1887439 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-10:59:46:1887438 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-10:59:46:1887438 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9254s] [ 29%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:00:00.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:00:00.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:00:00:1887589 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:00:1887589 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:00:00:1887588 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:00:1887588 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3257s] [ 29%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:00:16.044] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:00:16.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:00:16:1887740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:16:1887740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:00:16:1887739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:16:1887739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8254s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:00:30.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:00:30.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:00:31:1887891 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:31:1887891 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:00:31:1887890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:31:1887890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9242s] [ 30%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:00:45.791] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:00:45.806] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:00:45:1888040 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:45:1888040 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:00:45:1888041 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:00:45:1888041 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7243s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:01:00.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:01:00.531] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:01:00:1888191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:00:1888191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:01:00:1888192 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:00:1888192 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2256s] [ 31%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:01:15.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:01:15.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:01:15:1888343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:15:1888343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:01:15:1888342 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:15:1888342 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8253s] [ 32%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:01:30.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:01:30.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:01:30:1888493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:30:1888493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:01:30:1888492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:30:1888492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8252s] [ 32%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:01:45.405] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:01:45.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:01:45:1888643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:45:1888643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:01:45:1888644 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:01:45:1888644 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7242s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:02:00.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:02:00.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:02:00:1888795 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:00:1888795 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:02:00:1888794 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:00:1888794 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3226s] [ 34%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:02:15.497] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:02:15.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:02:15:1888944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:15:1888944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:02:15:1888945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:15:1888945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.6238s] [ 34%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:02:30.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:02:30.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:02:30:1889095 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:30:1889095 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:02:30:1889096 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:30:1889096 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7187s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_local_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:02:44.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:02:44.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:02:45:1889246 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:45:1889246 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:02:45:1889247 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:45:1889247 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9199s] [ 35%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:02:59.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:02:59.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:02:59:1889397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:59:1889397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:02:59:1889396 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:02:59:1889396 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2255s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:03:14.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:03:14.948] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:03:15:1889548 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:15:1889548 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:03:15:1889547 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:15:1889547 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2257s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:03:30.153] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:03:30.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:03:30:1889697 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:30:1889698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:30:1889698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:03:30:1889697 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0205s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:03:45.174] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:03:45.176] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:03:45:1889848 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:45:1889848 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:03:45:1889849 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:03:45:1889849 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9251s] [ 37%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:04:00.111] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:04:00.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:04:00:1889999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:00:1889998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:00:1889998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:04:00:1889999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2259s] [ 38%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:04:15.353] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:04:15.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:04:15:1890149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:15:1890149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:04:15:1890150 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:15:1890150 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2255s] [ 39%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:04:30.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:04:30.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:04:30:1890300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:30:1890300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:04:30:1890299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:30:1890299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9250s] [ 39%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:04:45.496] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:04:45.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:04:45:1890449 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:45:1890449 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:04:45:1890450 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:04:45:1890450 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.7247s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:05:00.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:05:00.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:05:00:1890600 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:00:1890600 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:05:00:1890599 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:00:1890599 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1188s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:05:15.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:05:15.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:05:15:1890750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:15:1890750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:05:15:1890751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:15:1890751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1197s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:05:30.463] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:05:30.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:05:30:1890901 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:30:1890901 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:05:30:1890900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:30:1890900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7248s] [ 41%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:05:45.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:05:45.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:05:45:1891050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:45:1891050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:05:45:1891051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:05:45:1891051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8249s] [ 42%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:06:00.093] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:06:00.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:06:00:1891200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:00:1891200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:06:00:1891201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:00:1891201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1256s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:06:15.132] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:06:15.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:06:15:1891352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:15:1891352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:06:15:1891351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:15:1891351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2261s] [ 43%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:06:30.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:06:30.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:06:30:1891502 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:30:1891502 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:06:30:1891501 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:30:1891501 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8252s] [ 44%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_sharded_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:06:45.174] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:06:45.195] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:06:45:1891651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:45:1891651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:06:45:1891652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:06:45:1891652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8251s] [ 44%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:07:00.053] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:07:00.075] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:07:00:1891803 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:00:1891803 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:07:00:1891804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:00:1891804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2255s] [ 45%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:07:15.237] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:07:15.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:07:15:1891953 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:15:1891953 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:07:15:1891954 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:15:1891954 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3260s] [ 45%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:07:30.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:07:30.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:07:30:1892104 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:30:1892104 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:07:30:1892103 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:30:1892103 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2254s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:07:45.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:07:45.801] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:07:46:1892255 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:46:1892255 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:07:46:1892254 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:07:46:1892254 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2259s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:08:01.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:08:01.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:08:01:1892406 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:01:1892405 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:01:1892406 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:08:01:1892405 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2238s] [ 47%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:08:16.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:08:16.332] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:08:16:1892559 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:16:1892559 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:08:16:1892558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:16:1892558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1255s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:08:31.389] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:08:31.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:08:31:1892709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:31:1892709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:08:31:1892708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:31:1892708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2259s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload0_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:08:46.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:08:46.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:08:46:1892860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:46:1892860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:08:46:1892859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:08:46:1892859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2248s] [ 49%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:09:01.811] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:09:01.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:09:01:1893010 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:01:1893010 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:09:02:1893009 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:02:1893009 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1256s] [ 49%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:09:17.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:09:17.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:09:17:1893161 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:17:1893160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:17:1893160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:09:17:1893161 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1252s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:09:32.084] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:09:32.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:09:32:1893311 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:32:1893310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:32:1893311 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:09:32:1893310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2258s] [ 50%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_False_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:09:47.301] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:09:47.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:09:47:1893462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:47:1893461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:09:47:1893461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:09:47:1893462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2259s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_False [2025-09-19 11:10:02.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:10:02.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:10:02:1893612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:02:1893612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:10:02:1893611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:02:1893611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1198s] [ 51%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_False_use_orig_params_True [2025-09-19 11:10:17.655] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:10:17.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:10:17:1893762 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:17:1893762 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:10:17:1893763 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:17:1893763 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1199s] [ 52%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_False [2025-09-19 11:10:32.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:10:32.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:10:32:1893912 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:32:1893912 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:10:33:1893913 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:33:1893913 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1276s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_buffers_save_and_load_state_dict_state_dict_type_state_dict_cpu_offload1_mixed_precision_True_state_dict_rank0_and_offload_True_use_orig_params_True [2025-09-19 11:10:47.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:10:47.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:10:48:1894062 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:48:1894062 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:10:48:1894063 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:10:48:1894063 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1253s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_keys_state_dict_type_local_state_dict [2025-09-19 11:11:03.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:11:03.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:11:03:1894212 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:03:1894212 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:11:03:1894213 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:03:1894213 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1254s] [ 54%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_keys_state_dict_type_sharded_state_dict [2025-09-19 11:11:18.145] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:11:18.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:11:18:1894363 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:18:1894363 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:11:18:1894364 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:18:1894364 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.7247s] [ 54%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_keys_state_dict_type_state_dict [2025-09-19 11:11:32.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:11:32.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:11:33:1894513 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:33:1894513 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:11:33:1894514 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:33:1894514 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9245s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_both_after_wrap_rank0_only_and_offload_False [2025-09-19 11:11:47.812] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:11:47.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:11:48:1894663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:48:1894663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:11:48:1894664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:11:48:1894664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1256s] [ 55%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_both_after_wrap_rank0_only_and_offload_True [2025-09-19 11:12:02.935] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:12:02.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:12:03:1894814 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:03:1894815 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:03:1894814 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:12:03:1894815 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1258s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_both_rank0_only_and_offload_False [2025-09-19 11:12:18.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:12:18.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:12:18:1894966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:18:1894966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:12:18:1894965 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:18:1894965 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1258s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_both_rank0_only_and_offload_True [2025-09-19 11:12:33.183] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:12:33.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:12:33:1895116 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:33:1895116 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:12:33:1895117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:33:1895117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1192s] [ 57%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_dest_rank0_only_and_offload_False [2025-09-19 11:12:48.275] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:12:48.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:12:48:1895267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:48:1895267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:12:48:1895268 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:12:48:1895268 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2249s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_dest_rank0_only_and_offload_True [2025-09-19 11:13:03.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:13:03.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:13:03:1895418 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:03:1895418 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:13:03:1895417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:03:1895417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3259s] [ 58%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_source_after_wrap_rank0_only_and_offload_False [2025-09-19 11:13:18.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:13:18.876] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:13:19:1895569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:19:1895569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:13:19:1895568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:19:1895568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1256s] [ 59%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_source_after_wrap_rank0_only_and_offload_True [2025-09-19 11:13:33.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:13:33.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:13:34:1895719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:34:1895719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:13:34:1895720 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:34:1895720 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2256s] [ 59%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_source_rank0_only_and_offload_False [2025-09-19 11:13:49.209] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:13:49.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:13:49:1895871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:49:1895871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:13:49:1895870 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:13:49:1895870 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1222s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_sharded_state_dict_checkpoint_wrap_source_rank0_only_and_offload_True [2025-09-19 11:14:04.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:14:04.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:14:04:1896020 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:04:1896020 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:14:04:1896021 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:04:1896021 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0248s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_both_after_wrap_rank0_only_and_offload_False [2025-09-19 11:14:19.387] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:14:19.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:14:19:1896172 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:19:1896172 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:14:19:1896171 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:19:1896171 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1258s] [ 61%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_both_after_wrap_rank0_only_and_offload_True [2025-09-19 11:14:34.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:14:34.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:14:34:1896322 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:34:1896322 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:14:34:1896321 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:34:1896321 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1254s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_both_rank0_only_and_offload_False [2025-09-19 11:14:49.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:14:49.623] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:14:49:1896472 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:49:1896472 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:14:49:1896473 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:14:49:1896473 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2260s] [ 62%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_both_rank0_only_and_offload_True [2025-09-19 11:15:04.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:15:04.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:15:05:1896623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:05:1896623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:15:05:1896622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:05:1896622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2259s] [ 63%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_dest_rank0_only_and_offload_False [2025-09-19 11:15:20.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:15:20.085] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:15:20:1896775 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:20:1896775 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:15:20:1896774 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:20:1896774 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1256s] [ 63%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_dest_rank0_only_and_offload_True [2025-09-19 11:15:35.201] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:15:35.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:15:35:1896924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:35:1896924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:15:35:1896925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:35:1896925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2193s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_source_after_wrap_rank0_only_and_offload_False [2025-09-19 11:15:50.500] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:15:50.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:15:50:1897074 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:50:1897074 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:15:50:1897075 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:15:50:1897075 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.4231s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_source_after_wrap_rank0_only_and_offload_True [2025-09-19 11:16:05.807] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:16:05.840] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:16:06:1897226 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:06:1897226 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:16:06:1897227 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:06:1897227 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2236s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_source_rank0_only_and_offload_False [2025-09-19 11:16:21.057] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:16:21.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:16:21:1897376 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:21:1897376 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:16:21:1897377 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:21:1897377 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1255s] [ 65%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_fsdp_state_dict_with_activation_checkpoint_state_dict_type_state_dict_checkpoint_wrap_source_rank0_only_and_offload_True [2025-09-19 11:16:36.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:16:36.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:16:36:1897527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:36:1897527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:16:36:1897526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:36:1897526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2257s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_full_state_dict_missing_unexpected_keys_cleaned [2025-09-19 11:16:51.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:16:51.460] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:16:51:1897677 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:51:1897677 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:16:51:1897676 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:16:51:1897676 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9242s] [ 67%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_local_state_dict_with_empty_ranks [2025-09-19 11:17:06.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:17:06.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:17:06:1897831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:17:06:1897831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:17:06:1897832 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:17:06:1897832 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.0461s] [ 67%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_local_state_dict_mixed_precision_False_state_dict_rank0_and_offload_False [2025-09-19 11:17:36.388] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:17:36.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:17:36:1897991 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:17:36:1897991 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:17:36:1897990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:17:36:1897990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.4487s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_local_state_dict_mixed_precision_False_state_dict_rank0_and_offload_True [2025-09-19 11:18:06.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:18:06.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:18:07:1898150 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:07:1898150 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:18:07:1898151 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:07:1898151 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0249s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_local_state_dict_mixed_precision_True_state_dict_rank0_and_offload_False [2025-09-19 11:18:21.856] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:18:21.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:18:22:1898301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:22:1898302 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:22:1898301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:18:22:1898302 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3259s] [ 69%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_local_state_dict_mixed_precision_True_state_dict_rank0_and_offload_True [2025-09-19 11:18:37.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:18:37.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:18:37:1898461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:37:1898461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:18:37:1898462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:37:1898462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8253s] [ 69%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_sharded_state_dict_mixed_precision_False_state_dict_rank0_and_offload_False [2025-09-19 11:18:51.983] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:18:51.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:18:52:1898611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:52:1898611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:18:52:1898612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:18:52:1898612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1499s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_sharded_state_dict_mixed_precision_False_state_dict_rank0_and_offload_True [2025-09-19 11:19:22.166] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:19:22.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:19:22:1898771 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:22:1898771 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:19:22:1898772 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:22:1898772 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8258s] [ 70%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_sharded_state_dict_mixed_precision_True_state_dict_rank0_and_offload_False [2025-09-19 11:19:36.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:19:37.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:19:37:1898923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:37:1898923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:19:37:1898922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:37:1898922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3259s] [ 71%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_sharded_state_dict_mixed_precision_True_state_dict_rank0_and_offload_True [2025-09-19 11:19:52.317] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:19:52.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:19:52:1899080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:52:1899080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:19:52:1899081 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:19:52:1899081 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8250s] [ 72%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_state_dict_mixed_precision_False_state_dict_rank0_and_offload_False [2025-09-19 11:20:07.168] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:20:07.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:20:07:1899231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:20:07:1899231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:20:07:1899232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:20:07:1899232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2482s] [ 72%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_state_dict_mixed_precision_False_state_dict_rank0_and_offload_True [2025-09-19 11:20:37.477] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:20:37.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:20:37:1899390 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:20:37:1899389 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:20:37:1899390 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:20:37:1899389 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.3479s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_state_dict_mixed_precision_True_state_dict_rank0_and_offload_False [2025-09-19 11:21:07.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:21:07.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:21:07:1899549 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:07:1899549 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:21:07:1899550 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:07:1899550 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3256s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_save_and_load_after_forward_state_dict_state_dict_type_state_dict_mixed_precision_True_state_dict_rank0_and_offload_True [2025-09-19 11:21:23.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:21:23.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:21:23:1899709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:23:1899708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:23:1899709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:21:23:1899708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4260s] [ 74%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_sharded_load_multi_backend_pg [2025-09-19 11:21:38.509] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:21:38.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:21:38:1899867 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:38:1899866 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:38:1899867 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:21:38:1899866 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-11:21:51:1899867:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:21:51:1899866:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-11:21:51:1899866:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:21:51:1899867:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.8265s] [ 74%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_shared_module_and_shared_parameter [2025-09-19 11:21:54.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:21:54.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:21:54:1900038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:54:1900038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:21:54:1900037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:21:54:1900037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1255s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_sharded_state_dict_state_dict_rank0_and_offload_False_fsdp_root_False [2025-09-19 11:22:09.481] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:22:09.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:22:09:1900189 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:22:09:1900189 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:22:09:1900190 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:22:09:1900190 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1483s] [ 75%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_sharded_state_dict_state_dict_rank0_and_offload_False_fsdp_root_True [2025-09-19 11:22:39.608] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:22:39.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:22:39:1900348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:22:39:1900348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:22:39:1900349 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:22:39:1900349 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1481s] [ 76%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_sharded_state_dict_state_dict_rank0_and_offload_True_fsdp_root_False [2025-09-19 11:23:09.733] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:23:09.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:23:09:1900510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:09:1900510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:23:09:1900511 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:09:1900511 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8250s] [ 77%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_sharded_state_dict_state_dict_rank0_and_offload_True_fsdp_root_True [2025-09-19 11:23:24.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:23:24.595] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:23:24:1900661 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:24:1900661 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:23:24:1900662 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:24:1900662 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8252s] [ 77%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_state_dict_state_dict_rank0_and_offload_False_fsdp_root_False [2025-09-19 11:23:39.363] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:23:39.381] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:23:39:1900812 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:39:1900812 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:23:39:1900813 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:23:39:1900813 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [29.8476s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_state_dict_state_dict_rank0_and_offload_False_fsdp_root_True [2025-09-19 11:24:09.240] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:24:09.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:24:09:1900973 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:24:09:1900973 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:24:09:1900972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:24:09:1900972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.1475s] [ 78%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_state_dict_state_dict_rank0_and_offload_True_fsdp_root_False [2025-09-19 11:24:39.412] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:24:39.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:24:39:1901131 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:24:39:1901131 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:24:39:1901132 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:24:39:1901132 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [29.9347s] [ 79%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_load_into_local_module_state_dict_type_state_dict_state_dict_rank0_and_offload_True_fsdp_root_True [2025-09-19 11:25:09.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:25:09.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:25:09:1901291 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:09:1901291 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:25:09:1901292 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:09:1901292 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.3347s] [ 79%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_rank0_offload_save_load_flow_use_orig_params_False [2025-09-19 11:25:39.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:25:39.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:25:39:1901450 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:39:1901450 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:25:39:1901451 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:39:1901451 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.3195s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_rank0_offload_save_load_flow_use_orig_params_True [2025-09-19 11:25:55.001] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:25:55.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:25:55:1901601 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:55:1901601 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:25:55:1901602 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:25:55:1901602 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3262s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_save_load_flow_state_dict_type_local_state_dict [2025-09-19 11:26:10.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:26:10.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:26:10:1901753 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:26:10:1901752 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:26:10:1901752 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:26:10:1901753 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [30.1477s] [ 81%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_save_load_flow_state_dict_type_sharded_state_dict [2025-09-19 11:26:40.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:26:40.475] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:26:40:1901911 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:26:40:1901911 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:26:40:1901912 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:26:40:1901912 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [29.8470s] [ 82%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_save_load_flow_state_dict_type_state_dict [2025-09-19 11:27:10.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:27:10.320] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:27:10:1902071 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:27:10:1902072 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:27:10:1902071 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:27:10:1902072 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2483s] [ 82%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_skip_module_state_dict_type_local_state_dict_double_nest_True [2025-09-19 11:27:40.665] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:27:40.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:27:40:1902230 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:27:40:1902231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:27:40:1902231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:27:40:1902230 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.4483s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_skip_module_state_dict_type_sharded_state_dict_double_nest_True [2025-09-19 11:28:11.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:28:11.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:28:11:1902392 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:28:11:1902392 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:28:11:1902391 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:28:11:1902391 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [29.9477s] [ 83%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_skip_module_state_dict_type_state_dict_double_nest_True [2025-09-19 11:28:41.048] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:28:41.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:28:41:1902550 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:28:41:1902550 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:28:41:1902551 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:28:41:1902551 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.2467s] [ 84%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_type [2025-09-19 11:29:11.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:29:11.196] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:29:11:1902711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:11:1902711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:29:11:1902712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:11:1902712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0253s] [ 84%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_False_ignore_inner_False_mixed_precision_False [2025-09-19 11:29:26.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:29:26.348] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:29:26:1902863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:26:1902863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:29:26:1902862 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:26:1902862 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0247s] [ 85%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_False_ignore_inner_False_mixed_precision_True [2025-09-19 11:29:41.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:29:41.286] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:29:41:1903013 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:41:1903013 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:29:41:1903012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:41:1903012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1255s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_False_ignore_inner_True_mixed_precision_False [2025-09-19 11:29:56.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:29:56.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:29:56:1903163 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:56:1903163 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:29:56:1903162 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:29:56:1903162 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.7245s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_False_ignore_inner_True_mixed_precision_True [2025-09-19 11:30:11.124] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:30:11.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:30:11:1903314 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:11:1903314 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:30:11:1903313 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:11:1903313 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0246s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_True_ignore_inner_False_mixed_precision_False [2025-09-19 11:30:26.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:30:26.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:30:26:1903464 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:26:1903464 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:30:26:1903465 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:26:1903465 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1256s] [ 87%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_True_ignore_inner_False_mixed_precision_True [2025-09-19 11:30:41.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:30:41.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:30:41:1903614 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:41:1903615 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:41:1903615 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:30:41:1903614 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2198s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_True_ignore_inner_True_mixed_precision_False [2025-09-19 11:30:56.573] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:30:56.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:30:56:1903765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:56:1903765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:30:56:1903766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:30:56:1903766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1191s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_sharded_state_dict_prefix_True_ignore_inner_True_mixed_precision_True [2025-09-19 11:31:11.611] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:31:11.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:31:11:1903916 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:11:1903916 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:31:11:1903917 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:11:1903917 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0189s] [ 89%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_False_ignore_inner_False_mixed_precision_False [2025-09-19 11:31:26.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:31:26.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:31:26:1904067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:26:1904067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:31:26:1904066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:26:1904066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0245s] [ 89%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_False_ignore_inner_False_mixed_precision_True [2025-09-19 11:31:41.656] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:31:41.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:31:41:1904216 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:41:1904216 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:31:41:1904217 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:41:1904217 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8247s] [ 90%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_False_ignore_inner_True_mixed_precision_False [2025-09-19 11:31:56.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:31:56.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:31:56:1904369 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:56:1904369 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:31:56:1904368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:31:56:1904368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8248s] [ 91%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_False_ignore_inner_True_mixed_precision_True [2025-09-19 11:32:11.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:32:11.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:32:11:1904518 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:11:1904518 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:32:11:1904519 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:11:1904519 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9231s] [ 91%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_True_ignore_inner_False_mixed_precision_False [2025-09-19 11:32:26.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:32:26.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:32:26:1904670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:26:1904670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:32:26:1904669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:26:1904669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0252s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_True_ignore_inner_False_mixed_precision_True [2025-09-19 11:32:41.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:32:41.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:32:41:1904819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:41:1904819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:32:41:1904820 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:41:1904820 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0195s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_True_ignore_inner_True_mixed_precision_False [2025-09-19 11:32:56.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:32:56.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:32:56:1904972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:56:1904972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:32:56:1904971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:32:56:1904971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1189s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_ignored_modules_state_dict_type_state_dict_prefix_True_ignore_inner_True_mixed_precision_True [2025-09-19 11:33:11.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:33:11.436] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:33:11:1905123 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:11:1905123 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:33:11:1905124 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:11:1905124 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.9187s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_manual_ac_wrapper_state_dict_type_sharded_state_dict_rank0_only_and_offload_False [2025-09-19 11:33:26.320] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:33:26.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:33:26:1905274 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:26:1905274 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:33:26:1905273 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:26:1905273 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1250s] [ 94%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_manual_ac_wrapper_state_dict_type_sharded_state_dict_rank0_only_and_offload_True [2025-09-19 11:33:41.449] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:33:41.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:33:41:1905425 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:41:1905425 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:33:41:1905424 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:41:1905424 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8250s] [ 94%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_manual_ac_wrapper_state_dict_type_state_dict_rank0_only_and_offload_False [2025-09-19 11:33:56.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:33:56.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:33:56:1905575 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:56:1905575 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:33:56:1905574 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:33:56:1905574 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1255s] [ 95%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_manual_ac_wrapper_state_dict_type_state_dict_rank0_only_and_offload_True [2025-09-19 11:34:11.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:34:11.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:34:11:1905725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:11:1905725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:34:11:1905726 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:11:1905726 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.2255s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_shared_parameters_state_dict_type_local_state_dict [2025-09-19 11:34:26.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:34:26.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:34:26:1905877 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:26:1905877 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:34:26:1905876 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:26:1905876 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0253s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_shared_parameters_state_dict_type_sharded_state_dict [2025-09-19 11:34:41.669] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:34:41.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:34:41:1906027 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:41:1906027 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:34:41:1906026 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:41:1906026 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0256s] [ 97%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_state_dict_with_shared_parameters_state_dict_type_state_dict [2025-09-19 11:34:56.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:34:56.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:34:56:1906176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:56:1906177 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:34:56:1906177 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:34:56:1906176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1255s] [ 97%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_torch_save_load [2025-09-19 11:35:11.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:11.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:35:11:1906329 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:11:1906328 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:11:1906329 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:11:1906328 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.8251s] [ 98%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_world_size_one [2025-09-19 11:35:26.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:26.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:35:26:1906478 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:26:1906478 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:26:1906479 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:26:1906479 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9248s] [ 98%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict::test_wrong_state_dict_config [2025-09-19 11:35:41.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:41.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:35:41:1906630 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:41:1906630 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:41:1906631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:41:1906631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9251s] [ 99%]
../../../../test/distributed/fsdp/test_fsdp_state_dict.py::TestFSDPStateDict4GPUs::test_local_state_dict_reshard [2025-09-19 11:35:56.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:56.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:56.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:35:56.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:35:56:1906783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:56:1906783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:56:1906782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:56:1906782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:56:1906780 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:56:1906780 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:35:56:1906781 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:35:56:1906781 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:36:24:1906780:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:24:1906781:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.2553s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_state_dict.py.xml -
======================= 179 passed in 3183.47s (0:53:03) =======================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 11:36:28.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/fsdp/test_fsdp_tp_integration.py::TestTPFSDPIntegration::test_fsdp_tp_extension_grad [2025-09-19 11:36:30.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:36:30.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:36:30.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:36:30.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:36:31:1907181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:36:31:1907181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:36:31:1907180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:36:31:1907180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:36:31:1907179 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:36:31:1907179 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:36:31:1907178 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:36:31:1907178 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:36:43:1907179:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:43:1907178:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:43:1907180:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:43:1907181:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:44:1907181:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:44:1907179:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:44:1907178:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:36:44:1907180:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.9791s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_tp_integration.py::TestTPFSDPIntegration::test_fsdp_tp_integration [2025-09-19 11:37:02.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:02.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:02.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:02.709] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:37:02:1907512 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:02:1907512 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:02:1907514 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:02:1907514 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:02:1907515 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:02:1907515 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:02:1907513 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:02:1907513 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:15:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:15:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:15:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:15:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:16:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:16:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:16:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:16:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:32:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:33:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:34:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:35:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907513:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907514:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:36:1907515:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
skip_if_not_powerof2_worldsize_xpu x: 4
dist init r=2, world=4
skip_if_not_powerof2_worldsize_xpu x: 4
dist init r=3, world=4
skip_if_not_powerof2_worldsize_xpu x: 4
dist init r=0, world=4
skip_if_not_powerof2_worldsize_xpu x: 4
PASSED [36.7636s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_tp_integration.py::TestTPFSDPIntegration::test_fsdp_tp_sync_module_state [2025-09-19 11:37:39.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:39.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:39.472] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:39.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:37:39:1907960 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:39:1907960 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:39:1907962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:39:1907962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:39:1907959 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:39:1907959 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:39:1907961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:39:1907961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:52:1907959:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:52:1907962:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:52:1907960:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-11:37:52:1907961:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [16.0254s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_tp_integration.py.xml -
========================= 3 passed in 86.69s (0:01:26) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 11:37:56.435] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/fsdp/test_fsdp_traversal.py::TestTraversalXPU::test_fsdp_modules_xpu [2025-09-19 11:37:58.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:37:58.648] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:37:58:1908346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:58:1908346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:37:58:1908345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:37:58:1908345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1231s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_traversal.py.xml -
============================== 1 passed in 17.30s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 11:38:14.667] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/fsdp/test_fsdp_uneven.py::TestUnevenParamShardXPU::test_one_iteration_xpu [2025-09-19 11:38:16.859] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:38:16.868] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:38:16.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:38:16.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:38:17:1908569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:17:1908569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:38:17:1908571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:17:1908571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:38:17:1908572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:17:1908572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:38:17:1908570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:17:1908570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.3571s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_uneven.py.xml -
============================== 1 passed in 33.56s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 11:38:49.203] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 15 items
Running 15 items in this shard

../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_named_parameters_and_buffers [2025-09-19 11:38:51.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:38:51.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:38:51:1908961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:51:1908961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:38:51:1908960 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:38:51:1908960 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3444s] [  6%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_params_param_data [2025-09-19 11:39:06.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:39:06.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:39:06:1909110 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:06:1909110 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:39:06:1909111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:06:1909111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.4200s] [ 13%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_params_recurse [2025-09-19 11:39:22.020] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:39:22.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:39:22:1909262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:22:1909262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:39:22:1909261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:22:1909261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.1197s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_params_respects_reshard [2025-09-19 11:39:37.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:39:37.061] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:39:37:1909412 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:37:1909412 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:39:37:1909411 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:39:37:1909411 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.3483s] [ 26%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_params_writeback [2025-09-19 11:40:07.412] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:40:07.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:40:07:1909570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:07:1909570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:40:07:1909569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:07:1909569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0253s] [ 33%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_singleton_param_writeback [2025-09-19 11:40:22.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:40:22.469] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:40:22:1909721 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:22:1909721 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:40:22:1909722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:22:1909722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9191s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_unshard_submodule [2025-09-19 11:40:37.371] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:40:37.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:40:37:1909872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:37:1909871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:37:1909872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:40:37:1909871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.1248s] [ 46%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_with_grads_core [2025-09-19 11:40:52.497] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:40:52.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:40:52:1910022 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:52:1910022 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:40:52:1910023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:40:52:1910023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [43.7674s] [ 53%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParams::test_with_grads_none_grads [2025-09-19 11:41:36.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:41:36.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:41:36:1910181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:36:1910181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:41:36:1910182 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:36:1910182 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.0251s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsNoShard::test_unshard_params_param_data_no_shard [2025-09-19 11:41:51.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:41:51:1910332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:51:1910332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.3069s] [ 66%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsNoShard::test_unshard_params_writeback_no_shard [2025-09-19 11:41:54.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:41:54:1910406 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:54:1910406 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.2063s] [ 73%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsErrors::test_offload_to_cpu_no_shard_raises [2025-09-19 11:41:57.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:41:57.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:41:58:1910481 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:58:1910482 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:41:58:1910482 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:41:58:1910481 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8244s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsErrors::test_rank0_only_with_writeback_raises [2025-09-19 11:42:12.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:42:12.623] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:42:12:1910633 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:12:1910633 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:42:12:1910632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:12:1910632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [14.8239s] [ 86%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsErrors::test_unshard_params_from_backward_raises [2025-09-19 11:42:27.440] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:42:27.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:42:27:1910783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:27:1910783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:42:27:1910784 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:27:1910784 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [14.9242s] [ 93%]
../../../../test/distributed/fsdp/test_fsdp_unshard_params.py::TestUnshardParamsErrors::test_unshard_params_from_forward_raises [2025-09-19 11:42:42.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:42:42.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:42:42:1910944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:42:1910944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:42:42:1910943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:42:42:1910943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.0252s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_unshard_params.py.xml -
======================== 15 passed in 248.21s (0:04:08) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 11:42:57.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 25 items
Running 25 items in this shard

../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_cpu_offload_sharding_strategy_str_full_shard [2025-09-19 11:43:08.911] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:43:08.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:43:18:1911558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:43:18:1911558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:43:20:1911557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:43:20:1911557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [54.6870s] [  4%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_cpu_offload_sharding_strategy_str_no_shard [2025-09-19 11:44:03.603] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:44:03.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:44:13:1912494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:44:13:1912494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:44:15:1912493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:44:15:1912493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [39.5609s] [  8%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_cpu_offload_sharding_strategy_str_shard_grad_op [2025-09-19 11:44:43.193] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:44:43.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:44:53:1913428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:44:53:1913428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:44:54:1913427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:44:54:1913427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [54.3860s] [ 12%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_sharding_strategy_str_full_shard [2025-09-19 11:45:37.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:45:37.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:45:47:1914362 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:45:47:1914362 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:45:48:1914361 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:45:48:1914361 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [55.7854s] [ 16%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_sharding_strategy_str_no_shard [2025-09-19 11:46:33.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:46:33.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:46:43:1915296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:46:43:1915296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:46:44:1915297 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:46:44:1915297 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [79.8164s] [ 20%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_sharding_strategy_str_shard_grad_op [2025-09-19 11:47:53.183] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:47:53.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:48:03:1916232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:48:03:1916232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:48:04:1916231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:48:04:1916231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
FAILED [300.0263s] [ 24%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_trainability [2025-09-19 11:52:53.193] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:52:53.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:53:03:1917176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:53:03:1917176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:53:04:1917175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:53:04:1917175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [56.1859s] [ 28%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_fsdp_compile [2025-09-19 11:53:49.355] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:53:49.368] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:53:59:1918127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:53:59:1918127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:54:00:1918128 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:54:00:1918128 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [73.9129s] [ 32%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_multiple_optimizers [2025-09-19 11:55:03.301] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:55:03.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:55:13:1919714 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:55:13:1919714 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:55:14:1919713 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:55:14:1919713 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [53.8825s] [ 36%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsUnshardReshard::test_multiple_forward_offload_params_False [2025-09-19 11:55:57.220] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:55:57.228] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:56:07:1920648 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:56:07:1920648 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:56:08:1920649 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:56:08:1920649 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [42.4676s] [ 40%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsUnshardReshard::test_multiple_forward_offload_params_True [2025-09-19 11:56:39.623] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:56:39.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:56:49:1921583 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:56:49:1921583 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:56:51:1921584 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:56:51:1921584 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [42.2650s] [ 44%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsUnshardReshard::test_summon_between_two_forwards_offload_params_False [2025-09-19 11:57:21.972] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:57:21.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:57:32:1922707 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:57:32:1922707 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:57:33:1922708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:57:33:1922708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [42.6673s] [ 48%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsUnshardReshard::test_summon_between_two_forwards_offload_params_True [2025-09-19 11:58:04.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:58:04.636] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:58:14:1923641 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:58:14:1923641 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:58:16:1923642 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:58:16:1923642 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [42.5664s] [ 52%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsParamAccess::test_access_params_after_forward [2025-09-19 11:58:47.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:58:47.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:58:57:1924765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:58:57:1924765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:58:58:1924766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:58:58:1924766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [41.0650s] [ 56%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsWriteback::test_grad_writeback [2025-09-19 11:59:28.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 11:59:28.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-11:59:38:1925701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:59:38:1925701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-11:59:39:1925700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-11:59:39:1925700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [41.5639s] [ 60%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsWriteback::test_no_reshard_and_mixed_precision [2025-09-19 12:00:09.836] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:00:09.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:00:19:1926635 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:00:19:1926635 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:00:21:1926636 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:00:21:1926636 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [41.3663s] [ 64%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsWriteback::test_param_writeback [2025-09-19 12:00:51.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:00:51.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:01:01:1927567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:01:1927567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:01:02:1927568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:02:1927568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [26.0433s] [ 68%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsWriteback::test_writeback_between_fwd_and_bwd_for_no_reshard_raises [2025-09-19 12:01:17.175] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:01:17.195] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:01:27:1928494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:27:1928494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:01:28:1928493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:28:1928493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [25.9409s] [ 72%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsWriteback::test_writeback_shape_mismatch [2025-09-19 12:01:43.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:01:43.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:01:53:1929426 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:53:1929426 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:01:54:1929427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:01:54:1929427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [25.7418s] [ 76%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsFQNs::test_named_parameters_in_forward [2025-09-19 12:02:08.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:02:08.889] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:02:08.896] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:02:08.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:02:19:1930354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:19:1930354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:02:20:1930352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:20:1930352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:02:21:1930353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:21:1930353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:02:23:1930351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:23:1930351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [29.4507s] [ 80%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsNoSync::test_no_sync_correctness [2025-09-19 12:02:38.311] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:02:38.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:02:48:1932203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:48:1932203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:02:49:1932202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:02:49:1932202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [41.1655s] [ 84%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsNoSync::test_no_sync_mixed_precision [2025-09-19 12:03:19.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:03:19.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:03:29:1933138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:03:29:1933138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:03:30:1933137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:03:30:1933137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [41.0508s] [ 88%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsInit::test_non_uniform_requires_grad [2025-09-19 12:04:00.543] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:00.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:00.556] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:00.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:04:10:1934073 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:10:1934073 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:12:1934074 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:12:1934074 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:13:1934075 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:13:1934075 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:14:1934072 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:14:1934072 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [29.5517s] [ 92%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestMultiTensorApply::test_multi_tensor_apply_size0_tensors_cpu PASSED [0.0041s] [ 96%]
../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestMultiTensorApply::test_multi_tensor_apply_size0_tensors_cuda PASSED [0.0169s] [100%]

=================================== FAILURES ===================================
_ TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_full_shard _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 289, in test_diff_hyperparams
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 376, in _test_diff_hyperparams
    self._check_train_parity(
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 199, in _check_train_parity
    torch.testing.assert_close(iter_losses[0], iter_losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 0.5508145093917847 but got 0.4790288209915161.
Absolute difference: 0.07178568840026855 (up to 1e-05 allowed)
Relative difference: 0.13032642963515084 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_use_orig_params.py TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_full_shard

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 289, in test_diff_hyperparams
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 376, in _test_diff_hyperparams
    self._check_train_parity(
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 199, in _check_train_parity
    torch.testing.assert_close(iter_losses[0], iter_losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 0.5508145093917847 but got 0.4790288209915161.
Absolute difference: 0.07178568840026855 (up to 1e-05 allowed)
Relative difference: 0.13032642963515084 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_use_orig_params.py TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_full_shard

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 11:45:36.205000 1911096 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1914361
I0919 11:45:36.206000 1911096 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1914362
_ TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_shard_grad_op _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.02375054359436 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 11:47:51.810000 1911096 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 1916231
I0919 11:47:51.810000 1911096 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 1916232
E0919 11:52:51.831000 1911096 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f19268ad640 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f19260ac640 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f19270ae640 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f19278af640 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   <no Python frame>
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f192e14f640 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f1a6d931e00 (most recent call first):
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/autograd/graph.py", line 841 in _engine_run_backward
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354 in backward
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/_tensor.py", line 625 in backward
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 324 in run_backward
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 192 in _check_train_parity
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 376 in _test_diff_hyperparams
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184 in run_subtests
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188 in run_subtests
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 289 in test_diff_hyperparams
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222 in wrapper
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552 in instantiated_test
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1237 in _run
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 11:52:51.833000 1911096 site-packages/torch/testing/_internal/common_distributed.py:965] 
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_fsdp_use_orig_params.py.xml -
=========================== short test summary info ============================
FAILED [55.7854s] ../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_sharding_strategy_str_full_shard - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 289, in test_diff_hyperparams
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 376, in _test_diff_hyperparams
    self._check_train_parity(
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 199, in _check_train_parity
    torch.testing.assert_close(iter_losses[0], iter_losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 0.5508145093917847 but got 0.4790288209915161.
Absolute difference: 0.07178568840026855 (up to 1e-05 allowed)
Relative difference: 0.13032642963515084 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_use_orig_params.py TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_full_shard

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 289, in test_diff_hyperparams
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 376, in _test_diff_hyperparams
    self._check_train_parity(
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/fsdp/test_fsdp_use_orig_params.py", line 199, in _check_train_parity
    torch.testing.assert_close(iter_losses[0], iter_losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Expected 0.5508145093917847 but got 0.4790288209915161.
Absolute difference: 0.07178568840026855 (up to 1e-05 allowed)
Relative difference: 0.13032642963515084 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_use_orig_params.py TestFSDPUseOrigParamsMultipleParamGroups.test_diff_hyperparams_sharding_strategy_str_full_shard

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [300.0263s] ../../../../test/distributed/fsdp/test_fsdp_use_orig_params.py::TestFSDPUseOrigParamsMultipleParamGroups::test_diff_hyperparams_sharding_strategy_str_shard_grad_op - RuntimeError: Process 0 terminated or timed out after 300.02375054359436 seconds
================== 2 failed, 23 passed in 1292.30s (0:21:32) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:04:31.919] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_False_xpu [2025-09-19 12:04:34.187] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:34.211] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:34.303] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:04:34.311] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:04:34:1935998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:34:1935998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:34:1936000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:34:1936000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:34:1935999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:34:1935999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:34:1935997 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:04:34:1935997 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:04:50:1936317:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:04:50:1936307:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:04:51:1936314:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:04:51:1936312:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:03:1935997:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:03:1935999:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:03:1936000:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:03:1935998:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.3610s] [ 12%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_model_load_state_dict_offload_to_cpu_True_xpu [2025-09-19 12:05:06.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:06.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:06.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:06.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:05:06:1936333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:06:1936330 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:06:1936333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:06:1936330 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:06:1936332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:06:1936332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:06:1936331 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:06:1936331 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:23:1936650:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:23:1936640:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:23:1936647:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:23:1936645:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:35:1936330:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:35:1936333:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:35:1936332:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:35:1936331:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.0574s] [ 25%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_False_xpu [2025-09-19 12:05:38.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:38.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:38.516] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:05:38.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:05:38:1936666 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:38:1936666 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:38:1936664 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:38:1936664 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:38:1936665 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:38:1936665 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:38:1936663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:05:38:1936663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:05:55:1936987:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:55:1936973:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:55:1936984:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:05:55:1936980:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:07:1936666:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:07:1936663:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:07:1936664:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:07:1936665:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.7409s] [ 37%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_optim_load_state_dict_offload_to_cpu_True_xpu [2025-09-19 12:06:11.251] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:11.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:11.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:11.286] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:06:11:1936999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:11:1936999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:11:1936998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:11:1936998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:11:1937001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:11:1937001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:11:1937000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:11:1937000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:28:1937310:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:28:1937317:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:28:1937307:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:28:1937321:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:40:1936998:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:40:1937000:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:40:1936999:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:06:40:1937001:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.3572s] [ 50%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_False_xpu [2025-09-19 12:06:43.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:43.656] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:43.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:06:43.679] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:06:43:1937330 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:43:1937330 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:43:1937331 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:43:1937331 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:44:1937333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:44:1937333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:06:44:1937332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:06:44:1937332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:00:1937642:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:00:1937648:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:00:1937639:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:00:1937653:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:12:1937333:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:12:1937331:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:12:1937332:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:12:1937330:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937330:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937333:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937332:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937331:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937639:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937653:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937642:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:13:1937648:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.8577s] [ 62%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_dtensor_sharded_tensor_state_dict_identical_offload_to_cpu_True_xpu [2025-09-19 12:07:16.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:16.501] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:16.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:16.547] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:07:16:1937683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:16:1937683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:16:1937682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:16:1937682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:16:1937684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:16:1937684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:16:1937681 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:16:1937681 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:33:1937998:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:33:1938004:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:33:1937993:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:33:1937991:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:45:1937681:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:45:1937684:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:45:1937682:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:45:1937683:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937683:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937684:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937682:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937681:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937993:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937991:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1937998:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:07:46:1938004:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.9581s] [ 75%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_hsdp_init_with_device_mesh_xpu [2025-09-19 12:07:49.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:49.471] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:49.471] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:07:49.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:07:49:1938030 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:49:1938030 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:49:1938029 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:49:1938029 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:49:1938031 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:49:1938031 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:07:49:1938032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:07:49:1938032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:06:1938349:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:06:1938339:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:06:1938342:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:06:1938352:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:18:1938032:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:18:1938029:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:18:1938030:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:18:1938031:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.0488s] [ 87%]
../../../../test/distributed/fsdp/test_hsdp_dtensor_state_dict.py::TestHSDPWithDeviceMeshAndDTensorXPU::test_root_module_is_not_FSDP_xpu [2025-09-19 12:08:21.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:08:21.508] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:08:21.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:08:21.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:08:21:1938363 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:21:1938363 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:21:1938366 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:21:1938366 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:21:1938364 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:21:1938364 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:21:1938365 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:21:1938365 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:34:1938364:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:34:1938363:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:34:1938365:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:34:1938366:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:50:1938689:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:50:1938679:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:50:1938676:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:08:50:1938686:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [31.9505s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_hsdp_dtensor_state_dict.py.xml -
======================== 8 passed in 261.52s (0:04:21) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:08:54.503] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/fsdp/test_shard_utils.py::TestShardUtilsDistributed::test_create_chunk_sharded_tensor [2025-09-19 12:08:56.764] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:08:56.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:08:57:1938770 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:57:1938770 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:08:57:1938771 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:08:57:1938771 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [15.2271s] [ 50%]
../../../../test/distributed/fsdp/test_shard_utils.py::TestShardUtilsDistributedDTensor::test_create_chunk_dtensor [2025-09-19 12:09:11.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:11.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:09:12:1938922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:12:1938922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:12:1938921 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:12:1938921 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.0205s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_shard_utils.py.xml -
============================== 2 passed in 32.25s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:09:27.691] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 5 items
Running 5 items in this shard

../../../../test/distributed/fsdp/test_utils.py::TestUtilsXPU::test_apply_to_tensors_cpu_xpu_xpu PASSED [0.0051s] [ 20%]
../../../../test/distributed/fsdp/test_utils.py::TestUtilsXPU::test_apply_to_tensors_device_list0_xpu PASSED [0.0016s] [ 40%]
../../../../test/distributed/fsdp/test_utils.py::TestUtilsXPU::test_apply_to_tensors_device_list1_xpu PASSED [0.0017s] [ 60%]
../../../../test/distributed/fsdp/test_utils.py::TestUtilsXPU::test_packed_sequence_xpu PASSED [0.0020s] [ 80%]
../../../../test/distributed/fsdp/test_utils.py::TestUtilsXPU::test_replace_by_prefix_xpu PASSED [0.0005s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_utils.py.xml -
============================== 5 passed in 2.21s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:09:30.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 52 items
Running 52 items in this shard

../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_bn_always_wrapped_individually [2025-09-19 12:09:32.347] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:32.351] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:32.351] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:32.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:09:33:1939219 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:33:1939219 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:33:1939217 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:33:1939217 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:33:1939218 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:33:1939218 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:33:1939220 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:33:1939220 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [15.8381s] [  1%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_error_already_wrapped_nested_False_device_init_mode0 [2025-09-19 12:09:48.189] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:48.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:48.205] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:09:48.211] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:09:49:1939518 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:49:1939518 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:49:1939521 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:49:1939521 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:49:1939519 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:49:1939519 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:09:49:1939520 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:09:49:1939520 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [15.7300s] [  3%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_error_already_wrapped_nested_False_device_init_mode1 [2025-09-19 12:10:03.905] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:03.905] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:03.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:03.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:10:04:1939820 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:04:1939820 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:04:1939818 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:04:1939818 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:04:1939821 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:04:1939821 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:04:1939819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:04:1939819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [15.7296s] [  5%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_error_already_wrapped_nested_True_device_init_mode0 [2025-09-19 12:10:19.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:19.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:19.771] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:19.783] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:10:20:1940120 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:20:1940120 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:20:1940122 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:20:1940122 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:20:1940119 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:20:1940119 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:20:1940121 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:20:1940121 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [15.9297s] [  7%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_error_already_wrapped_nested_True_device_init_mode1 [2025-09-19 12:10:35.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:35.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:35.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:35.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:10:36:1940421 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:36:1940421 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:36:1940420 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:36:1940420 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:36:1940422 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:36:1940422 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:36:1940419 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:36:1940419 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [15.7293s] [  9%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch0_forward_prefetch_False_device_init_mode0 [2025-09-19 12:10:51.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:51.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:51.345] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:10:51.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:10:52:1940720 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:52:1940720 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:52:1940719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:52:1940719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:52:1940721 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:52:1940721 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:10:52:1940722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:10:52:1940722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.2551s] [ 11%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch0_forward_prefetch_False_device_init_mode1 [2025-09-19 12:11:22.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:22.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:22.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:22.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:11:23:1941038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:23:1941038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:23:1941037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:23:1941037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:23:1941039 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:23:1941039 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:23:1941036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:23:1941036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.2549s] [ 13%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch0_forward_prefetch_True_device_init_mode0 [2025-09-19 12:11:53.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:53.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:53.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:11:53.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:11:54:1941354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:54:1941354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:54:1941353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:54:1941353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:54:1941356 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:54:1941356 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:11:54:1941355 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:11:54:1941355 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.2561s] [ 15%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch0_forward_prefetch_True_device_init_mode1 [2025-09-19 12:12:25.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:25.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:25.112] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:25.115] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:12:25:1941671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:25:1941671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:25:1941674 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:25:1941674 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:25:1941672 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:25:1941672 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:26:1941673 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:26:1941673 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.3562s] [ 17%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch1_forward_prefetch_False_device_init_mode0 [2025-09-19 12:12:56.428] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:56.428] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:56.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:12:56.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:12:57:1941992 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:57:1941992 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:57:1941990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:57:1941990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:57:1941991 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:57:1941991 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:12:57:1941989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:12:57:1941989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [31.0555s] [ 19%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch1_forward_prefetch_False_device_init_mode1 [2025-09-19 12:13:27.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:27.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:27.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:27.525] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:13:28:1942309 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:28:1942309 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:28:1942308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:28:1942308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:28:1942307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:28:1942307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:28:1942310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:28:1942310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.2552s] [ 21%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch1_forward_prefetch_True_device_init_mode0 [2025-09-19 12:13:58.720] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:58.720] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:58.720] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:13:58.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:13:59:1942627 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:59:1942627 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:59:1942628 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:59:1942628 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:59:1942625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:59:1942625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:13:59:1942626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:13:59:1942626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [31.1547s] [ 23%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload0_backward_prefetch1_forward_prefetch_True_device_init_mode1 [2025-09-19 12:14:29.900] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:14:29.909] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:14:29.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:14:29.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:14:30:1942949 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:14:30:1942947 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:14:30:1942949 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:14:30:1942947 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:14:30:1942950 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:14:30:1942950 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:14:30:1942948 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:14:30:1942948 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.4560s] [ 25%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch0_forward_prefetch_False_device_init_mode0 [2025-09-19 12:15:01.355] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:01.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:01.368] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:01.387] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:15:02:1943267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:02:1943267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:02:1943269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:02:1943269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:02:1943270 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:02:1943270 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:02:1943268 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:02:1943268 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.6293s] [ 26%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch0_forward_prefetch_False_device_init_mode1 [2025-09-19 12:15:16.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:17.000] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:17.021] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:17.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:15:17:1943574 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:17:1943574 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:17:1943572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:17:1943572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:17:1943573 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:17:1943573 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:18:1943575 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:18:1943575 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.1546s] [ 28%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch0_forward_prefetch_True_device_init_mode0 [2025-09-19 12:15:48.148] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:48.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:48.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:15:48.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:15:49:1943889 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:49:1943889 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:49:1943890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:49:1943890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:49:1943891 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:49:1943891 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:15:49:1943892 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:15:49:1943892 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [15.9297s] [ 30%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch0_forward_prefetch_True_device_init_mode1 [2025-09-19 12:16:04.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:04.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:04.085] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:04.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:16:04:1944192 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:04:1944192 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:04:1944193 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:04:1944193 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:05:1944191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:05:1944191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:05:1944194 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:05:1944194 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.4558s] [ 32%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch1_forward_prefetch_False_device_init_mode0 [2025-09-19 12:16:35.520] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:35.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:35.543] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:35.544] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:16:36:1944509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:36:1944509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:36:1944512 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:36:1944512 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:36:1944511 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:36:1944511 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:36:1944510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:36:1944510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [15.8294s] [ 34%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch1_forward_prefetch_False_device_init_mode1 [2025-09-19 12:16:51.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:51.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:51.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:16:51.368] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:16:52:1944813 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:52:1944814 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:52:1944813 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:52:1944814 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:52:1944811 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:52:1944811 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:16:52:1944812 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:16:52:1944812 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [30.9540s] [ 36%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch1_forward_prefetch_True_device_init_mode0 [2025-09-19 12:17:22.320] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:22.323] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:22.324] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:22.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:17:23:1945134 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:23:1945134 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:23:1945132 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:23:1945132 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:23:1945135 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:23:1945135 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:23:1945133 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:23:1945133 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.7301s] [ 38%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_main_wrap_api_cpu_offload1_backward_prefetch1_forward_prefetch_True_device_init_mode1 [2025-09-19 12:17:38.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:38.057] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:38.057] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:17:38.060] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:17:38:1945432 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:38:1945432 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:38:1945433 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:38:1945433 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:38:1945435 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:38:1945435 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:17:38:1945434 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:17:38:1945434 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [30.8543s] [ 40%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_wrap_batchnorm_individually_use_or_policy_False [2025-09-19 12:18:08.900] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:08.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:08.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:08.921] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:18:09:1945754 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:09:1945754 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:09:1945751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:09:1945751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:09:1945753 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:09:1945753 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:09:1945752 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:09:1945752 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.8300s] [ 42%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_wrap_batchnorm_individually_use_or_policy_True [2025-09-19 12:18:24.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:24.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:24.767] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:24.779] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:18:25:1946054 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:25:1946054 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:25:1946051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:25:1946051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:25:1946052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:25:1946052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:25:1946053 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:25:1946053 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [15.9312s] [ 44%]
../../../../test/distributed/fsdp/test_wrap.py::TestFSDPWrap::test_zero_argument [2025-09-19 12:18:40.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:40.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:40.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:18:40.677] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:18:41:1946353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:41:1946353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:41:1946354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:41:1946354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:41:1946352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:41:1946352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:18:41:1946351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:41:1946351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [15.8288s] [ 46%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_always_wrap PASSED [0.0373s] [ 48%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_always_wrap_with_ignored_modules_wrap_method0 SKIPPED [0.0002s] [ 50%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_always_wrap_with_ignored_modules_wrap_method1 SKIPPED [0.0001s] [ 51%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_api SKIPPED [0.0001s] [ 53%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_preset_exclude_wrap SKIPPED [0.0001s] [ 55%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_preset_exclude_wrap_include_children SKIPPED [0.0001s] [ 57%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_preset_force_leaf SKIPPED [0.0002s] [ 59%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_preset_force_leaf_custom SKIPPED [0.0001s] [ 61%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode0_cpu_offload0_use_device_id_False 2025:09:19-12:18:55:1946653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:18:55:1946653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [0.6272s] [ 63%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode0_cpu_offload0_use_device_id_True 2025:09:19-12:18:56:1946653:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [0.2719s] [ 65%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode0_cpu_offload1_use_device_id_False 2025:09:19-12:18:56:1946653:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [0.2713s] [ 67%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode0_cpu_offload1_use_device_id_True 2025:09:19-12:18:56:1946653:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [0.2792s] [ 69%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode1_cpu_offload0_use_device_id_False 2025:09:19-12:18:56:1946653:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [0.2710s] [ 71%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode1_cpu_offload0_use_device_id_True 2025:09:19-12:18:57:1946653:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [0.2709s] [ 73%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode1_cpu_offload1_use_device_id_False PASSED [0.0004s] [ 75%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_smoke_test_device_init_mode1_cpu_offload1_use_device_id_True PASSED [0.0003s] [ 76%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_with_ignored_modules_wrap_method0 SKIPPED [0.0002s] [ 78%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_auto_wrap_with_ignored_modules_wrap_method1 SKIPPED [0.0001s] [ 80%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_custom_policy SKIPPED [0.0001s] [ 82%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_frozen_params SKIPPED [0.0001s] [ 84%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_module_wrap_policy SKIPPED [0.0005s] [ 86%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_module_wrap_policy_callable SKIPPED [0.0001s] [ 88%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_transformer_auto_wrap_policy SKIPPED [0.0001s] [ 90%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_wrap_disabled_outside_context SKIPPED [0.0001s] [ 92%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_wrap_override_defaults SKIPPED [0.0001s] [ 94%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_wrap_wrap_method0 SKIPPED [0.0001s] [ 96%]
../../../../test/distributed/fsdp/test_wrap.py::TestAutoWrap::test_wrap_wrap_method1 SKIPPED [0.0001s] [ 98%]
../../../../test/distributed/fsdp/test_wrap.py::TestWrapUtils::test_validate_frozen_params PASSED [0.0118s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_fsdp_test_wrap.py.xml -
================== 34 passed, 18 skipped in 568.34s (0:09:28) ==================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:19:00.175] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/test_backends.py::TestMiscCollectiveUtilsCPU::test_create_pg_cpu [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.0128s] [ 25%]
../../../../test/distributed/test_backends.py::TestMiscCollectiveUtilsCPU::test_device_to_backend_mapping_cpu PASSED [0.0005s] [ 50%]
../../../../test/distributed/test_backends.py::TestMiscCollectiveUtilsXPU::test_create_pg_xpu PASSED [0.0014s] [ 75%]
../../../../test/distributed/test_backends.py::TestMiscCollectiveUtilsXPU::test_device_to_backend_mapping_xpu PASSED [0.0004s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_backends.py.xml -
============================== 4 passed in 2.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:19:02.825] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 27 items
Running 27 items in this shard

../../../../test/distributed/test_c10d_common.py::TimeoutTest::test_store_based_barrier PASSED [31.1942s] [  3%]
../../../../test/distributed/test_c10d_common.py::ComputeBucketAssignmentTest::test_multi_limit_multi_dtype PASSED [0.0010s] [  7%]
../../../../test/distributed/test_c10d_common.py::ComputeBucketAssignmentTest::test_multi_limit_single_dtype PASSED [0.0006s] [ 11%]
../../../../test/distributed/test_c10d_common.py::ComputeBucketAssignmentTest::test_single_limit_multi_dtype PASSED [0.0006s] [ 14%]
../../../../test/distributed/test_c10d_common.py::ComputeBucketAssignmentTest::test_single_limit_single_dtype PASSED [0.0006s] [ 18%]
../../../../test/distributed/test_c10d_common.py::CommTest::test_debug_level [2025-09-19 12:19:36.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:36.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.7075s] [ 22%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_abort [2025-09-19 12:19:38.700] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:38.709] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:38.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:38.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8088s] [ 25%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_backend_class_attr [2025-09-19 12:19:41.495] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:41.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:41.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:41.532] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9091s] [ 29%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_backend_config [2025-09-19 12:19:44.409] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:44.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:44.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:44.420] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9087s] [ 33%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_canonicalize_helper [2025-09-19 12:19:47.309] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:47.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:47.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:47.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1088s] [ 37%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_collectives [2025-09-19 12:19:50.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:50.447] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:50.455] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:50.456] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9086s] [ 40%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_get_backend_name [2025-09-19 12:19:53.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:53.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:53.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:53.356] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9087s] [ 44%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_init_process_group_with_multiple_backends [2025-09-19 12:19:56.247] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:56.264] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:56.264] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:56.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9090s] [ 48%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_is_backend_available [2025-09-19 12:19:59.155] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:59.172] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:59.173] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:19:59.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9086s] [ 51%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_send_recv [2025-09-19 12:20:02.060] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:02.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:02.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:02.085] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.3093s] [ 55%]
../../../../test/distributed/test_c10d_common.py::PythonProcessGroupExtensionTest::test_shutdown [2025-09-19 12:20:05.364] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:05.371] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:05.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:05.409] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1094s] [ 59%]
../../../../test/distributed/test_c10d_common.py::ProcessGroupWithDispatchedCollectivesTests::test_default_process_group [2025-09-19 12:20:08.475] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [4.0078s] [ 62%]
../../../../test/distributed/test_c10d_common.py::ProcessGroupWithDispatchedCollectivesTests::test_init_process_group_for_all_backends [2025-09-19 12:20:12.488] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [2.8064s] [ 66%]
../../../../test/distributed/test_c10d_common.py::ProcessGroupWithDispatchedCollectivesTests::test_init_process_group_optional_backend [2025-09-19 12:20:15.377] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.6059s] [ 70%]
../../../../test/distributed/test_c10d_common.py::ReduceOpTest::test_op_isinstance_of_reduceop PASSED [0.0009s] [ 74%]
../../../../test/distributed/test_c10d_common.py::ReduceOpTest::test_reduceop_copyable PASSED [0.0020s] [ 77%]
../../../../test/distributed/test_c10d_common.py::ReduceOpTest::test_reduceop_equal PASSED [0.0021s] [ 81%]
../../../../test/distributed/test_c10d_common.py::ReduceOpTest::test_reduceop_pickle PASSED [0.0027s] [ 85%]
../../../../test/distributed/test_c10d_common.py::LocalRankTest::testNodeLocalRank [2025-09-19 12:20:17.907] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:17.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:17.924] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:17.951] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9089s] [ 88%]
../../../../test/distributed/test_c10d_common.py::LocalRankTest::testNodeLocalRankOverridesFallback [2025-09-19 12:20:20.823] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:20.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:20.843] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:20.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9087s] [ 92%]
../../../../test/distributed/test_c10d_common.py::LocalRankTest::testWithoutEnv [2025-09-19 12:20:23.833] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:23.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:23.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:23.867] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9087s] [ 96%]
../../../../test/distributed/test_c10d_common.py::LocalRankTest::testWithoutEnvWithFallback [2025-09-19 12:20:26.661] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:26.664] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:26.680] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:26.699] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9087s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_c10d_common.py.xml -
======================== 27 passed in 86.78s (0:01:26) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:20:31.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/test_c10d_logger.py::C10dErrorLoggerTest::test_exception_logger [2025-09-19 12:20:33.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:33.176] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:33.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:33.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:20:33:1951327 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:33:1951327 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:33:1951328 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:33:1951328 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:33:1951326 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:33:1951326 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:33:1951325 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:33:1951325 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0080s] [ 50%]
../../../../test/distributed/test_c10d_logger.py::C10dErrorLoggerTest::test_get_or_create_logger [2025-09-19 12:20:49.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:49.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:49.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:49.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8079s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_c10d_logger.py.xml -
============================== 2 passed in 20.71s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:20:52.755] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 18 items
Running 18 items in this shard

../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_all_gather_object_cpu [2025-09-19 12:20:54.935] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:54.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:54.972] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:54.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:20:55:1951984 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:55:1951984 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:55:1951982 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:55:1951982 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:55:1951985 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:55:1951985 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:55:1951983 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:55:1951983 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.3115s] [  5%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_broadcast_object_list_cpu [2025-09-19 12:20:59.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:59.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:59.256] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:20:59.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:20:59:1952283 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:59:1952283 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:59:1952284 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:59:1952284 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:59:1952285 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:59:1952285 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:20:59:1952282 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:20:59:1952282 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.0093s] [ 11%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_gather_object_cpu [2025-09-19 12:21:03.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:03.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:03.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:03.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:03:1952582 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:03:1952582 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:03:1952584 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:03:1952584 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:03:1952583 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:03:1952583 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:03:1952585 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:03:1952585 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1106s] [ 16%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_scatter_object_list_cpu [2025-09-19 12:21:07.360] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:07.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:07.394] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:07.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:07:1952883 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:07:1952883 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:07:1952885 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:07:1952885 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:07:1952884 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:07:1952884 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:07:1952882 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:07:1952882 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1104s] [ 22%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_send_recv_object_list_cpu [2025-09-19 12:21:11.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:11.521] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:11.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:11.538] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:11:1953184 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:11:1953184 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:11:1953183 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:11:1953183 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.6099s] [ 27%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_subpg_all_gather_object_cpu [2025-09-19 12:21:15.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:15.159] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:15.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:15.192] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:15:1953480 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:15:1953480 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:15:1953478 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:15:1953478 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:15:1953477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:15:1953477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:15:1953479 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:15:1953479 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1105s] [ 33%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_subpg_broadcast_object_cpu [2025-09-19 12:21:19.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:19.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:19.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:19.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:19:1953782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:19:1953782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:19:1953783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:19:1953783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:19:1953781 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:19:1953781 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:19:1953784 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:19:1953784 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.9106s] [ 38%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_subpg_gather_object_cpu [2025-09-19 12:21:23.124] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:23.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:23.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:23.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:23:1954088 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:23:1954088 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:23:1954086 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:23:1954086 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:23:1954085 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:23:1954085 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:23:1954087 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:23:1954087 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.0106s] [ 44%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_subpg_scatter_object_cpu [2025-09-19 12:21:27.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:27.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:27.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:27.167] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:27:1954392 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:27:1954392 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:27:1954390 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:27:1954390 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:27:1954391 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:27:1954391 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:27:1954389 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:27:1954389 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.9104s] [ 50%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_all_gather_object_xpu [2025-09-19 12:21:31.029] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:31.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:31.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:31.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:31:1954693 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:31:1954693 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:31:1954696 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:31:1954696 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:31:1954695 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:31:1954695 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:31:1954694 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:31:1954694 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.3112s] [ 55%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_broadcast_object_list_xpu [2025-09-19 12:21:35.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:35.359] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:35.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:35.492] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:35:1954995 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:35:1954995 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:35:1954994 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:35:1954994 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:35:1954993 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:35:1954993 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:35:1954996 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:35:1954996 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1115s] [ 61%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_gather_object_xpu [2025-09-19 12:21:39.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:39.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:39.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:39.508] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:39:1955296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:39:1955296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:39:1955294 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:39:1955294 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:39:1955295 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:39:1955295 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:39:1955293 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:39:1955293 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1110s] [ 66%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_scatter_object_list_xpu [2025-09-19 12:21:43.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:43.620] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:43.625] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:43.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:43:1955597 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:43:1955597 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:43:1955594 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:43:1955594 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:44:1955596 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:44:1955596 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:44:1955595 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:44:1955595 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.0108s] [ 72%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_send_recv_object_list_xpu [2025-09-19 12:21:47.589] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:47.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:47.616] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:47.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:47:1955895 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:47:1955895 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:48:1955894 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:48:1955894 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.7107s] [ 77%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_subpg_all_gather_object_xpu [2025-09-19 12:21:51.291] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:51.316] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:51.325] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:51.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:51:1956189 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:51:1956189 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:51:1956190 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:51:1956190 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:51:1956188 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:51:1956188 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:51:1956191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:51:1956191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.2116s] [ 83%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_subpg_broadcast_object_xpu [2025-09-19 12:21:55.583] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:55.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:55.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:55.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:55:1956493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:55:1956493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:55:1956492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:55:1956492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:55:1956495 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:55:1956495 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:56:1956494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:56:1956494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.9110s] [ 88%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_subpg_gather_object_xpu [2025-09-19 12:21:59.416] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:59.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:59.429] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:21:59.436] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:21:59:1956796 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:59:1956799 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:59:1956799 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:59:1956796 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:59:1956798 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:59:1956798 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:21:59:1956797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:21:59:1956797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.0113s] [ 94%]
../../../../test/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_subpg_scatter_object_xpu [2025-09-19 12:22:03.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:22:03.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:22:03.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:22:03.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:22:03:1957103 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:03:1957103 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:22:03:1957104 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:03:1957104 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:22:03:1957105 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:03:1957105 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:22:03:1957102 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:03:1957102 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.8113s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_c10d_object_collectives.py.xml -
======================== 18 passed in 74.50s (0:01:14) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:22:08.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_grouped_scheduler_node [2025-09-19 12:22:19.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:22:19.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:22:33:1957867 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:33:1957867 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:22:33:1957866 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:22:33:1957866 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [29.3490s] [ 12%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_inductor_default_comms_ordering [2025-09-19 12:22:48.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:22:48.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [20.5325s] [ 25%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_nccl_heuristics [2025-09-19 12:23:09.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:23:09.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [13.5236s] [ 37%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_raise_comms [2025-09-19 12:23:22.569] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:23:22.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:23:35:1961051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:23:35:1961052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:23:35:1961051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:23:35:1961052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
final peak_memory=128
final peak_memory=128
PASSED [27.8375s] [ 50%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_reorder_compute_for_overlap [2025-09-19 12:23:50.432] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:23:50.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:24:05:1962003 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:24:05:1962003 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:24:05:1962002 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:24:05:1962002 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
final peak_memory=192
final peak_memory=192
PASSED [30.1483s] [ 62%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_reorder_compute_for_overlap_custom_runtime_estimation [2025-09-19 12:24:20.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:24:20.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:24:34:1963012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:24:34:1963012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:24:34:1963011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:24:34:1963011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
final peak_memory=192
final peak_memory=192
PASSED [29.0470s] [ 75%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_sink_waits [2025-09-19 12:24:49.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:24:49.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:25:03:1963944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:03:1963944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:25:03:1963943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:03:1963943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
final peak_memory=128
final peak_memory=128
PASSED [28.5444s] [ 87%]
../../../../test/distributed/test_compute_comm_reordering.py::TestComputeCommReorderingMultiProc::test_sink_waits_raise_comms [2025-09-19 12:25:18.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:25:18.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:25:33:1964876 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:33:1964876 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:25:33:1964877 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:33:1964877 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
final peak_memory=128
final peak_memory=128
final peak_memory=128
final peak_memory=128
PASSED [30.0467s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_compute_comm_reordering.py.xml -
======================== 8 passed in 220.04s (0:03:40) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:25:49.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 13 items
Running 13 items in this shard

../../../../test/distributed/test_control_collectives.py::TestCollectives::test_all_gather_timeout PASSED [0.1672s] [  7%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_all_sum PASSED [0.0056s] [ 15%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_all_sum_timeout PASSED [0.0051s] [ 23%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_barrier PASSED [0.0016s] [ 30%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_barrier_timeout PASSED [0.0044s] [ 38%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_broadcast PASSED [0.0020s] [ 46%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_broadcast_timeout PASSED [0.0018s] [ 53%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_gather PASSED [0.0026s] [ 61%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_gather_timeout PASSED [0.0040s] [ 69%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_scatter PASSED [0.0019s] [ 76%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_scatter_timeout PASSED [0.0017s] [ 84%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_simple_user_func PASSED [0.0020s] [ 92%]
../../../../test/distributed/test_control_collectives.py::TestCollectives::test_unique PASSED [0.0009s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_control_collectives.py.xml -
============================== 13 passed in 2.20s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:25:52.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 56 items
Running 56 items in this shard

../../../../test/distributed/test_device_mesh.py::DeviceMeshTestGlooBackend::test_device_mesh_reuse_default_group SKIPPED [0.0003s] [  1%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshSetDeviceTest::test_auto_set_device_from_heuristic [2025-09-19 12:25:54.847] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:25:54.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:25:54.860] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:25:54.865] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:25:55:1966026 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:55:1966026 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:25:55:1966024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:55:1966024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:25:55:1966025 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:55:1966025 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:25:55:1966023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:25:55:1966023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8104s] [  3%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshSetDeviceTest::test_auto_set_device_from_local_rank [2025-09-19 12:26:10.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:10.487] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:10.487] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:10.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:26:10:1966334 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:10:1966334 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:10:1966335 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:10:1966335 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:10:1966333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:10:1966333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:10:1966332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:10:1966332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [  5%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshSetDeviceTest::test_manual_set_device [2025-09-19 12:26:25.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:26.035] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:26.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:26.059] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:26:26:1966645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:26:1966645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:26:1966644 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:26:1966644 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:26:1966643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:26:1966643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:26:1966642 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:26:1966642 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0299s] [  7%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_2d_mesh_eager_init_subgroup [2025-09-19 12:26:42.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:42.030] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:42.040] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:42.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:26:42:1966951 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:42:1966951 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:42:1966953 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:42:1966953 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:42:1966952 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:42:1966952 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:42:1966954 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:42:1966954 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5282s] [  8%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_2d_mesh_non_eager_init_subgroup [2025-09-19 12:26:57.515] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:57.551] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:57.552] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:26:57.553] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:26:57:1967262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:57:1967262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:57:1967259 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:57:1967259 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:57:1967261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:57:1967261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:26:57:1967260 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:26:57:1967260 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5292s] [ 10%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_assert_invalid_mesh_tensor [2025-09-19 12:27:13.085] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:13.092] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:13.112] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:13.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:27:13:1967571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:13:1967571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:13:1967572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:13:1967572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:13:1967569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:13:1967569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:13:1967570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:13:1967570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [ 12%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_device_mesh_2d [2025-09-19 12:27:28.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:28.719] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:28.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:28.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:27:28:1967870 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:28:1967870 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:28:1967872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:28:1967872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:28:1967873 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:28:1967873 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:28:1967871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:28:1967871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5293s] [ 14%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_device_mesh_init_backend [2025-09-19 12:27:44.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:44.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:44.251] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:44.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:27:44:1968181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:44:1968181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:44:1968180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:44:1968180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:44:1968179 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:44:1968179 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:27:44:1968182 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:27:44:1968182 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6293s] [ 16%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_fake_pg_device_mesh [2025-09-19 12:27:59.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:59.887] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:59.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:27:59.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9084s] [ 17%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_from_group_with_global_pg [2025-09-19 12:28:02.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:02.915] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:02.915] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:02.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:28:03:1968765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:03:1968765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:03:1968767 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:03:1968767 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:03:1968766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:03:1968766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:03:1968764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:03:1968764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7293s] [ 19%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_from_group_with_invalid_mesh [2025-09-19 12:28:18.515] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:18.517] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:18.541] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:18.552] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:28:18:1969067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:18:1969067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:18:1969064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:18:1969064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:18:1969066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:18:1969066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:18:1969065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:18:1969065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5295s] [ 21%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_get_group_and_get_all_groups [2025-09-19 12:28:34.043] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:34.055] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:34.055] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:34.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:28:34:1969375 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:34:1969375 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:34:1969373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:34:1969373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:34:1969372 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:34:1969372 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:34:1969374 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:34:1969374 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6292s] [ 23%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_get_local_rank [2025-09-19 12:28:49.659] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:49.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:49.683] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:28:49.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:28:49:1969681 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:49:1969681 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:49:1969684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:49:1969684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:49:1969682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:49:1969682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:28:49:1969683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:28:49:1969683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4286s] [ 25%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_get_local_rank_raises_exception [2025-09-19 12:29:05.103] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:05.131] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:05.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:05.143] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:29:05:1969995 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:05:1969995 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:05:1969993 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:05:1969993 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:05:1969994 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:05:1969994 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:05:1969992 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:05:1969992 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5291s] [ 26%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_init_process_group [2025-09-19 12:29:20.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:20.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:20.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:20.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:29:21:1970304 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:21:1970304 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:21:1970303 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:21:1970303 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:21:1970305 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:21:1970305 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:21:1970302 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:21:1970302 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2303s] [ 28%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_raises_invalid_device_type [2025-09-19 12:29:36.855] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:36.885] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:36.885] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:36.885] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9089s] [ 30%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTest::test_set_mesh_dim_group_options [2025-09-19 12:29:39.783] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:39.783] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:39.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:39.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:29:39:1970897 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:39:1970897 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:39:1970899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:39:1970899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:40:1970900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:40:1970900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:40:1970898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:40:1970898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 32%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_device_mesh_hash [2025-09-19 12:29:55.307] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:55.307] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:55.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:29:55.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:29:55:1971201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:55:1971202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:55:1971201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:55:1971202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:55:1971204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:55:1971204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:29:55:1971203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:29:55:1971203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6303s] [ 33%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_device_mesh_nd [2025-09-19 12:30:10.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:10.949] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:10.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:10.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.9088s] [ 35%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_device_mesh_parent_child_hash [2025-09-19 12:30:13.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:13.889] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:13.889] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:13.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:30:14:1971819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:14:1971819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:14:1971821 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:14:1971821 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:14:1971822 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:14:1971822 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:14:1971820 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:14:1971820 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5292s] [ 37%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_from_group_with_mesh_shape_2d [2025-09-19 12:30:29.359] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:29.371] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:29.387] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:29.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:30:29:1972135 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:29:1972135 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:29:1972136 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:29:1972136 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:29:1972137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:29:1972137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:29:1972138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:29:1972138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [ 39%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_from_group_with_mesh_shape_3d [2025-09-19 12:30:44.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:45.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:45.011] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:30:45.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:30:45:1972451 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:45:1972451 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:45:1972453 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:45:1972453 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:45:1972454 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:45:1972454 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:30:45:1972452 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:30:45:1972452 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6293s] [ 41%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshTestNDim::test_get_local_rank_3d [2025-09-19 12:31:00.621] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:00.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:00.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:00.671] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8085s] [ 42%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_backend_override_argument_dict_with_idx_and_backend_eager [2025-09-19 12:31:03.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:03.444] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:03.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:03.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:31:03:1973054 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:03:1973054 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:03:1973052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:03:1973052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:03:1973053 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:03:1973053 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:03:1973051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:03:1973051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6297s] [ 44%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_backend_override_argument_dict_with_idx_and_backend_lazy [2025-09-19 12:31:19.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:19.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:19.095] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:19.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:31:19:1973358 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:19:1973358 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:19:1973356 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:19:1973356 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:19:1973357 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:19:1973357 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:19:1973359 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:19:1973359 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6272s] [ 46%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_backend_override_argument_dict_with_name_and_options [2025-09-19 12:31:34.667] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:34.735] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:34.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:34.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.7079s] [ 48%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_backend_override_argument_errors [2025-09-19 12:31:37.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:37.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:37.451] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:37.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:31:37:1973947 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:37:1973947 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:37:1973944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:37:1973944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:37:1973946 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:37:1973946 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:37:1973945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:37:1973945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6271s] [ 50%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_init_device_mesh [2025-09-19 12:31:53.039] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:53.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:53.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:31:53.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:31:53:1974244 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:53:1974244 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:53:1974246 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:53:1974246 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:53:1974247 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:53:1974247 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:31:53:1974245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:31:53:1974245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6264s] [ 51%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_raises_duplicate_mesh_dim_names [2025-09-19 12:32:08.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:08.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:08.697] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:08.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:32:08:1974562 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:08:1974562 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:08:1974563 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:08:1974563 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:08:1974564 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:08:1974564 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:08:1974561 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:08:1974561 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5267s] [ 53%]
../../../../test/distributed/test_device_mesh.py::InitDeviceMeshTest::test_raises_mesh_shape_mesh_dim_names_mismatch [2025-09-19 12:32:24.195] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:24.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:24.201] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:24.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:32:24:1974863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:24:1974863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:24:1974865 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:24:1974865 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:24:1974864 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:24:1974864 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:24:1974866 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:24:1974866 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5257s] [ 55%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_cache_and_reuse_submesh_slice_result [2025-09-19 12:32:39.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:39.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:39.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:39.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:32:39:1975163 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:39:1975163 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:39:1975166 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:39:1975166 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:39:1975165 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:39:1975165 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:40:1975164 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:40:1975164 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6230s] [ 57%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_flatten_mesh_1d [2025-09-19 12:32:55.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:55.381] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:55.394] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:32:55.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:32:55:1975475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:55:1975475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:55:1975472 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:55:1975472 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:55:1975474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:55:1975474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:32:55:1975473 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:32:55:1975473 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4290s] [ 58%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_flatten_mesh_3d [2025-09-19 12:33:10.751] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:10.787] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:10.841] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:10.863] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:33:10:1975772 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:10:1975772 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:11:1975774 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:11:1975774 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:11:1975773 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:11:1975775 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:11:1975773 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:11:1975775 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6299s] [ 60%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_flatten_mesh_4d [2025-09-19 12:33:26.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:26.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:26.453] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:26.455] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8082s] [ 62%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_get_item_1d [2025-09-19 12:33:29.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:29.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:29.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:29.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:33:29:1976384 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:29:1976384 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:29:1976382 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:29:1976382 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:29:1976383 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:29:1976383 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:29:1976381 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:29:1976381 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6300s] [ 64%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_get_item_2d [2025-09-19 12:33:44.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:44.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:44.860] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:33:44.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:33:45:1976682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:45:1976682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:45:1976683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:45:1976683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:45:1976684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:45:1976684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:33:45:1976685 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:33:45:1976685 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6299s] [ 66%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_get_item_3d [2025-09-19 12:34:00.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:00.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:00.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:00.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8083s] [ 67%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_get_item_3d_noncontiguous_slicing [2025-09-19 12:34:03.283] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:03.296] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:03.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:03.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8086s] [ 69%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_raises_invalid_mesh_dim_name [2025-09-19 12:34:06.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:06.144] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:06.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:06.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:34:06:1977568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:06:1977568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:06:1977567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:06:1977567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:06:1977569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:06:1977569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:06:1977570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:06:1977570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5219s] [ 71%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_raises_no_mesh_dim_found [2025-09-19 12:34:21.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:21.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:21.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:21.659] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:34:21:1977880 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:21:1977880 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:21:1977878 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:21:1977878 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:21:1977879 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:21:1977879 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:21:1977877 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:21:1977877 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6215s] [ 73%]
../../../../test/distributed/test_device_mesh.py::TestDeviceMeshGetItem::test_reconstruct_mesh_with_flatten_dim [2025-09-19 12:34:37.239] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:37.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:37.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:37.286] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8070s] [ 75%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_get_all_submeshes [2025-09-19 12:34:40.063] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:40.073] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:40.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:40.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:34:40:1978475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:40:1978475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:40:1978474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:40:1978474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:40:1978477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:40:1978477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:40:1978476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:40:1978476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4207s] [ 76%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_get_mesh_dim_by_name [2025-09-19 12:34:55.460] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:55.467] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:55.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:34:55.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:34:55:1978783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:55:1978783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:55:1978784 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:55:1978784 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:55:1978782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:55:1978782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:34:55:1978785 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:34:55:1978785 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7292s] [ 78%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_get_root_mesh [2025-09-19 12:35:11.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:11.228] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:11.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:11.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:35:11:1979093 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:11:1979093 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:11:1979091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:11:1979091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:11:1979092 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:11:1979092 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:11:1979090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:11:1979090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6293s] [ 80%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_get_root_mesh_dim_exist [2025-09-19 12:35:26.812] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:26.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:26.840] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:26.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:35:27:1979406 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:27:1979406 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:27:1979405 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:27:1979405 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:27:1979404 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:27:1979404 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:27:1979403 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:27:1979403 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 82%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_get_root_mesh_dim_not_exist [2025-09-19 12:35:42.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:42.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:42.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:42.388] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:35:42:1979711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:42:1979711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:42:1979714 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:42:1979714 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:42:1979713 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:42:1979713 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:42:1979712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:42:1979712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6299s] [ 83%]
../../../../test/distributed/test_device_mesh.py::TestMeshEnv::test_mesh_slice_fake_tensor_mode [2025-09-19 12:35:58.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:58.016] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:58.016] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:35:58.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:35:58:1980012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:58:1980012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:58:1980013 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:58:1980013 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:58:1980011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:58:1980011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:35:58:1980014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:35:58:1980014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5295s] [ 85%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_all_gather_uneven [2025-09-19 12:36:13.495] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:13.560] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:13.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:13.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:36:13:1980320 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:13:1980320 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:13:1980321 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:13:1980321 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:13:1980322 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:13:1980322 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:13:1980323 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:13:1980323 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8299s] [ 87%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_broadcast_1d [2025-09-19 12:36:29.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:29.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:29.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:29.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:36:29:1980620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:29:1980620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:29:1980622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:29:1980622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:29:1980621 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:29:1980621 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:29:1980623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:29:1980623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7301s] [ 89%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_broadcast_nd [2025-09-19 12:36:45.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:45.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:45.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:36:45.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:36:45:1980922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:45:1980922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:45:1980920 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:45:1980920 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:45:1980923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:45:1980923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:45:1980921 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:36:45:1980921 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:36:46:1980920:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980921:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980922:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980923:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980920:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980921:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980923:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980922:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980920:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980921:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980923:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:36:46:1980922:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.4309s] [ 91%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_reduce_scatter_contiguous [2025-09-19 12:37:01.575] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:01.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:01.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:01.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:37:01:1981247 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:01:1981247 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:01:1981245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:01:1981245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:01:1981246 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:01:1981246 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:01:1981248 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:01:1981248 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.8548s] [ 92%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_reduce_scatter_uneven [2025-09-19 12:37:32.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:32.403] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:32.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:37:32.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:37:32:1981549 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:32:1981549 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:32:1981546 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:32:1981546 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:32:1981548 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:32:1981548 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:37:32:1981547 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:37:32:1981547 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.7547s] [ 94%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_scatter_1d [2025-09-19 12:38:03.091] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:03.188] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:03.188] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:03.188] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:38:03:1981848 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:03:1981848 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:03:1981849 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:03:1981849 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:03:1981847 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:03:1981847 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:03:1981850 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:03:1981850 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7301s] [ 96%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_scatter_nd [2025-09-19 12:38:18.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:18.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:18.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:18.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:38:19:1982149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:19:1982149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:19:1982150 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:19:1982150 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:19:1982148 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:19:1982148 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:19:1982151 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:19:1982151 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:19:1982150:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:19:1982151:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:19:1982148:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:19:1982149:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982149:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982148:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982151:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982150:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982150:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982148:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982151:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:38:20:1982149:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.5306s] [ 98%]
../../../../test/distributed/test_device_mesh.py::DeviceMeshCollectiveTest::test_scatter_uneven [2025-09-19 12:38:35.422] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:35.435] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:35.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:38:35.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:38:35:1982475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:35:1982475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:35:1982476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:35:1982476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:35:1982474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:35:1982474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:38:35:1982477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:38:35:1982477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_device_mesh.py.xml -
================== 49 passed, 7 skipped in 778.54s (0:12:58) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:38:52.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 60 items
Running 60 items in this shard

../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_call_method_forward PASSED [5.7100s] [  1%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_ddp_optimizer_inductor_strides_dont_specialize PASSED [1.3187s] [  3%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_hf_bert_ddp_aot_eager PASSED [8.2901s] [  5%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_hf_bert_ddp_inductor PASSED [48.4115s] [  6%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_issue90375 PASSED [0.0574s] [  8%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_symbol_splitting PASSED [1.2551s] [ 10%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_unbacked_symbol_splitting_direct PASSED [1.0639s] [ 11%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_unbacked_symbol_splitting_indirect PASSED [1.5494s] [ 13%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_unbacked_symbol_splitting_no_binding PASSED [1.6532s] [ 15%]
../../../../test/distributed/test_dynamo_distributed.py::TestFakeDistributedSingleProc::test_unbacked_symbol_splitting_torture_multi PASSED [0.7399s] [ 16%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_asymmetric_compilation [2025-09-19 12:40:13.459] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:13.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:13.476] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:13.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [16.2343s]on) [ 18%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_asymmetric_compilation_with_fx_cache [2025-09-19 12:40:29.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:29.681] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:29.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:29.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [16.3325s]on) [ 20%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_scalar [2025-09-19 12:40:45.996] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:46.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:46.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:40:46.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:40:57:1988174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:40:57:1988174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:40:58:1988172 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:40:58:1988172 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:40:59:1988173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:40:59:1988173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:40:59:1988171 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:40:59:1988171 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:06:1988171:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:06:1988173:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:06:1988174:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:06:1988172:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [24.9457s] [ 21%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_speculation_divergence [2025-09-19 12:41:10.992] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:11.015] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:11.057] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:11.131] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:41:25:1990738 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:25:1990738 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:25:1990737 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:25:1990737 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:25:1990736 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:25:1990736 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:25:1990739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:25:1990739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:30:1990739:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:30:1990737:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:30:1990736:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:30:1990738:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [23.9448s] [ 23%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_tensor [2025-09-19 12:41:34.928] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:34.941] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:34.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:41:34.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:41:49:1993244 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:49:1993244 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:49:1993245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:49:1993245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:49:1993243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:49:1993243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:49:1993246 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:41:49:1993246 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:41:51:1993244:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:51:1993243:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:51:1993246:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:41:51:1993245:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [34.6621s] [ 25%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_dim_mismatch [2025-09-19 12:42:09.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:09.604] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:09.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:09.613] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:42:20:1995796 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:20:1995796 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:22:1995795 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:22:1995795 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:23:1995797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:23:1995797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:23:1995794 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:23:1995794 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:26:1995795:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:26:1995796:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:26:1995797:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:26:1995794:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [21.8411s] [ 26%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_graph_break_empty_graph_still_collective [2025-09-19 12:42:31.427] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:31.433] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:31.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:31.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:42:42:1998233 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:42:1998233 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:44:1998234 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:44:1998234 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:45:1998232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:45:1998232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:45:1998231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:42:45:1998231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:42:47:1998231:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:47:1998232:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:47:1998233:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:42:47:1998234:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
woof
woof
woof
woof
woof
woof
woof
woof
woof
woof
woof
woof
PASSED [22.0412s] [ 28%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_missing_source [2025-09-19 12:42:53.475] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:53.485] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:53.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:42:53.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:43:04:2000648 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:04:2000648 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:06:2000646 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:06:2000646 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:07:2000647 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:07:2000647 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:07:2000645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:07:2000645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:12:2000648:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:12:2000645:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:12:2000647:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:12:2000646:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [22.4437s] [ 30%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_scalar_missing_source [2025-09-19 12:43:15.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:15.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:16.012] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:16.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:43:27:2003132 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:27:2003132 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:28:2003131 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:28:2003131 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:29:2003130 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:29:2003130 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:29:2003129 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:29:2003129 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:34:2003129:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:34:2003131:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:34:2003132:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:43:34:2003130:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [22.8421s] [ 31%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_type_mismatch [2025-09-19 12:43:38.761] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:38.761] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:38.767] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:43:38.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:43:53:2005612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:53:2005612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:54:2005611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:54:2005611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:56:2005610 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:56:2005610 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:43:56:2005613 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:43:56:2005613 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:00:2005612:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:00:2005610:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:00:2005613:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:00:2005611:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [26.3489s] [ 33%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_ddp_activation_checkpointing [2025-09-19 12:44:05.187] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:05.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:05.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:05.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:44:19:2008092 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:19:2008092 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:20:2008090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:20:2008090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:20:2008089 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:20:2008089 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:20:2008091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:20:2008091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:21:2008089:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:21:2008090:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:21:2008092:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:21:2008091:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [24.0456s] [ 35%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_ddp_baseline_aot_eager_multiprocess [2025-09-19 12:44:29.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:29.220] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:29.295] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:29.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:44:40:2010522 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:40:2010522 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:42:2010523 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:42:2010523 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:43:2010521 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:43:2010521 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:43:2010524 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:44:43:2010524 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:44:44:2010523:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:44:2010524:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:44:2010522:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:44:44:2010521:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [19.3342s] [ 36%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_ddp_optimizer_cudagraph SKIPPED [0.0002s] [ 38%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_activation_checkpointing [2025-09-19 12:44:48.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:48.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:48.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:44:48.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:45:01:2012400 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:01:2012400 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:01:2012401 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:01:2012401 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:02:2012402 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:02:2012402 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:02:2012399 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:02:2012399 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:04:2012399:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:04:2012400:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:04:2012401:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:04:2012402:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [22.8414s] [ 40%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_aot_eager [2025-09-19 12:45:11.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:11.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:11.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:11.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:45:23:2014830 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:23:2014830 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:24:2014831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:24:2014831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:26:2014829 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:26:2014829 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:26:2014828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:26:2014828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:27:2014828:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:27:2014830:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:27:2014831:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:27:2014829:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [20.3351s] [ 41%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_inductor [2025-09-19 12:45:31.701] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:31.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:31.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:31.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:45:46:2016709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:46:2016709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:47:2016707 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:47:2016707 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:47:2016706 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:47:2016706 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:47:2016708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:45:47:2016708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:45:49:2016706:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:49:2016708:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:49:2016707:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:45:49:2016709:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [24.9732s] [ 43%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_setattr [2025-09-19 12:45:56.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:56.652] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:56.672] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:45:56.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:46:10:2019248 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:10:2019248 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:11:2019247 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:11:2019247 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:11:2019245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:11:2019245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:11:2019246 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:11:2019246 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:12:2019245:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:12:2019247:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:12:2019246:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:12:2019248:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [19.7384s] [ 45%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_unspecialized_forced_getattr_inline [2025-09-19 12:46:16.404] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:16.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:16.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:16.415] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:46:30:2021107 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:30:2021107 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:30:2021105 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:30:2021105 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:31:2021104 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:31:2021104 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:31:2021106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:31:2021106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:32:2021104:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:32:2021106:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:32:2021105:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:32:2021107:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [19.6382s] [ 46%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_fsdp_unspecialized_forced_getattr_no_inline [2025-09-19 12:46:36.036] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:36.039] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:36.047] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:36.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:46:48:2022967 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:48:2022967 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:48:2022965 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:48:2022965 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:50:2022964 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:50:2022964 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:50:2022966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:46:50:2022966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:46:51:2022964:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:51:2022965:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:51:2022967:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:46:51:2022966:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [18.8366s] [ 48%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_get_pg_attr [2025-09-19 12:46:54.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:54.881] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:54.885] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:46:54.897] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [20.9411s] [ 50%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_guard_collective [2025-09-19 12:47:15.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:15.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:15.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:15.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:47:28:2027233 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:28:2027233 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:29:2027235 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:29:2027235 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:31:2027234 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:31:2027234 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:31:2027232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:31:2027232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:32:2027232:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:47:32:2027234:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:47:32:2027235:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:47:32:2027233:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [21.1409s] [ 51%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_ddp_aot_eager [2025-09-19 12:47:36.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:36.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:37.029] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:47:37.063] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:47:50:2029652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:50:2029652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:51:2029650 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:51:2029650 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:53:2029651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:53:2029651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:47:53:2029649 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:47:53:2029649 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:48:14:2029649:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:48:14:2029652:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:48:14:2029650:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:48:14:2029651:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [47.7854s] [ 53%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_ddp_aot_eager_static_graph [2025-09-19 12:48:24.733] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:48:24.739] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:48:24.740] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:48:24.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:48:40:2031789 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:48:40:2031789 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:48:41:2031787 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:48:41:2031787 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:48:41:2031786 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:48:41:2031786 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:48:41:2031788 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:48:41:2031788 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:49:14:2031786:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:49:14:2031789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:49:14:2031788:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:49:14:2031787:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [59.8057s] [ 55%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_ddp_inductor [2025-09-19 12:49:24.545] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:49:24.553] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:49:24.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:49:24.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:49:39:2033921 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:49:39:2033921 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:49:40:2033923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:49:40:2033923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:49:40:2033922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:49:40:2033922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:49:40:2033920 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:49:40:2033920 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:50:02:2033920:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:50:02:2033922:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:50:02:2033923:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:50:02:2033921:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [69.3158s] [ 56%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_ddp_inductor_static_graph [2025-09-19 12:50:33.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:50:33.888] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:50:33.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:50:33.896] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:50:48:2036736 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:50:48:2036736 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:50:48:2036735 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:50:48:2036735 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:50:50:2036734 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:50:50:2036734 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:50:50:2036733 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:50:50:2036733 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:51:23:2036735:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:51:23:2036734:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:51:23:2036733:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:51:23:2036736:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [65.3172s] [ 58%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_fsdp [2025-09-19 12:51:39.192] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:51:39.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:51:39.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:51:39.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:51:54:2039547 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:51:54:2039547 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:51:55:2039549 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:51:55:2039549 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:51:55:2039546 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:51:55:2039546 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:51:55:2039548 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:51:55:2039548 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:52:10:2039548:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:52:10:2039549:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:52:10:2039546:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:52:10:2039547:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Running hf_bert test for FSDP without recursive wrapping
Running hf_bert test for FSDP without recursive wrapping
Running hf_bert test for FSDP without recursive wrapping
Running hf_bert test for FSDP without recursive wrapping
PASSED [75.5293s] [ 60%]
../../../../test/distributed/test_dynamo_distributed.py::TestMultiProc::test_hf_bert_fsdp_activation_checkpointing [2025-09-19 12:52:54.701] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:52:54.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:52:54.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:52:54.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:53:09:2044001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:53:09:2044001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:53:09:2044000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:53:09:2044000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:53:11:2043999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:53:11:2043999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:53:11:2043998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:53:11:2043998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:53:27:2044000:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:53:27:2043998:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:53:27:2044001:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:53:27:2043999:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Running hf_bert_activation_checkpointing test for FSDP with recursive wrapping BertLayer instances
Running hf_bert_activation_checkpointing test for FSDP with recursive wrapping BertLayer instances
Running hf_bert_activation_checkpointing test for FSDP with recursive wrapping BertLayer instances
Running hf_bert_activation_checkpointing test for FSDP with recursive wrapping BertLayer instances
PASSED [63.0048s] [ 61%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_aot_autograd 2025:09:19-12:53:56:1982776 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:53:56:1982776 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [1.2697s] [ 63%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_async_subclass_no_specialize PASSED [0.5684s] [ 65%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_compiled_flex_attention_full_model_ddp SKIPPED [0.0007s] [ 66%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_compiled_flex_attention_local_ddp SKIPPED [0.0004s] [ 68%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_custom_layer PASSED [0.1311s] [ 70%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_ddp_baseline_aot_eager PASSED [0.4035s] [ 71%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_ddp_baseline_inductor PASSED [0.6803s] [ 73%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_empty_graph_inductor PASSED [0.0078s] [ 75%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_dup_tensors_diff_source PASSED [0.2425s] [ 76%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_dup_tensors_same_source PASSED [0.1953s] [ 78%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_orig_params_assert PASSED [0.2328s] [ 80%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_skip_guards PASSED [1.4144s] [ 81%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_skip_register_attr_or_module PASSED [0.3857s] [ 83%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_fsdp_staticmethod PASSED [0.8647s] [ 85%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split PASSED [0.4456s] [ 86%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split_ctx_manager PASSED [2.2296s] [ 88%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split_inductor PASSED [2.6214s] [ 90%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split_inductor_layout_optimizations_inference PASSED [1.3768s] [ 91%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split_inductor_layout_optimizations_training PASSED [1.5750s] [ 93%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_graph_split_inductor_transpose PASSED [4.5766s] [ 95%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_higher_order_op PASSED [0.2429s] [ 96%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_ignored_parameters PASSED [0.1173s] [ 98%]
../../../../test/distributed/test_dynamo_distributed.py::TestSingleProc::test_no_split PASSED [0.1210s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_dynamo_distributed.py.xml -
================== 55 passed, 5 skipped in 925.26s (0:15:25) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:54:21.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 13 items
Running 13 items in this shard

../../../../test/distributed/test_fake_pg.py::TestFakePG::test_all_reduce PASSED [0.1988s] [  7%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_allgather PASSED [0.0010s] [ 15%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_alltoall PASSED [0.0008s] [ 23%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_alltoall_base PASSED [0.0007s] [ 30%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_broadcast PASSED [0.0007s] [ 38%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_construct_fsdp PASSED [0.0271s] [ 46%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_fake_pg_tracing SKIPPED [0.0005s] [ 53%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_fsdp_fake_e2e SKIPPED [0.0005s] [ 61%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_fsdp_tp_fake_e2e SKIPPED [0.0003s] [ 69%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_recv PASSED [0.0006s] [ 76%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_reduce_scatter PASSED [0.0006s] [ 84%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_scatter PASSED [0.0007s] [ 92%]
../../../../test/distributed/test_fake_pg.py::TestFakePG::test_send PASSED [0.0006s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_fake_pg.py.xml -
======================== 10 passed, 3 skipped in 2.14s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:54:24.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 11 items
Running 11 items in this shard

../../../../test/distributed/test_functional_api.py::TestMetaCollectives::test_all_reduce PASSED [0.0018s] [  9%]
../../../../test/distributed/test_functional_api.py::TestMakeFx::test_all_reduce_tracing PASSED [0.0174s] [ 18%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_gather_into_tensor_coalesced_xpu [2025-09-19 12:54:35.421] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:35.427] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:35.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:35.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:54:45:2049567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:54:45:2049567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:54:46:2049569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:54:46:2049569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:54:47:2049568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:54:47:2049568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:54:49:2049566 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:54:49:2049566 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.1318s] [ 27%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_to_all_single_1d_input_xpu [2025-09-19 12:54:53.535] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:53.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:53.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:54:53.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:55:03:2051416 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:03:2051416 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:04:2051419 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:04:2051419 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:06:2051418 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:06:2051418 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:07:2051417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:07:2051417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.0314s] [ 36%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_to_all_single_split_sizes_none_xpu [2025-09-19 12:55:11.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:11.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:11.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:11.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:55:21:2053262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:21:2053262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:22:2053260 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:22:2053260 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:24:2053261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:24:2053261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:25:2053263 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:25:2053263 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.1328s] [ 45%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_to_all_single_xpu [2025-09-19 12:55:29.731] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:29.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:29.744] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:29.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:55:39:2055107 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:39:2055107 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:40:2055108 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:40:2055108 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:42:2055106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:42:2055106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:55:43:2055109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:55:43:2055109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [17.8334s] [ 54%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_tracing_with_dce_code_xpu [2025-09-19 12:55:47.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:47.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:47.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:55:47.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [16.6314s] [ 63%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_tracing_with_fakepg_xpu [2025-09-19 12:56:04.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:04.197] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:04.205] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:04.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [16.6305s] [ 72%]
../../../../test/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_tracing_xpu [2025-09-19 12:56:20.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:20.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:20.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:20.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:56:33:2060620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:56:33:2060620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:56:34:2060622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:56:34:2060622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:56:34:2060621 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:56:34:2060621 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:56:39:2060623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:56:39:2060623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [34.5601s] [ 81%]
../../../../test/distributed/test_functional_api.py::TestDistributedBackendCollectivesWithWorldSize4XPU::test_permute_tensor_with_sub_group_xpu [2025-09-19 12:56:55.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:55.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:55.447] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:56:55.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:57:05:2063080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:05:2063080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:06:2063077 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:06:2063077 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:07:2063079 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:07:2063079 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:09:2063078 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:09:2063078 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:10:2063079:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:57:10:2063080:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:57:10:2063078:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-12:57:10:2063077:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [18.3340s] [ 90%]
../../../../test/distributed/test_functional_api.py::TestFunctionalAutogradWithDistributedBackendXPU::test_all_to_all_single_xpu [2025-09-19 12:57:13.700] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:57:13.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:57:13.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:57:13.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:57:23:2064937 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:23:2064937 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:24:2064935 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:24:2064935 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:26:2064938 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:26:2064938 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:27:2064936 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:27:2064936 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.2338s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_functional_api.py.xml -
======================== 11 passed in 187.70s (0:03:07) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 12:57:33.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 62 items
Running 62 items in this shard

../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_all_to_all_recompute_is_always_banned_override_with_ac_False [2025-09-19 12:57:44.245] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:57:44.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:57:56:2067255 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:56:2067255 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:57:56:2067256 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:57:56:2067256 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4294s] [  1%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_all_to_all_recompute_is_always_banned_override_with_ac_True [2025-09-19 12:57:59.747] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:57:59.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:58:10:2068196 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:10:2068196 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:58:10:2068195 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:10:2068195 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.4240s] [  3%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_all_to_all_single_inductor [2025-09-19 12:58:14.097] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:58:14.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:58:30:2069126 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:30:2069126 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:58:30:2069127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:30:2069127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [21.1352s] [  4%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_all_to_all_single_inductor_split_sizes_none [2025-09-19 12:58:35.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:58:35.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:58:50:2070489 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:50:2070489 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:58:51:2070490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:58:51:2070490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [21.0301s] [  6%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allgather_contiguous_input [2025-09-19 12:58:56.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:58:56.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:59:09:2071788 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:09:2071788 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:59:09:2071787 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:09:2071787 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.1310s] [  8%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allgather_into_tensor_inductor [2025-09-19 12:59:14.523] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:59:14.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:59:25:2073049 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:25:2073049 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:59:25:2073050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:25:2073050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.1313s] [  9%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allgather_output_buffer_reuse [2025-09-19 12:59:32.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:59:32.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-12:59:45:2074254 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:45:2074254 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-12:59:45:2074253 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-12:59:45:2074253 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [17.8279s] [ 11%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allgather_scalar_tensor_input [2025-09-19 12:59:50.360] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 12:59:50.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:00:02:2075509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:02:2075509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:00:02:2075508 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:02:2075508 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [17.5295s] [ 12%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allreduce_inductor [2025-09-19 13:00:07.939] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:00:07.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:00:19:2076711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:19:2076711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:00:19:2076712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:19:2076712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [29.7490s] [ 14%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allreduce_inductor_cudagraph_trees [2025-09-19 13:00:37.640] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:00:37.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:00:50:2077932 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:50:2077932 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:00:50:2077931 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:00:50:2077931 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.2508s] [ 16%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_allreduce_input_buffer_reuse [2025-09-19 13:01:08.897] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:01:08.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:01:21:2079271 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:01:21:2079271 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:01:21:2079272 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:01:21:2079272 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [28.4467s] [ 17%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_broadcast_inductor [2025-09-19 13:01:37.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:01:37.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:01:48:2080494 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:01:48:2080494 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:01:48:2080495 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:01:48:2080495 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.0308s] [ 19%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_c10d_functional_tagged_pt2_compliant [2025-09-19 13:01:55.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:01:55.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [13.6223s] [ 20%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_eager_allreduce_inductor_wait [2025-09-19 13:02:09.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:02:09.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:02:20:2082632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:02:20:2082632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:02:20:2082631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:02:20:2082631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
eager_out, (tensor([[36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.]], device='xpu:1'),)
inductor_out, [tensor([[36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.],
        [36., 36., 36., 36.]], device='xpu:1')]
eager_out, (tensor([[24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.]], device='xpu:0'),)
inductor_out, [tensor([[24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.],
        [24., 24., 24., 24.]], device='xpu:0')]
PASSED [30.0490s] [ 22%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_eager_async_allreduce_inductor_wait [2025-09-19 13:02:39.151] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:02:39.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:02:50:2083838 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:02:50:2083838 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:02:50:2083837 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:02:50:2083837 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
XFAIL [33.2546s] [ 24%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_inductor_allreduce_eager_wait [2025-09-19 13:03:12.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:03:12.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:03:23:2084764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:03:23:2084764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:03:23:2084763 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:03:23:2084763 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [29.7489s] [ 25%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_permute_tensor [2025-09-19 13:03:42.103] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:03:42.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:03:55:2085966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:03:55:2085966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:03:55:2085967 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:03:55:2085967 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.8321s] [ 27%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesMultiProc::test_reduce_scatter_tensor_inductor [2025-09-19 13:04:00.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:04:00.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:04:11:2087169 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:04:11:2087169 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:04:11:2087170 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:04:11:2087170 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [32.9533s] [ 29%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_all_gather_bucket_bucket_mode_all SKIPPED [0.0003s] [ 30%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_all_gather_bucket_bucket_mode_all_custom_ops SKIPPED [0.0001s] [ 32%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_all_gather_bucket_path SKIPPED [0.0001s] [ 33%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_backwards 2025:09:19-13:04:32:2066796 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:04:32:2066796 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [0.7896s] [ 35%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_get_world_group_source_GroupMember_WORLD PASSED [0.0388s] [ 37%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_get_world_group_source__get_default_group PASSED [0.0365s] [ 38%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_get_world_group_source_group_WORLD PASSED [0.0339s] [ 40%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_graphbreaks_unsupported_async_op PASSED [0.0570s] [ 41%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_pg_var PASSED [0.0254s] [ 43%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather SKIPPED [0.0005s] [ 45%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather_args_match SKIPPED [0.0004s] [ 46%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather_list SKIPPED [0.0004s] [ 48%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_to_all_single PASSED [0.0390s] [ 50%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_pg_mode_kwargs PASSED [0.0331s] [ 51%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_pg_mode_kwargs_none PASSED [0.0330s] [ 53%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_pg_mode_positional PASSED [0.0327s] [ 54%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_pg_mode_positional_none PASSED [0.0325s] [ 56%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_pg_mode_unspecified PASSED [0.0323s] [ 58%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_reduce_op_reduce_op0 PASSED [0.0317s] [ 59%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_reduce_op_reduce_op1 PASSED [0.0308s] [ 61%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_reduce_op_reduce_op2 PASSED [0.0305s] [ 62%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_reduce_op_reduce_op3 PASSED [0.0304s] [ 64%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_allreduce_reduce_op_reduce_op4 PASSED [0.0332s] [ 66%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_reduce_scatter SKIPPED [0.0005s] [ 67%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_support_collective_op_with_async_op_False SKIPPED [0.0004s] [ 69%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_trace_all_gather_tensor SKIPPED [0.0004s] [ 70%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_trace_all_gather_tensor_pg SKIPPED [0.0004s] [ 72%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_trace_allgather_coalesced SKIPPED [0.0004s] [ 74%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_trace_allreduce PASSED [0.0305s] [ 75%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_trace_reduce_scatter_tensor SKIPPED [0.0005s] [ 77%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_all_gather_coalesced SKIPPED [0.0004s] [ 79%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_doesnt_mutate_shared PASSED [1.4230s] [ 80%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_doesnt_mutate_shared_graph_partition PASSED [0.0476s] [ 82%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_reduce_scatter_coalesced SKIPPED [0.0005s] [ 83%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_single_op PASSED [0.1201s] [ 85%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_inductor_steal_buffer PASSED [0.2841s] [ 87%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_meta PASSED [0.0011s] [ 88%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reduce_scatter_bucket_bucket_mode_all SKIPPED [0.0002s] [ 90%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reduce_scatter_bucket_bucket_mode_all_custom_ops SKIPPED [0.0001s] [ 91%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reorder_peak_memory SKIPPED [0.0004s] [ 93%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reorder_peak_memory_bucketed_bucket_mode_all SKIPPED [0.0001s] [ 95%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reorder_peak_memory_bucketed_bucket_mode_all_custom_ops SKIPPED [0.0001s] [ 96%]
../../../../test/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_reorder_respects_wait_dep SKIPPED [0.0004s] [ 98%]
../../../../test/distributed/test_inductor_collectives.py::TestSyncDecisionCrossRanks::test_sync_decision_cross_ranks [2025-09-19 13:04:37.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:04:37.417] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:04:46:2088482 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:04:46:2088482 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:04:48:2088481 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:04:48:2088481 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.6256s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_inductor_collectives.py.xml -
============ 41 passed, 20 skipped, 1 xfailed in 438.61s (0:07:18) =============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:04:53.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 22 items
Running 22 items in this shard

../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_all_to_all_single_list PASSED [0.2009s] [  4%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_all_to_all_single_none PASSED [0.0034s] [  9%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_all_to_all_single_tensor PASSED [0.0039s] [ 13%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_broadcast_object_list PASSED [0.0041s] [ 18%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_collective_error_on_rank_non_zero PASSED [0.0028s] [ 22%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_collective_error_on_rank_non_zero_all PASSED [0.0032s] [ 27%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_collective_error_on_rank_zero PASSED [0.0025s] [ 31%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithWrapper::test_skip PASSED [0.0021s] [ 36%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_all_reduce PASSED [0.0037s] [ 40%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_all_reduce_coalesced PASSED [0.0039s] [ 45%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_all_reduce_ops PASSED [0.0059s] [ 50%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_all_to_all PASSED [0.0042s] [ 54%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_allgather PASSED [0.0046s] [ 59%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_assert_equal_on_rank PASSED [0.0032s] [ 63%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_broadcast PASSED [0.0054s] [ 68%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_broadcast_object_list PASSED [0.0041s] [ 72%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_bwd_sees_fwd_pg SKIPPED [0.0027s]on) [ 77%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_gather PASSED [0.0031s] [ 81%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_reduce_scatter PASSED [0.0046s] [ 86%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_scatter PASSED [0.0030s] [ 90%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_subpg PASSED [0.0038s] [ 95%]
../../../../test/distributed/test_multi_threaded_pg.py::TestCollectivesWithBaseClass::test_using_pg_from_another_thread PASSED [0.0034s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_multi_threaded_pg.py.xml -
======================== 21 passed, 1 skipped in 2.18s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:04:57.043] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 120 items
Running 120 items in this shard

../../../../test/distributed/test_store.py::FileStoreTest::test_append PASSED [0.1862s] [  0%]
../../../../test/distributed/test_store.py::FileStoreTest::test_clone PASSED [0.0010s] [  1%]
../../../../test/distributed/test_store.py::FileStoreTest::test_compare_set PASSED [0.0018s] [  2%]
../../../../test/distributed/test_store.py::FileStoreTest::test_init_pg_and_rpc_with_same_file [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.1103s] [  3%]
../../../../test/distributed/test_store.py::FileStoreTest::test_multi_get PASSED [0.0009s] [  4%]
../../../../test/distributed/test_store.py::FileStoreTest::test_multi_set PASSED [0.0007s] [  5%]
../../../../test/distributed/test_store.py::FileStoreTest::test_queues SKIPPED [0.0010s] [  5%]
../../../../test/distributed/test_store.py::FileStoreTest::test_queues_bidirectional SKIPPED [0.0008s] [  6%]
../../../../test/distributed/test_store.py::FileStoreTest::test_queues_nonblocking SKIPPED [0.0008s] [  7%]
../../../../test/distributed/test_store.py::FileStoreTest::test_queues_timeout SKIPPED [0.0017s] [  8%]
../../../../test/distributed/test_store.py::FileStoreTest::test_refcount PASSED [0.0008s] [  9%]
../../../../test/distributed/test_store.py::FileStoreTest::test_set_get_check PASSED [0.0013s] [ 10%]
../../../../test/distributed/test_store.py::FileStoreTest::test_simple_wait PASSED [1.0063s] [ 10%]
../../../../test/distributed/test_store.py::HashStoreTest::test_append PASSED [0.0008s] [ 11%]
../../../../test/distributed/test_store.py::HashStoreTest::test_clone PASSED [0.0006s] [ 12%]
../../../../test/distributed/test_store.py::HashStoreTest::test_compare_set PASSED [0.0016s] [ 13%]
../../../../test/distributed/test_store.py::HashStoreTest::test_multi_get PASSED [0.0006s] [ 14%]
../../../../test/distributed/test_store.py::HashStoreTest::test_multi_set PASSED [0.0006s] [ 15%]
../../../../test/distributed/test_store.py::HashStoreTest::test_queues PASSED [0.0006s] [ 15%]
../../../../test/distributed/test_store.py::HashStoreTest::test_queues_bidirectional PASSED [0.0013s] [ 16%]
../../../../test/distributed/test_store.py::HashStoreTest::test_queues_nonblocking PASSED [0.0007s] [ 17%]
../../../../test/distributed/test_store.py::HashStoreTest::test_queues_timeout PASSED [0.0106s] [ 18%]
../../../../test/distributed/test_store.py::HashStoreTest::test_set_get_check PASSED [0.0009s] [ 19%]
../../../../test/distributed/test_store.py::HashStoreTest::test_simple_wait PASSED [0.2507s] [ 20%]
../../../../test/distributed/test_store.py::PrefixStoreTest::test_get_underlying_store PASSED [0.0017s] [ 20%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_append PASSED [0.0007s] [ 21%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_clone PASSED [0.0007s] [ 22%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_compare_set PASSED [0.0018s] [ 23%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_multi_get PASSED [0.0006s] [ 24%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_multi_set PASSED [0.0006s] [ 25%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_queues SKIPPED [0.0006s] [ 25%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_queues_bidirectional SKIPPED [0.0007s] [ 26%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_queues_nonblocking SKIPPED [0.0006s] [ 27%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_queues_timeout SKIPPED [0.0006s] [ 28%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_set_get_check PASSED [0.0011s] [ 29%]
../../../../test/distributed/test_store.py::PrefixFileStoreTest::test_simple_wait PASSED [1.0062s] [ 30%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_address_already_in_use PASSED [0.0014s] [ 30%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_agent_store PASSED [0.0016s] [ 31%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_append PASSED [0.0016s] [ 32%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_clone PASSED [0.0013s] [ 33%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_compare_set PASSED [0.0024s] [ 34%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_init_pg_and_rpc_with_same_socket [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.0213s] [ 35%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_multi_get PASSED [0.0015s] [ 35%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_multi_set PASSED [0.0015s] [ 36%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_multi_worker_with_fixed_world_size PASSED [0.0041s] [ 37%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_multi_worker_with_nonfixed_world_size PASSED [0.0017s] [ 38%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_multitenancy PASSED [0.0014s] [ 39%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_numkeys_delkeys PASSED [2.0044s] [ 40%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_queues SKIPPED [0.0010s] [ 40%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_queues_bidirectional SKIPPED [0.0010s] [ 41%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_queues_nonblocking SKIPPED [0.0011s] [ 42%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_queues_timeout SKIPPED [0.0010s] [ 43%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_repr PASSED [0.0016s] [ 44%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_set_get_check PASSED [0.0020s] [ 45%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_simple_wait PASSED [0.2518s] [ 45%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_store_timeout_on_missing_clients PASSED [3.0055s] [ 46%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_take_over_listen_socket PASSED [0.0011s] [ 47%]
../../../../test/distributed/test_store.py::TCPStoreTest::test_world_size_0_raises PASSED [0.0006s] [ 48%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_address_already_in_use PASSED [0.0013s] [ 49%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_agent_store PASSED [0.0017s] [ 50%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_append PASSED [0.0015s] [ 50%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_clone PASSED [0.0014s] [ 51%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_compare_set PASSED [0.0026s] [ 52%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_init_pg_and_rpc_with_same_socket [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.0214s] [ 53%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_multi_get PASSED [0.0017s] [ 54%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_multi_set PASSED [0.0017s] [ 55%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_multi_worker_with_fixed_world_size PASSED [0.0043s] [ 55%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_multi_worker_with_nonfixed_world_size PASSED [0.0019s] [ 56%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_multitenancy PASSED [0.0016s] [ 57%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_numkeys_delkeys PASSED [2.0045s] [ 58%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_queues PASSED [0.0016s] [ 59%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_queues_bidirectional PASSED [0.0022s] [ 60%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_queues_nonblocking PASSED [0.0013s] [ 60%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_queues_timeout PASSED [0.0115s] [ 61%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_repr PASSED [0.0013s] [ 62%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_set_get_check PASSED [0.0021s] [ 63%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_simple_wait PASSED [0.2521s] [ 64%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_store_timeout_on_missing_clients PASSED [3.0067s] [ 65%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_take_over_listen_socket PASSED [0.0012s] [ 65%]
../../../../test/distributed/test_store.py::LibUvTCPStoreTest::test_world_size_0_raises PASSED [0.0005s] [ 66%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_append PASSED [0.0013s] [ 67%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_clone PASSED [0.0013s] [ 68%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_compare_set PASSED [0.0026s] [ 69%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_multi_get PASSED [0.0012s] [ 70%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_multi_set PASSED [0.0013s] [ 70%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_queues PASSED [0.0015s] [ 71%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_queues_bidirectional PASSED [0.0021s] [ 72%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_queues_nonblocking PASSED [0.0013s] [ 73%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_queues_timeout PASSED [0.0113s] [ 74%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_set_get_check PASSED [0.0020s] [ 75%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_simple_wait PASSED [0.2520s] [ 75%]
../../../../test/distributed/test_store.py::PrefixTCPStoreTest::test_underlying_non_prefix_store PASSED [0.0012s] [ 76%]
../../../../test/distributed/test_store.py::PythonStoreTest::test_set_get PASSED [0.0005s] [ 77%]
../../../../test/distributed/test_store.py::RendezvousTest::test_unknown_handler PASSED [0.0005s] [ 78%]
../../../../test/distributed/test_store.py::RendezvousTest::test_url_with_node_params PASSED [0.0005s] [ 79%]
../../../../test/distributed/test_store.py::RendezvousEnvTest::test_nominal PASSED [0.0014s] [ 80%]
../../../../test/distributed/test_store.py::RendezvousFileTest::test_common_errors PASSED [0.0007s] [ 80%]
../../../../test/distributed/test_store.py::RendezvousFileTest::test_nominal PASSED [0.0010s] [ 81%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_common_errors PASSED [0.0006s] [ 82%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_dns_timeout PASSED [1.0078s] [ 83%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_nominal PASSED [0.0015s] [ 84%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_tcp_store_timeout_doest_break_client PASSED [0.1020s] [ 85%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_tcp_store_timeout_set PASSED [0.1019s] [ 85%]
../../../../test/distributed/test_store.py::RendezvousTCPTest::test_tcp_store_url_with_libuv PASSED [0.0012s] [ 86%]
../../../../test/distributed/test_store.py::TestPythonStore::test_append_roundtrip PASSED [0.0007s] [ 87%]
../../../../test/distributed/test_store.py::TestPythonStore::test_extended_methods_fallbacks PASSED [0.0007s] [ 88%]
../../../../test/distributed/test_store.py::TestPythonStore::test_has_extended_api_passthrough PASSED [0.0007s] [ 89%]
../../../../test/distributed/test_store.py::TestPythonStore::test_has_extended_api_roundtrip PASSED [0.0005s] [ 90%]
../../../../test/distributed/test_store.py::TestPythonStore::test_multi_get_roundtrip PASSED [0.0006s] [ 90%]
../../../../test/distributed/test_store.py::TestPythonStore::test_multi_set_roundtrip PASSED [0.0006s] [ 91%]
../../../../test/distributed/test_store.py::TestPythonStore::test_optional_methods_fail PASSED [0.0006s] [ 92%]
../../../../test/distributed/test_store.py::TestMultiThreadedWait::test_wait_file_store PASSED [0.0118s] [ 93%]
../../../../test/distributed/test_store.py::TestMultiThreadedWait::test_wait_hash_store PASSED [0.0017s] [ 94%]
../../../../test/distributed/test_store.py::TestMultiThreadedWait::test_wait_prefix_file_store PASSED [0.0014s] [ 95%]
../../../../test/distributed/test_store.py::TestMultiThreadedWait::test_wait_tcp_store PASSED [0.0024s] [ 95%]
../../../../test/distributed/test_store.py::TestMultiThreadedWait::test_wait_tcp_store_uv PASSED [0.0020s] [ 96%]
../../../../test/distributed/test_store.py::TimeoutTest::test_interrupt_doesnt_break_wait PASSED [5.0066s] [ 97%]
../../../../test/distributed/test_store.py::InitPgWithNonUvStore::test_with_env_var [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.0114s] [ 98%]
../../../../test/distributed/test_store.py::InitPgWithNonUvStore::test_with_url_param [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.0113s] [ 99%]
../../../../test/distributed/test_store.py::TestClientProtocol::test_client_connect PASSED [0.0011s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_store.py.xml -
======================= 108 passed, 12 skipped in 21.87s =======================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:05:19.897] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 10 items
Running 10 items in this shard

../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsCPU::test_stage_backward_cpu PASSED [0.0176s] [ 10%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsCPU::test_stage_backward_input_cpu PASSED [0.0101s] [ 20%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsCPU::test_stage_backward_weight_cpu PASSED [0.0107s] [ 30%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsCPU::test_stage_backward_weight_grad_validation_cpu PASSED [0.0244s] [ 40%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsCPU::test_stage_backward_weight_multiple_iters_cpu PASSED [0.0445s] [ 50%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsXPU::test_stage_backward_input_xpu PASSED [0.3704s] [ 60%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsXPU::test_stage_backward_weight_grad_validation_xpu PASSED [0.0118s] [ 70%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsXPU::test_stage_backward_weight_multiple_iters_xpu SKIPPED [0.0006s] [ 80%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsXPU::test_stage_backward_weight_xpu SKIPPED [0.0004s] [ 90%]
../../../../test/distributed/pipelining/test_backward.py::StageBackwardTestsXPU::test_stage_backward_xpu SKIPPED [0.0004s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_backward.py.xml -
========================= 7 passed, 3 skipped in 2.70s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:05:23.605] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/pipelining/test_microbatch.py::MicrobatchTestsCPU::test_chunk_spec_cpu PASSED [8.9056s] [ 25%]
../../../../test/distributed/pipelining/test_microbatch.py::MicrobatchTestsCPU::test_split_and_merge_cpu PASSED [0.0070s] [ 50%]
../../../../test/distributed/pipelining/test_microbatch.py::MicrobatchTestsXPU::test_chunk_spec_xpu SKIPPED [0.0007s] [ 75%]
../../../../test/distributed/pipelining/test_microbatch.py::MicrobatchTestsXPU::test_split_and_merge_xpu PASSED [0.0068s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_microbatch.py.xml -
======================== 3 passed, 1 skipped in 11.11s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:05:35.760] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/pipelining/test_pipe.py::PipeTests::test_model_split_ModelClass0 PASSED [9.0798s] [ 33%]
../../../../test/distributed/pipelining/test_pipe.py::PipeTests::test_model_split_ModelClass1 PASSED [0.1361s] [ 66%]
../../../../test/distributed/pipelining/test_pipe.py::PipeTests::test_model_split_ModelClass2 PASSED [0.0655s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_pipe.py.xml -
============================== 3 passed in 11.28s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:05:48.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 33 items
Running 33 items in this shard

../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_get_schedule_class PASSED [0.1772s] [  3%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_schedule_with_single_stage_ScheduleClass0 PASSED [0.0298s] [  6%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_schedule_with_single_stage_ScheduleClass1 PASSED [0.0210s] [  9%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_schedule_with_single_stage_ScheduleClass2 PASSED [0.0203s] [ 12%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_schedule_with_single_stage_ScheduleClass3 PASSED [0.0205s] [ 15%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_schedule_with_single_stage_ScheduleClass4 PASSED [0.0195s] [ 18%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleTest::test_zero_bubble_schedule_errors_with_compile PASSED [0.9766s] [ 21%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_ScheduleClass0 PASSED [8.2870s] [ 24%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_ScheduleClass1 PASSED [8.0726s] [ 27%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_flex_and_zero_bubble_ScheduleClass0 PASSED [8.1441s] [ 30%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_flex_and_zero_bubble_ScheduleClass1 PASSED [6.0077s] [ 33%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_for_v_schedules_ScheduleClass0 PASSED [0.0059s] [ 36%]
../../../../test/distributed/pipelining/test_schedule.py::TestSchedulePlan::test_pipeline_order_for_v_schedules_ScheduleClass1 PASSED [0.0027s] [ 39%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleCsv::test_csv_compare_ScheduleClass0_csv_name_dualpipev_4rank_10mb PASSED [0.0190s] [ 42%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref0 PASSED [0.0008s] [ 45%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref1 PASSED [0.0007s] [ 48%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref2 PASSED [0.0006s] [ 51%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref3 PASSED [0.0006s] [ 54%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref4 PASSED [0.0006s] [ 57%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref5 PASSED [0.0006s] [ 60%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref6 PASSED [0.0006s] [ 63%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_action_parse_action_str_and_ref7 PASSED [0.0006s] [ 66%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_csv_csv_name_zb1p_2rank_2stagep PASSED [0.0141s] [ 69%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_grad_with_split_b_w PASSED [0.7216s] [ 72%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_grad_with_v_schedule PASSED [0.2159s] [ 75%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_merge_bw_test_info0 PASSED [0.0014s] [ 78%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_send_recv_test_info0 PASSED [0.0024s] [ 81%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_send_recv_test_info1 PASSED [0.0074s] [ 84%]
../../../../test/distributed/pipelining/test_schedule.py::TestScheduleLowering::test_unshard_reshard_test_info0 PASSED [0.0009s] [ 87%]
../../../../test/distributed/pipelining/test_schedule.py::TestValidateSchedule::test_invalid_schedule_missing_action PASSED [0.0004s] [ 90%]
../../../../test/distributed/pipelining/test_schedule.py::TestValidateSchedule::test_invalid_schedule_missing_rank PASSED [0.0003s] [ 93%]
../../../../test/distributed/pipelining/test_schedule.py::TestValidateSchedule::test_valid_schedule PASSED [0.0004s] [ 96%]
../../../../test/distributed/pipelining/test_schedule.py::ScheduleUtilTests::test_generate_stage_to_rank_mapping PASSED [0.0012s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_schedule.py.xml -
============================= 33 passed in 34.81s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:06:24.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/pipelining/test_transformer.py::TransformerTestsCPU::test_ir_cpu PASSED [9.1143s] [ 50%]
../../../../test/distributed/pipelining/test_transformer.py::TransformerTestsXPU::test_ir_xpu PASSED [0.4170s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_transformer.py.xml -
============================== 2 passed in 11.61s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:06:36.991] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/pipelining/test_unflatten.py::UnflattenTestsCPU::test_unflatten_cpu PASSED [8.8776s] [ 50%]
../../../../test/distributed/pipelining/test_unflatten.py::UnflattenTestsXPU::test_unflatten_xpu PASSED [0.3784s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_pipelining_test_unflatten.py.xml -
============================== 2 passed in 11.46s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:06:49.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 45 items
Running 45 items in this shard

../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_0 FAILED [5.9107s] [  2%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_1 FAILED [3.5993s] [  4%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_find_all_gather_patterns PASSED [0.0190s] [  6%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_find_reduce_scatter_patterns PASSED [0.0097s] [  8%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False FAILED [0.1217s] [ 11%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True FAILED [0.1075s] [ 13%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_1_return_A_False PASSED [3.1111s] [ 15%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_1_return_A_True PASSED [3.0565s] [ 17%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_2_return_A_False PASSED [0.0014s] [ 20%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_2_return_A_True PASSED [0.0011s] [ 22%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False FAILED [0.1197s] [ 24%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True FAILED [0.1182s] [ 26%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False FAILED [3.0986s] [ 28%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True FAILED [3.2552s] [ 31%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_2_return_A_False PASSED [3.0812s] [ 33%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_2_return_A_True PASSED [3.1213s] [ 35%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False FAILED [0.0106s] [ 37%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True FAILED [0.0082s] [ 40%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_False PASSED [3.1393s] [ 42%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_True PASSED [3.1178s] [ 44%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_2_return_A_False PASSED [0.0016s] [ 46%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_2_return_A_True PASSED [0.0011s] [ 48%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False FAILED [0.0119s] [ 51%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True FAILED [0.0089s] [ 53%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False FAILED [0.0134s] [ 55%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True FAILED [0.0133s] [ 57%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_False PASSED [3.1401s] [ 60%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_True PASSED [3.1427s] [ 62%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0 FAILED [0.1136s] [ 64%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1 FAILED [3.1331s] [ 66%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_2 PASSED [0.0013s] [ 68%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0 FAILED [0.1179s] [ 71%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1 FAILED [3.0895s] [ 73%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2 FAILED [3.0943s] [ 75%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0 FAILED [0.0085s] [ 77%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1 FAILED [0.0092s] [ 80%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_2 PASSED [0.0011s] [ 82%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0 FAILED [0.0083s] [ 84%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1 FAILED [0.0098s] [ 86%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2 FAILED [0.0099s] [ 88%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0 FAILED [0.0118s] [ 91%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1 FAILED [0.0106s] [ 93%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2 FAILED [0.0105s] [ 95%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_get_unexposed_collectives PASSED [0.0244s] [ 97%]
../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTP4GPUTest::test_extra_collectives FAILED [2.9738s] [100%]

=================================== FAILURES ===================================
_____________ MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_0 _____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 495, in test_dtensor_seq_par
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'0_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpb4146qqg/sb/csbuofcmzwnubnipmbbfwf5a2iyb3cuftzt26zlatgclmmihvwct.py\n# Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   input_tensor_2 => relu\n# Graph fragment:\n#   %mm : Tensor "f32[16, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[16, 8][8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n#   return %relu\ntriton_poi_fused_relu_0 = async_compile.triton(\'triton_poi_fused_relu_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 128}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_relu_0\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 1536}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (8, 10), (10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((16, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf3)\n            del primals_2\n            buf4 = buf3; del buf3  # reuse\n            # Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_relu_0.run(buf4, 128, stream=stream0)\n            buf5 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf4, reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf5)\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf6 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf5, \'sum\', 2, \'0\')\n            assert_size_stride(buf6, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf6, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf6)\n            del buf5\n        return (buf6, primals_3, buf0, buf4, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:04.665000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpb4146qqg
_____________ MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_1 _____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 495, in test_dtensor_seq_par
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'1_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpm_8r7w82/ac/caczbg4u2wt2qjfqt2qvlu66klo44cpxqw6qqd7rsnbr2c6c72k5.py\n# Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   input_tensor_1 => cat, split\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 2560}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = ((xindex // 10) % 16)\n    x0 = (xindex % 10)\n    x2 = xindex // 160\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 8, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 10*(x1) + 80*x2), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 16, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (160 + x0 + 10*((-8) + x1) + 80*x2), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bv/cbvspzcwnfjrment6kwdueb4bulxnusse73xap6o5lkenhkbfjzq.py\n# Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   input_tensor_2 => relu\n#   linear => view_2\n# Graph fragment:\n#   %mm : Tensor "f32[32, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0" = PlaceHolder[target=relu]\n#   %view_2 : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 16, 8]), kwargs = {})\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_2,), kwargs = {})\n#   %le : Tensor "b8[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})\n#   return %relu,%le\ntriton_poi_fused__unsafe_view_relu_threshold_backward_1 = async_compile.triton(\'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 256}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'out_ptr0\': \'*i1\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 3584}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_relu_threshold_backward_1(in_out_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp3 = 0.0\n    tmp4 = tmp2 <= tmp3\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n    tl.store(out_ptr0 + (x0), tmp4, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bs/cbs4y5r2kem5rmfzuaeqyjqyrggdhgyzewjlahbtmrqvgeoayy3w.py\n# Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   linear_1 => view_6\n#   outputs_1 => cat_1, reduce_scatter_tensor, split_1\n# Graph fragment:\n#   %mm_1 : Tensor "f32[32, 10][10, 1]xpu:0" = PlaceHolder[target=mm_1]\n#   %view_6 : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm_1, [2, 16, 10]), kwargs = {})\n#   %split_1 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_6, 8, 1), kwargs = {})\n#   %cat_1 : Tensor "f32[4, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_2, %getitem_3],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat_1, sum, 2, 0), kwargs = {})\n#   return %buf7\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 5120}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 80\n    x0 = (xindex % 80)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 160*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (80 + x0 + 160*((-2) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (2, 8, 10), (80, 10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((2, 16, 10), (160, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 320, stream=stream0)\n            buf4 = empty_strided_xpu((32, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1, linear], Original ATen: [aten.split, aten.cat, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf4)\n            del primals_2\n            buf5 = reinterpret_tensor(buf4, (2, 16, 8), (128, 8, 1), 0); del buf4  # reuse\n            buf11 = empty_strided_xpu((2, 16, 8), (128, 8, 1), torch.bool)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_relu_threshold_backward_1.run(buf5, buf11, 256, stream=stream0)\n            buf6 = empty_strided_xpu((32, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2, linear_1], Original ATen: [aten._unsafe_view, aten.relu, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf5, (32, 8), (8, 1), 0), reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf6)\n            buf7 = empty_strided_xpu((4, 8, 10), (80, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2.run(buf6, buf7, 320, stream=stream0)\n            del buf6\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf8 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf7, \'sum\', 2, \'0\')\n            assert_size_stride(buf8, (2, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf8, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf8)\n            del buf7\n        return (buf8, primals_3, reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(buf5, (32, 8), (8, 1), 0), buf11, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((2, 8, 10), (80, 10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:08.270000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpm_8r7w82
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'2_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:08.424000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpwbu5rvl2
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'3_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:08.533000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp69n96mbx
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'6_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:14.828000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp9mkritag
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'7_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:14.947000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpbhr08n87
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'8_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpiuoj7_cr/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk => split\n#   res_1 => cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1, matmul], Original ATen: [aten.split, aten.cat, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n            del buf3\n        return (reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:18.046000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpiuoj7_cr
_ MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'9_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmp_hgb2xki/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk => split\n#   res_1 => cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n        return (buf3, reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:21.303000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp_hgb2xki
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (None, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:27.519000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp9s1a85hf
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (wait_tensor, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:27.529000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpzlqofyh5
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:33.805000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmphdcs5n21
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (wait_tensor, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:33.815000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpflu73ymw
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:33.830000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp1ai02fz0
_ MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (view_2, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:33.843000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpo2tike2n
__ MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0 __
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'16_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf0, \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (32, 16), (16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:40.244000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp343yf3dc
__ MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1 __
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'17_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpzqx15axw/7h/c7hbjgjdiuc76eklnwumvpmy2uq6zbv5vugz7ce34uo6iuuyznpn.py\n# Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[64, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%mm, 8, 1), kwargs = {})\n#   %cat : Tensor "f32[128, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[64, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 1024}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 16384}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 1024\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 8\n    x0 = (xindex % 8)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 64, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 128, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*((-64) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((128, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 1024, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (64, 8), (8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:43.378000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpzqx15axw
__ MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0 __
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'18_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [matmul, tensor], Original ATen: [aten._unsafe_view, _c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(reinterpret_tensor(buf0, (2, 64, 16), (1024, 16, 1), 0), \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (1, 64, 16), (1024, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:43.500000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpxn4xzlpw
__ MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1 __
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'19_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpn__v6j38/ky/cky76qe6ulvhwz75qtbhi5mpj36h7hkyugmjzpgygruigs6pofyj.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   matmul => view_1\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n#   %cat : Tensor "f32[4, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 512\n    x0 = (xindex % 512)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 1024*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (512 + x0 + 1024*((-2) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 32, 16), (512, 16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 32, 16), (512, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:46.590000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpn__v6j38
__ MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2 __
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'20_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpd8le4hwt/po/cpouspfi54hul3qalnekjvjux2rmijzvciqupffes76iygzetxkq.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   matmul => view_1\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n#   %cat : Tensor "f32[4, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex // 512\n    x0 = (xindex % 8)\n    x1 = ((xindex // 8) % 64)\n    x3 = xindex\n    tmp0 = x2\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*x1 + 1024*(x2)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*x1 + 1024*((-2) + x2)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 64, 8), (512, 8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 64, 8), (512, 8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.686000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpd8le4hwt
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%_scaled_mm, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.695000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmphpqpclos
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%_scaled_mm, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.706000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp0e0vrn75
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_1, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.717000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpzukywhl_
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.727000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp7o_gqpu8
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.738000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmppkdtkdum
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_2, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.751000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp0la4hbyj
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.763000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp0o04risd
_ MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 32, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:49.774000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp5nwu094s
________________ MicroPipelineTP4GPUTest.test_extra_collectives ________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 547, in test_extra_collectives
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'21_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpqsiv8knn/n2/cn27kropqbqilnc2cqdal3tcjzwjjxrajr7j74lc5qm2en3rmmhi.py\n# Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   full_hidden => div\n#   pow_1 => pow_1\n#   sqrt => sqrt\n#   sum_1 => sum_1\n#   tensor_2 => reduce_scatter_tensor\n# Graph fragment:\n#   %buf6 : Tensor  = PlaceHolder[target=buf6]\n#   %sum_1 : Tensor "f32[][]xpu:0" = PlaceHolder[target=sum_1]\n#   %pow_1 : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%wait_tensor_1, 2), kwargs = {})\n#   %sum_1 : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%pow_1,), kwargs = {})\n#   %sqrt : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})\n#   %div : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%wait_tensor_1, %sqrt), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[16, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%div, avg, 2, 3), kwargs = {})\n#   return %sum_1,%buf8\ntriton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0 = async_compile.triton(\'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.persistent_reduction(\n    size_hints={\'x\': 1, \'r0_\': 256},\n    reduction_hint=ReductionHint.INNER,\n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr1\': \'*fp32\', \'xnumel\': \'constexpr\', \'r0_numel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {\'xnumel\': 1}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (3,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': None, \'num_load\': 1, \'num_reduction\': 1, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'r0_\': 1792}}\n)\n@triton.jit\ndef triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 224\n    R0_BLOCK: tl.constexpr = 256\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tmp0 * tmp0\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n    tmp4 = tl.where(r0_mask, tmp2, 0)\n    tmp5 = tl.sum(tmp4, 1)[:, None].to(tl.float32)\n    tmp6 = libdevice.sqrt(tmp5)\n    tmp7 = (tmp0 / tmp6)\n    tl.store(out_ptr1 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp7, r0_mask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1, arg2_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (8, 10), (10, 1))\n        assert_size_stride(arg1_1, (7, 10), (10, 1))\n        assert_size_stride(arg2_1, (10, 7), (7, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'1\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((16, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t, hidden], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(arg1_1, (10, 7), (1, 10), 0), out=buf3)\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf4 = torch.ops._c10d_functional.all_gather_into_tensor.default(buf3, 2, \'3\')\n            assert_size_stride(buf4, (32, 7), (7, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf4, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [res_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf4)\n            del buf3\n            buf8 = empty_strided_xpu((32, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0.run(buf4, buf8, 1, 224, stream=stream0)\n            # Topologically Sorted Source Nodes: [sqrt, full_hidden, tensor_2], Original ATen: [aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            buf9 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf8, \'avg\', 2, \'3\')\n            assert_size_stride(buf9, (16, 7), (7, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf9, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf4\n            # Topologically Sorted Source Nodes: [res_2], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf9)\n            del buf8\n            buf12 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t_1, matmul_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf9, reinterpret_tensor(arg2_1, (7, 10), (1, 7), 0), out=buf12)\n            del arg2_1\n            # Topologically Sorted Source Nodes: [tensor_3], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf13 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf12, \'avg\', 2, \'1\')\n            assert_size_stride(buf13, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf13, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf9\n            # Topologically Sorted Source Nodes: [res_3], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf13)\n            del buf12\n        return (buf13, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((7, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg2_1 = rand_strided((10, 7), (7, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1, arg2_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTP4GPUTest.test_extra_collectives

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 13:07:52.774000 2092465 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmpqsiv8knn
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_parallel_test_micro_pipeline_tp.py.xml -
=========================== short test summary info ============================
FAILED [5.9107s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_0 - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'0_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpb4146qqg/sb/csbuofcmzwnubnipmbbfwf5a2iyb3cuftzt26zlatgclmmihvwct.py\n# Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   input_tensor_2 => relu\n# Graph fragment:\n#   %mm : Tensor "f32[16, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[16, 8][8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n#   return %relu\ntriton_poi_fused_relu_0 = async_compile.triton(\'triton_poi_fused_relu_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 128}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_relu_0\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 1536}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (8, 10), (10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((16, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf3)\n            del primals_2\n            buf4 = buf3; del buf3  # reuse\n            # Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_relu_0.run(buf4, 128, stream=stream0)\n            buf5 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf4, reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf5)\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf6 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf5, \'sum\', 2, \'0\')\n            assert_size_stride(buf6, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf6, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf6)\n            del buf5\n        return (buf6, primals_3, buf0, buf4, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.5993s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_1 - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'1_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpm_8r7w82/ac/caczbg4u2wt2qjfqt2qvlu66klo44cpxqw6qqd7rsnbr2c6c72k5.py\n# Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   input_tensor_1 => cat, split\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 2560}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = ((xindex // 10) % 16)\n    x0 = (xindex % 10)\n    x2 = xindex // 160\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 8, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 10*(x1) + 80*x2), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 16, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (160 + x0 + 10*((-8) + x1) + 80*x2), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bv/cbvspzcwnfjrment6kwdueb4bulxnusse73xap6o5lkenhkbfjzq.py\n# Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   input_tensor_2 => relu\n#   linear => view_2\n# Graph fragment:\n#   %mm : Tensor "f32[32, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0" = PlaceHolder[target=relu]\n#   %view_2 : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 16, 8]), kwargs = {})\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_2,), kwargs = {})\n#   %le : Tensor "b8[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})\n#   return %relu,%le\ntriton_poi_fused__unsafe_view_relu_threshold_backward_1 = async_compile.triton(\'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 256}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'out_ptr0\': \'*i1\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 3584}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_relu_threshold_backward_1(in_out_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp3 = 0.0\n    tmp4 = tmp2 <= tmp3\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n    tl.store(out_ptr0 + (x0), tmp4, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bs/cbs4y5r2kem5rmfzuaeqyjqyrggdhgyzewjlahbtmrqvgeoayy3w.py\n# Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   linear_1 => view_6\n#   outputs_1 => cat_1, reduce_scatter_tensor, split_1\n# Graph fragment:\n#   %mm_1 : Tensor "f32[32, 10][10, 1]xpu:0" = PlaceHolder[target=mm_1]\n#   %view_6 : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm_1, [2, 16, 10]), kwargs = {})\n#   %split_1 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_6, 8, 1), kwargs = {})\n#   %cat_1 : Tensor "f32[4, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_2, %getitem_3],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat_1, sum, 2, 0), kwargs = {})\n#   return %buf7\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 5120}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 80\n    x0 = (xindex % 80)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 160*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (80 + x0 + 160*((-2) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (2, 8, 10), (80, 10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((2, 16, 10), (160, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 320, stream=stream0)\n            buf4 = empty_strided_xpu((32, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1, linear], Original ATen: [aten.split, aten.cat, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf4)\n            del primals_2\n            buf5 = reinterpret_tensor(buf4, (2, 16, 8), (128, 8, 1), 0); del buf4  # reuse\n            buf11 = empty_strided_xpu((2, 16, 8), (128, 8, 1), torch.bool)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_relu_threshold_backward_1.run(buf5, buf11, 256, stream=stream0)\n            buf6 = empty_strided_xpu((32, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2, linear_1], Original ATen: [aten._unsafe_view, aten.relu, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf5, (32, 8), (8, 1), 0), reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf6)\n            buf7 = empty_strided_xpu((4, 8, 10), (80, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2.run(buf6, buf7, 320, stream=stream0)\n            del buf6\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf8 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf7, \'sum\', 2, \'0\')\n            assert_size_stride(buf8, (2, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf8, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf8)\n            del buf7\n        return (buf8, primals_3, reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(buf5, (32, 8), (8, 1), 0), buf11, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((2, 8, 10), (80, 10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1217s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'2_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1075s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'3_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1197s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'6_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1182s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'7_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0986s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'8_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpiuoj7_cr/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk => split\n#   res_1 => cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1, matmul], Original ATen: [aten.split, aten.cat, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n            del buf3\n        return (reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.2552s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'9_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmp_hgb2xki/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk => split\n#   res_1 => cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n        return (buf3, reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0106s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (None, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0082s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (wait_tensor, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0119s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0089s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (wait_tensor, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0134s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0133s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True - AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (view_2, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1136s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0 - AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'16_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf0, \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (32, 16), (16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.1331s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1 - AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'17_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpzqx15axw/7h/c7hbjgjdiuc76eklnwumvpmy2uq6zbv5vugz7ce34uo6iuuyznpn.py\n# Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[64, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%mm, 8, 1), kwargs = {})\n#   %cat : Tensor "f32[128, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[64, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 1024}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 16384}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 1024\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 8\n    x0 = (xindex % 8)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 64, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 128, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*((-64) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((128, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 1024, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (64, 8), (8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.1179s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0 - AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'18_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [matmul, tensor], Original ATen: [aten._unsafe_view, _c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(reinterpret_tensor(buf0, (2, 64, 16), (1024, 16, 1), 0), \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (1, 64, 16), (1024, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0895s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1 - AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'19_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpn__v6j38/ky/cky76qe6ulvhwz75qtbhi5mpj36h7hkyugmjzpgygruigs6pofyj.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   matmul => view_1\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n#   %cat : Tensor "f32[4, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x1 = xindex // 512\n    x0 = (xindex % 512)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 1024*(x1)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (512 + x0 + 1024*((-2) + x1)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 32, 16), (512, 16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 32, 16), (512, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0943s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2 - AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'20_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpd8le4hwt/po/cpouspfi54hul3qalnekjvjux2rmijzvciqupffes76iygzetxkq.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk => split\n#   matmul => view_1\n#   self => cat\n#   tensor => reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n#   %cat : Tensor "f32[4, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex // 512\n    x0 = (xindex % 8)\n    x1 = ((xindex // 8) % 64)\n    x3 = xindex\n    tmp0 = x2\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 >= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 < tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*x1 + 1024*(x2)), tmp4 & xmask, other=0.0)\n    tmp6 = tmp0 >= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 < tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*x1 + 1024*((-2) + x2)), tmp6 & xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 64, 8), (512, 8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 64, 8), (512, 8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0085s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%_scaled_mm, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0092s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%_scaled_mm, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0083s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_1, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0098s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0099s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0118s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_2, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0106s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [0.0105s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2 - AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 32, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [2.9738s] ../../../../test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTP4GPUTest::test_extra_collectives - AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'21_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpqsiv8knn/n2/cn27kropqbqilnc2cqdal3tcjzwjjxrajr7j74lc5qm2en3rmmhi.py\n# Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   full_hidden => div\n#   pow_1 => pow_1\n#   sqrt => sqrt\n#   sum_1 => sum_1\n#   tensor_2 => reduce_scatter_tensor\n# Graph fragment:\n#   %buf6 : Tensor  = PlaceHolder[target=buf6]\n#   %sum_1 : Tensor "f32[][]xpu:0" = PlaceHolder[target=sum_1]\n#   %pow_1 : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%wait_tensor_1, 2), kwargs = {})\n#   %sum_1 : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%pow_1,), kwargs = {})\n#   %sqrt : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})\n#   %div : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%wait_tensor_1, %sqrt), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[16, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%div, avg, 2, 3), kwargs = {})\n#   return %sum_1,%buf8\ntriton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0 = async_compile.triton(\'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.persistent_reduction(\n    size_hints={\'x\': 1, \'r0_\': 256},\n    reduction_hint=ReductionHint.INNER,\n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr1\': \'*fp32\', \'xnumel\': \'constexpr\', \'r0_numel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {\'xnumel\': 1}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (3,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': None, \'num_load\': 1, \'num_reduction\': 1, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'r0_\': 1792}}\n)\n@triton.jit\ndef triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 224\n    R0_BLOCK: tl.constexpr = 256\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index < r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tmp0 * tmp0\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n    tmp4 = tl.where(r0_mask, tmp2, 0)\n    tmp5 = tl.sum(tmp4, 1)[:, None].to(tl.float32)\n    tmp6 = libdevice.sqrt(tmp5)\n    tmp7 = (tmp0 / tmp6)\n    tl.store(out_ptr1 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp7, r0_mask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1, arg2_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (8, 10), (10, 1))\n        assert_size_stride(arg1_1, (7, 10), (10, 1))\n        assert_size_stride(arg2_1, (10, 7), (7, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'1\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((16, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t, hidden], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(arg1_1, (10, 7), (1, 10), 0), out=buf3)\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf4 = torch.ops._c10d_functional.all_gather_into_tensor.default(buf3, 2, \'3\')\n            assert_size_stride(buf4, (32, 7), (7, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf4, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [res_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf4)\n            del buf3\n            buf8 = empty_strided_xpu((32, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0.run(buf4, buf8, 1, 224, stream=stream0)\n            # Topologically Sorted Source Nodes: [sqrt, full_hidden, tensor_2], Original ATen: [aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            buf9 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf8, \'avg\', 2, \'3\')\n            assert_size_stride(buf9, (16, 7), (7, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf9, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf4\n            # Topologically Sorted Source Nodes: [res_2], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf9)\n            del buf8\n            buf12 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t_1, matmul_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf9, reinterpret_tensor(arg2_1, (7, 10), (1, 7), 0), out=buf12)\n            del arg2_1\n            # Topologically Sorted Source Nodes: [tensor_3], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf13 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf12, \'avg\', 2, \'1\')\n            assert_size_stride(buf13, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf13, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf9\n            # Topologically Sorted Source Nodes: [res_3], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf13)\n            del buf12\n        return (buf13, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((7, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg2_1 = rand_strided((10, 7), (7, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1, arg2_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTP4GPUTest.test_extra_collectives

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
=================== 28 failed, 17 passed in 65.11s (0:01:05) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:07:56.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 16 items
Running 16 items in this shard

../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_loss_parallel [2025-09-19 13:07:59.036] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:07:59.037] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:07:59.040] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:07:59.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:07:59:2093718 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:07:59:2093718 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:07:59:2093717 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:07:59:2093717 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:07:59:2093716 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:07:59:2093716 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:07:59:2093719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:07:59:2093719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.7330s] [  6%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_mlp_inference [2025-09-19 13:08:15.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:15.768] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:15.768] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:15.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:08:16:2094032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:16:2094032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:16:2094033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:16:2094033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:16:2094034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:16:2094034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:16:2094035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:16:2094035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1299s] [ 12%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_mlp_training_is_seq_parallel_False_recompute_activation_False [2025-09-19 13:08:31.897] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:31.898] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:31.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:31.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:08:32:2094337 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:32:2094337 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:32:2094334 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:32:2094334 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:32:2094335 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:32:2094335 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:32:2094336 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:32:2094336 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3306s] [ 18%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_mlp_training_is_seq_parallel_True_recompute_activation_False [2025-09-19 13:08:48.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:48.240] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:48.245] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:08:48.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:08:48:2094652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:48:2094652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:48:2094654 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:48:2094654 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:48:2094653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:48:2094653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:08:48:2094651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:08:48:2094651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.4555s] [ 25%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_float64_thaw_all [2025-09-19 13:09:19.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:19.691] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:19.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:19.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.1090s] [ 31%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_all [2025-09-19 13:09:22.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:22.831] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:22.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:22.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2091s] [ 37%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_layers_0_attention_wv__layers_0_feed_forward_w1__layers_1_feed_forward_w2__layers_1_ffn_norm__output__tok_embeddings [2025-09-19 13:09:25.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:26.063] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:26.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:26.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.3091s] [ 43%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_layers_1_ffn_norm__norm__output__tok_embeddings [2025-09-19 13:09:29.303] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:29.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:29.345] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:29.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [4.0100s] [ 50%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_norm__output [2025-09-19 13:09:33.315] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:33.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:33.356] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:33.357] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.3091s] [ 56%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_norm__output__tok_embeddings [2025-09-19 13:09:36.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:36.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:36.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:36.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2088s] [ 62%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_output__tok_embeddings [2025-09-19 13:09:39.841] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:39.845] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:39.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:39.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2087s] [ 68%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_False_float32 [2025-09-19 13:09:43.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:43.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:43.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:43.191] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2018s] [ 75%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_False_float64 [2025-09-19 13:09:46.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:46.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:46.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:46.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2087s] [ 81%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_True_float32 [2025-09-19 13:09:49.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:49.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:49.507] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:49.512] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.3091s] [ 87%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_True_float64 [2025-09-19 13:09:52.779] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:52.779] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:52.904] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:52.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.2089s] [ 93%]
../../../../test/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_weight_tying [2025-09-19 13:09:55.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:56.000] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:56.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:09:56.020] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:09:56:2098143 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:09:56:2098143 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:09:56:2098141 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:09:56:2098141 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:09:56:2098144 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:09:56:2098144 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:09:56:2098142 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:09:56:2098142 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1305s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_parallel_test_tp_examples.py.xml -
================== 5 passed, 11 skipped in 135.19s (0:02:15) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:10:12.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/tensor/parallel/test_tp_random_state.py::TensorParallelRandomStateTests::test_model_init [2025-09-19 13:10:15.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:15.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:15.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:15.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:10:15:2098530 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:15:2098530 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:15:2098533 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:15:2098533 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:15:2098531 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:15:2098531 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:15:2098532 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:15:2098532 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:16:2098532:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:16:2098533:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:16:2098530:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:16:2098531:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:17:2098530:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:17:2098532:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:17:2098531:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:10:17:2098533:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.9089s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_parallel_test_tp_random_state.py.xml -
============================== 1 passed in 18.81s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:10:32.847] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 16 items
Running 16 items in this shard

../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_empty_plan [2025-09-19 13:10:34.923] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:34.931] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:34.946] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:34.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:10:35:2098921 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:35:2098921 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:35:2098924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:35:2098924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:35:2098923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:35:2098923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:35:2098922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:35:2098922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6300s] [  6%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_linear_col_wise_parallel [2025-09-19 13:10:50.459] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:50.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:50.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:10:50.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:10:50:2099221 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:50:2099221 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:50:2099222 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:50:2099222 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:50:2099224 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:50:2099224 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:10:50:2099223 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:10:50:2099223 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9298s] [ 12%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_linear_row_wise_parallel [2025-09-19 13:11:06.395] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:06.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:06.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:06.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:11:06:2099541 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:06:2099541 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:06:2099542 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:06:2099542 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:06:2099540 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:06:2099540 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:06:2099539 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:06:2099539 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0303s] [ 18%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_mlp_with_module_api [2025-09-19 13:11:22.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:22.495] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:22.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:22.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:11:22:2099859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:22:2099859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:22:2099858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:22:2099858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:22:2099857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:22:2099857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:22:2099860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:22:2099860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9232s] [ 25%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_mlp_with_module_api_nested [2025-09-19 13:11:38.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:38.432] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:38.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:38.451] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:11:38:2100173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:38:2100173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:38:2100176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:38:2100176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:38:2100174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:38:2100174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:38:2100175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:38:2100175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9301s] [ 31%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_multi_wildcard [2025-09-19 13:11:54.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:54.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:54.311] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:11:54.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:11:54:2100491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:54:2100491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:54:2100490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:54:2100490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:54:2100493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:54:2100493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:11:54:2100492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:11:54:2100492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9297s] [ 37%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_src_data_rank [2025-09-19 13:12:10.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:10.228] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:10.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:10.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:12:10:2100811 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:10:2100811 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:10:2100809 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:10:2100809 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:10:2100808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:10:2100808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:10:2100810 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:10:2100810 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7297s] [ 43%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_with_digit [2025-09-19 13:12:25.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:25.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:25.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:25.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:12:26:2101113 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:26:2101113 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:26:2101111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:26:2101111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:26:2101112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:26:2101112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:26:2101110 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:26:2101110 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9300s] [ 50%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_with_no_match [2025-09-19 13:12:41.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:41.907] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:41.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:41.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:12:42:2101426 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:42:2101426 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:42:2101428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:42:2101428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:42:2101429 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:42:2101429 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:42:2101427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:42:2101427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2304s] [ 56%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_with_question [2025-09-19 13:12:58.097] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:58.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:58.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:12:58.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:12:58:2101744 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:58:2101744 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:58:2101743 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:58:2101743 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:58:2101745 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:58:2101745 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:12:58:2101746 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:12:58:2101746 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0301s] [ 62%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_with_root_module [2025-09-19 13:13:14.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:14.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:14.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:14.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:13:14:2102061 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:14:2102061 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:14:2102063 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:14:2102063 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:14:2102062 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:14:2102062 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:14:2102064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:14:2102064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.2547s] [ 68%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_parallelize_module_with_star [2025-09-19 13:13:45.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:45.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:45.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:13:45.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:13:45:2102381 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:45:2102381 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:45:2102380 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:45:2102380 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:45:2102378 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:45:2102378 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:13:45:2102379 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:13:45:2102379 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0225s] [ 75%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_prepare_module_input [2025-09-19 13:14:01.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:01.492] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:01.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:01.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:14:01:2102697 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:01:2102697 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:01:2102698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:01:2102698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:01:2102700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:01:2102700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:01:2102699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:01:2102699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7306s] [ 81%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_prepare_module_input_output [2025-09-19 13:14:17.140] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:17.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:17.175] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:17.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:14:17:2103000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:17:2103000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:17:2102999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:17:2102999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:17:2102998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:17:2102998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:17:2103001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:17:2103001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0299s] [ 87%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_prepare_module_output [2025-09-19 13:14:33.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:33.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:33.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:33.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:14:33:2103299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:33:2103299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:33:2103298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:33:2103298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:33:2103301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:33:2103301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:33:2103300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:33:2103300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8297s] [ 93%]
../../../../test/distributed/tensor/parallel/test_parallelize_api.py::TensorParallelAPITests::test_under_devicemesh_context [2025-09-19 13:14:49.111] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:49.112] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:49.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:14:49.134] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:14:49:2103601 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:49:2103601 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:49:2103600 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:49:2103600 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:49:2103602 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:49:2103602 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:14:49:2103603 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:14:49:2103603 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1305s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_parallel_test_parallelize_api.py.xml -
======================== 16 passed in 272.31s (0:04:32) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:15:06.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 9 items
Running 9 items in this shard

../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_colwise_parallel_embedding [2025-09-19 13:15:08.174] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:08.223] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:08.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:08.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:15:08:2103991 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:08:2103991 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:08:2103989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:08:2103989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:08:2103992 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:08:2103992 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:08:2103990 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:08:2103990 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7323s] [ 11%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_colwise_parallel_style [2025-09-19 13:15:23.847] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:23.868] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:23.871] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:23.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:15:24:2104310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:24:2104310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:24:2104307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:24:2104307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:24:2104308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:24:2104308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:24:2104309 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:24:2104309 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.1550s] [ 22%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_prepare_module_input [2025-09-19 13:15:54.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:55.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:55.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:15:55.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:15:55:2104626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:55:2104626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:55:2104627 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:55:2104627 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:55:2104628 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:55:2104628 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:15:55:2104625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:15:55:2104625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7221s] [ 33%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_prepare_module_input_multiple_inputs [2025-09-19 13:16:10.711] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:10.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:10.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:10.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:16:10:2104929 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:10:2104929 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:10:2104930 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:10:2104930 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:10:2104927 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:10:2104927 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:11:2104928 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:11:2104928 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7292s] [ 44%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_prepare_module_kwargs_input [2025-09-19 13:16:26.423] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:26.433] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:26.465] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:26.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:16:26:2105232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:26:2105232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:26:2105229 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:26:2105229 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:26:2105231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:26:2105231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:26:2105230 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:26:2105230 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7298s] [ 55%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_prepare_module_output [2025-09-19 13:16:42.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:42.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:42.184] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:42.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:16:42:2105529 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:42:2105530 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:42:2105529 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:42:2105530 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:42:2105532 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:42:2105532 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:42:2105531 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:42:2105531 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [ 66%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_rowwise_parallel_embedding [2025-09-19 13:16:57.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:57.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:57.819] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:16:57.820] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:16:58:2105832 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:58:2105832 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:58:2105833 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:58:2105833 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:58:2105830 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:58:2105830 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:16:58:2105831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:16:58:2105831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.4562s] [ 77%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_rowwise_parallel_style [2025-09-19 13:17:29.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:17:29.270] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:17:29.291] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:17:29.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:17:29:2106152 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:17:29:2106152 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:17:29:2106153 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:17:29:2106153 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:17:29:2106154 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:17:29:2106154 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:17:29:2106155 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:17:29:2106155 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.3555s] [ 88%]
../../../../test/distributed/tensor/parallel/test_tp_style.py::TensorParallelStyleTest::test_sequence_parallel_style [2025-09-19 13:18:00.627] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:00.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:00.652] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:00.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:18:00:2106473 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:00:2106473 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:00:2106472 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:00:2106472 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:00:2106474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:00:2106474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:00:2106471 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:00:2106471 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3307s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_parallel_test_tp_style.py.xml -
======================== 9 passed in 190.86s (0:03:10) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:18:17.903] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_module [2025-09-19 13:18:19.997] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:20.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:20.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:20.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:18:20:2106865 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:20:2106865 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:20:2106862 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:20:2106862 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:20:2106863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:20:2106863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:20:2106864 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:20:2106864 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6325s] [ 12%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_module_casting [2025-09-19 13:18:35.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:35.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:35.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:35.584] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:18:35:2107163 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:35:2107163 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:35:2107162 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:35:2107162 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:35:2107164 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:35:2107164 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:35:2107165 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:35:2107165 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6219s] [ 25%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_module_input_fn_output_fn [2025-09-19 13:18:51.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:51.191] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:51.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:18:51.211] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:18:51:2107465 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:51:2107465 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:51:2107464 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:51:2107464 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:51:2107466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:51:2107466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:18:51:2107463 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:18:51:2107463 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [ 37%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_module_input_fn_output_fn_warning [2025-09-19 13:19:06.955] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:06.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:07.005] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:07.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:19:07:2107782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:07:2107782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:07:2107780 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:07:2107780 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:07:2107779 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:07:2107779 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:07:2107781 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:07:2107781 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6256s] [ 50%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_module_meta [2025-09-19 13:19:22.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:22.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:22.665] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:22.665] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:19:22:2108082 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:22:2108082 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:22:2108083 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:22:2108083 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:22:2108080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:22:2108080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:22:2108081 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:22:2108081 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6263s] [ 62%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_tensor_errors [2025-09-19 13:19:38.235] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:38.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:38.240] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:38.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:19:38:2108381 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:38:2108381 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:38:2108383 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:38:2108383 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:38:2108382 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:38:2108382 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:38:2108380 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:38:2108380 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:39:2108380:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108381:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108382:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108383:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108382:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108380:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108381:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:19:39:2108383:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.1268s] [ 75%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_tensor_rank [2025-09-19 13:19:54.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:54.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:54.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:19:54.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:19:54:2108701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:54:2108701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:54:2108698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:54:2108698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:54:2108700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:54:2108700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:19:54:2108699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:19:54:2108699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6259s] [ 87%]
../../../../test/distributed/tensor/test_api.py::DTensorAPITest::test_distribute_tensor_uneven_sharding [2025-09-19 13:20:09.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:10.001] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:10.001] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:10.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:20:10:2108999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:10:2108999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:10:2109000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:10:2109000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:10:2108998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:10:2108998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:10:2109001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:10:2109001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7259s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_api.py.xml -
======================== 8 passed in 127.83s (0:02:07) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:20:26.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/tensor/test_attention.py::RingAttentionTest::test_is_causal_behavior PASSED [0.0084s] [ 25%]
../../../../test/distributed/tensor/test_attention.py::RingAttentionTest::test_ring_attention_sdpa SKIPPED [0.0002s] [ 50%]
../../../../test/distributed/tensor/test_attention.py::CPFlexAttentionTest::test_cp_flex_attention SKIPPED [0.0002s] [ 75%]
../../../../test/distributed/tensor/test_attention.py::CPFlexAttentionTest::test_cp_flex_attention_document_mask SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_attention.py.xml -
========================= 1 passed, 3 skipped in 2.22s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:20:29.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 10 items
Running 10 items in this shard

../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_basic_propagation [2025-09-19 13:20:31.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:31.420] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:31.421] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:31.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.2198s] [ 10%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_errors PASSED [0.0026s] [ 20%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_linearity PASSED [0.0016s] [ 30%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_merge_sharding PASSED [0.0015s] [ 40%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_multi_sharding_on_mesh_dim PASSED [0.0015s] [ 50%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_einop_pointwise_propagation PASSED [0.0015s] [ 60%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_pointwise_enforce_sharding_multi_sharding_on_mesh_dim PASSED [0.0018s] [ 70%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_pointwise_multi_sharding_on_mesh_dim PASSED [0.0019s] [ 80%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_pointwise_rules_broadcasting PASSED [0.0013s] [ 90%]
../../../../test/distributed/tensor/test_common_rules.py::CommonRulesTest::test_pointwise_rules_suggestion PASSED [0.0013s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_common_rules.py.xml -
============================== 10 passed in 4.92s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:20:35.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 35 items
Running 35 items in this shard

../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_async_output [2025-09-19 13:20:37.915] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:37.932] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:37.933] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:37.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:20:38:2109855 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:38:2109855 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:38:2109852 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:38:2109852 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:38:2109854 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:38:2109854 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:38:2109853 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:38:2109853 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8078s] [  2%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_constructor [2025-09-19 13:20:53.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:53.589] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:53.599] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:20:53.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:20:53:2110155 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:53:2110155 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:53:2110154 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:53:2110154 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:53:2110156 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:53:2110156 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:20:53:2110153 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:20:53:2110153 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5304s] [  5%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_new_empty_strided [2025-09-19 13:21:09.098] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:09.103] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:09.104] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:09.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:21:09:2110456 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:09:2110456 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:09:2110455 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:09:2110455 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:09:2110454 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:09:2110454 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:09:2110453 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:09:2110453 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9299s] [  8%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_properties [2025-09-19 13:21:25.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:25.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:25.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:21:25:2110773 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:25:2110773 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[2025-09-19 13:21:25.283] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:21:25:2110770 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:25:2110770 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:25:2110772 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:25:2110772 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:25:2110771 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:25:2110771 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7221s] [ 11%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_save_load [2025-09-19 13:21:40.775] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:40.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:40.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:40.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:21:41:2111072 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:41:2111072 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:41:2111071 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:41:2111071 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:41:2111070 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:41:2111070 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:41:2111073 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:41:2111073 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7282s] [ 14%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_save_load_import [2025-09-19 13:21:56.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:56.527] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:56.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:21:56.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:21:59:2111373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:59:2111373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:59:2111372 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:59:2111372 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:59:2111371 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:59:2111371 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:21:59:2111370 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:21:59:2111370 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.5334s] [ 17%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_spec_hash [2025-09-19 13:22:15.021] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:15.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:15.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:15.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:22:15:2112207 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:15:2112207 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:15:2112205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:15:2112205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:15:2112206 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:15:2112206 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:15:2112204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:15:2112204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7305s] [ 20%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_spec_read_only_after_set [2025-09-19 13:22:30.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:30.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:30.765] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:30.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:22:30:2112508 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:30:2112508 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:30:2112507 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:30:2112507 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:30:2112505 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:30:2112505 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:31:2112506 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:31:2112506 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 22%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_dtensor_stride [2025-09-19 13:22:46.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:46.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:46.380] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:22:46.391] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:22:46:2112805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:46:2112805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:46:2112808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:46:2112808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:46:2112806 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:46:2112806 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:22:46:2112807 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:22:46:2112807 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7294s] [ 25%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_from_local [2025-09-19 13:23:02.035] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:02.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:02.059] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:02.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:23:02:2113106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:02:2113106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:02:2113107 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:02:2113107 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:02:2113109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:02:2113109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:02:2113108 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:02:2113108 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6270s] [ 28%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_from_local_negative_dim [2025-09-19 13:23:17.651] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:17.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:17.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:17.811] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:23:17:2113423 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:17:2113423 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:17:2113425 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:17:2113425 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:18:2113422 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:18:2113422 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:18:2113424 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:18:2113424 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6304s] [ 31%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_from_local_then_to_local [2025-09-19 13:23:33.289] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:33.301] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:33.306] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:33.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:23:33:2113723 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:33:2113723 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:33:2113725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:33:2113725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:33:2113726 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:33:2113726 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:33:2113724 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:33:2113724 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5292s] [ 34%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_from_local_uneven_sharding [2025-09-19 13:23:48.855] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:48.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:48.885] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:23:48.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:23:49:2114042 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:49:2114042 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:49:2114043 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:49:2114043 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:49:2114044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:49:2114044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:23:49:2114041 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:23:49:2114041 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6302s] [ 37%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_from_local_uneven_sharding_raise_error [2025-09-19 13:24:04.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:04.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:04.539] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:04.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:24:04:2114346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:04:2114346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:04:2114345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:04:2114345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:04:2114343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:04:2114343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:04:2114344 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:04:2114344 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7301s] [ 40%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_full_tensor_grad_hint [2025-09-19 13:24:20.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:20.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:20.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:20.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:24:20:2114644 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:20:2114644 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:20:2114645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:20:2114645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:20:2114646 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:20:2114646 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:20:2114647 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:20:2114647 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.1557s] [ 42%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_full_tensor_sync [2025-09-19 13:24:51.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:51.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:51.340] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:24:51.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:24:51:2114964 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:51:2114964 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:51:2114962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:51:2114962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:51:2114961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:51:2114961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:24:51:2114963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:24:51:2114963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7297s] [ 45%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_meta_dtensor [2025-09-19 13:25:07.060] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:07.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:07.073] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:07.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:25:07:2115263 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:07:2115263 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:07:2115265 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:07:2115265 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:07:2115264 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:07:2115264 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:07:2115262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:07:2115262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6295s] [ 48%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_modules_w_meta_dtensor [2025-09-19 13:25:22.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:22.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:22.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:22.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:25:22:2115564 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:22:2115564 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:22:2115563 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:22:2115563 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:23:2115566 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:23:2115566 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:23:2115565 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:23:2115565 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9389s] [ 51%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_shard_tensor [2025-09-19 13:25:38.621] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:38.625] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:38.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:38.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:25:38:2115881 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:38:2115881 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:38:2115880 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:38:2115880 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:38:2115882 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:38:2115882 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:38:2115883 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:38:2115883 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5295s] [ 54%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_shard_tensor_2d [2025-09-19 13:25:54.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:54.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:54.315] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:25:54.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:25:54:2116180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:54:2116180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:54:2116181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:54:2116181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:54:2116182 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:54:2116182 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:25:54:2116183 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:25:54:2116183 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 57%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_to_local [2025-09-19 13:26:09.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:09.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:09.761] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:09.761] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:26:10:2116492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:10:2116492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:10:2116491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:10:2116491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:10:2116489 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:10:2116489 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:10:2116490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:10:2116490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8298s] [ 60%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorTest::test_to_local_grad_hint [2025-09-19 13:26:25.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:25.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:25.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:25.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:26:25:2116806 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:25:2116806 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:25:2116807 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:25:2116807 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:25:2116809 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:25:2116809 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:25:2116808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:25:2116808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.9550s] [ 62%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_auto_implicit_replication [2025-09-19 13:26:56.488] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:56.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:56.503] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:26:56.527] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:26:56:2117124 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:56:2117124 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:56:2117125 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:56:2117125 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:56:2117123 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:56:2117123 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:26:56:2117122 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:26:56:2117122 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [ 65%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_default_value_sub_mesh [2025-09-19 13:27:12.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:12.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:12.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:12.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:27:12:2117425 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:12:2117425 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:12:2117427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:12:2117427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:12:2117426 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:12:2117426 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:12:2117424 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:12:2117424 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9313s] [ 68%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_device_mesh_nd [2025-09-19 13:27:28.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:28.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:28.110] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:28.110] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8089s] [ 71%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_dtensor_2d_mesh [2025-09-19 13:27:30.877] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:30.883] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:30.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:30.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:27:31:2118016 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:31:2118016 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:31:2118017 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:31:2118017 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:31:2118014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:31:2118014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:31:2118015 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:31:2118015 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5301s] [ 74%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_dtensor_api_device_mesh_context_manager [2025-09-19 13:27:46.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:46.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:46.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:46.395] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8086s] [ 77%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_dtensor_device_mesh_device_conversion [2025-09-19 13:27:49.209] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:49.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:49.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:27:49.249] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:27:49:2118611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:49:2118611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:49:2118612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:49:2118612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:49:2118613 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:49:2118613 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:27:49:2118614 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:27:49:2118614 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5295s] [ 80%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_dtensor_spec_local_shard_offset [2025-09-19 13:28:04.716] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:04.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:04.857] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:04.879] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.8087s] [ 82%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_from_local_sub_mesh [2025-09-19 13:28:07.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:07.577] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:07.577] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:07.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:28:07:2119201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:07:2119201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:07:2119200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:07:2119200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:07:2119203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:07:2119203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:07:2119202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:07:2119202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5296s] [ 85%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_implicit_replication [2025-09-19 13:28:23.068] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:23.071] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:23.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:23.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:28:23:2119503 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:23:2119503 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:23:2119505 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:23:2119505 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:23:2119504 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:23:2119504 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:23:2119506 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:23:2119506 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4295s] [ 88%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_metadata_consistency_check [2025-09-19 13:28:38.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:38.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:38.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:38.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:28:38:2119806 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:38:2119806 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:38:2119805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:38:2119805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:38:2119804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:38:2119804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:38:2119803 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:38:2119803 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8303s] [ 91%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_redistribute_sub_mesh [2025-09-19 13:28:54.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:54.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:54.423] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:28:54.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:28:54:2120108 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:54:2120108 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:54:2120109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:54:2120109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:54:2120106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:54:2120106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:54:2120107 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:28:54:2120107 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:28:55:2120108:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:28:55:2120106:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.3309s] [ 94%]
../../../../test/distributed/tensor/test_dtensor.py::DTensorMeshTest::test_vmap_embedding [2025-09-19 13:29:10.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:10.651] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:10.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:10.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:29:10:2120414 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:10:2120414 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:10:2120415 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:10:2120415 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:10:2120412 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:10:2120412 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:11:2120413 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:11:2120413 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0305s] [ 97%]
../../../../test/distributed/tensor/test_dtensor.py::TestDTensorPlacementTypes::test_split_tensor_1D [2025-09-19 13:29:26.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:26.727] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:26.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:29:26.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:29:26:2120713 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:26:2120713 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:26:2120716 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:26:2120716 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:27:2120714 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:27:2120714 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:29:27:2120715 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:29:27:2120715 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6297s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_dtensor.py.xml -
================== 32 passed, 3 skipped in 546.47s (0:09:06) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:29:43.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 42 items
Running 42 items in this shard

../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_device_mesh_compile PASSED [0.0637s] [  2%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_attribute_access_on_intermediate PASSED [0.2264s] [  4%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_basic PASSED [0.1913s] [  7%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_basic_export PASSED [0.1470s] [  9%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_constructor_w_dynamo_disable PASSED [0.0984s] [ 11%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_constructor_w_graph_break PASSED [0.0812s] [ 14%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_contiguous_dtensor_noncontiguous_local_as_tangent PASSED [2.5065s] [ 16%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_different_gradient_placement PASSED [4.4177s] [ 19%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dont_recompile_on_same_placement_devicemesh PASSED [0.1282s] [ 21%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dynamic PASSED [0.7758s] [ 23%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dynamic_cat SKIPPED [0.0003s]ensor.PythonKeyTracer) [ 26%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dynamic_loss_parallel_log_softmax PASSED [0.8773s] [ 28%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dynamic_slice SKIPPED [0.0003s]ensor.PythonKeyTracer) [ 30%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_dynamo_device_mesh_attrs PASSED [0.0819s] [ 33%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_noncontiguous_output PASSED [0.3307s] [ 35%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_partial_placement_graph_output PASSED [0.4191s] [ 38%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dtensor_partial_placement_redistribute_unbalanced_correct_strides PASSED [0.0058s] [ 40%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor PASSED [0.1010s] [ 42%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor_from_local PASSED [0.1311s] [ 45%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor_from_local_dynamic_shapes PASSED [1.3764s] [ 47%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor_from_local_redistribute PASSED [0.1222s] [ 50%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor_from_local_redistribute_async PASSED [0.0748s] [ 52%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_dtensor_recompile PASSED [0.1221s] [ 54%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_to_local_kwargs PASSED [0.0740s] [ 57%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_dynamo_to_local_kwargs_forward_hook PASSED [0.1741s] [ 59%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_fakify_dtensor PASSED [0.0697s] [ 61%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_graph_input_is_async PASSED [0.0908s] [ 64%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_placement_compile PASSED [0.1555s] [ 66%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_tp_compile_comm_reordering PASSED [1.7501s] [ 69%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_tp_compile_comm_reordering_graph_partition PASSED [0.9396s] [ 71%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompile::test_unwrap_async_collective_tensor_tangent PASSED [0.8146s] [ 73%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_2d_fsdp_tp_ac_compile_use_ca_False [2025-09-19 13:30:10.976] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:10.981] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:10.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:11.034] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:30:20:2121858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:30:20:2121858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:30:21:2121859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:30:21:2121859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:30:23:2121860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:30:23:2121860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:30:24:2121857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:30:24:2121857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:30:25:2121857:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:25:2121859:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:25:2121858:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:25:2121860:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:54:2121860:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:54:2121857:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:54:2121858:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:30:54:2121859:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [46.6811s] [ 76%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_2d_fsdp_tp_ac_compile_use_ca_True [2025-09-19 13:30:57.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:57.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:57.677] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:30:57.680] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:31:07:2123750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:07:2123750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:31:08:2123748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:08:2123748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:31:09:2123749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:09:2123749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:31:11:2123747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:11:2123747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:31:12:2123747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:12:2123749:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:12:2123748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:12:2123750:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:44:2123747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:44:2123748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:44:2123750:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:31:44:2123749:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [51.2872s] [ 78%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_2d_fsdp_tp_compile_use_ca_False [2025-09-19 13:31:48.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:31:48.941] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:31:48.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:31:49.005] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:31:58:2126422 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:58:2126422 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:31:59:2126423 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:31:59:2126423 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:01:2126420 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:01:2126420 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:02:2126421 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:02:2126421 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:03:2126420:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:03:2126421:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:03:2126423:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:03:2126422:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:31:2126421:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:31:2126420:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:31:2126422:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:31:2126423:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [46.1787s] [ 80%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_2d_fsdp_tp_compile_use_ca_True [2025-09-19 13:32:35.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:32:35.149] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:32:35.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:32:35.168] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:32:44:2128308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:44:2128308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:46:2128309 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:46:2128309 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:47:2128311 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:47:2128311 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:48:2128310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:32:48:2128310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:32:49:2128308:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:49:2128310:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:49:2128309:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:32:49:2128311:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:33:20:2128310:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:33:20:2128308:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:33:20:2128309:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:33:20:2128311:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [49.4831s] [ 83%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_compile_dtensor_redistribute_backward_use_ca_False [2025-09-19 13:33:24.687] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:33:24.700] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:33:24.700] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:33:24.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:33:34:2130746 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:33:34:2130746 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:33:35:2130749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:33:35:2130749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:33:36:2130747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:33:36:2130747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:33:38:2130748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:33:38:2130748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [45.1774s] [ 85%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_compile_dtensor_redistribute_backward_use_ca_True [2025-09-19 13:34:09.791] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:09.801] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:09.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:09.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:34:19:2132616 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:34:19:2132616 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:34:20:2132615 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:34:20:2132615 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:34:22:2132617 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:34:22:2132617 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:34:23:2132618 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:34:23:2132618 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [47.7808s] [ 88%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_compile_embedding_redistribute [2025-09-19 13:34:57.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:57.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:57.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:34:57.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:35:07:2135114 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:07:2135114 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:08:2135113 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:08:2135113 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:09:2135112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:09:2135112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:11:2135111 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:11:2135111 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [48.2834s] [ 90%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_tp_compile_fullgraph_is_seq_parallel_False_use_ca_False [2025-09-19 13:35:45.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:35:45.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:35:45.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:35:45.931] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:35:55:2137736 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:55:2137736 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:56:2137733 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:56:2137733 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:58:2137735 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:58:2137735 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:35:59:2137734 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:35:59:2137734 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.3531s] [ 92%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_tp_compile_fullgraph_is_seq_parallel_False_use_ca_True [2025-09-19 13:36:16.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:16.225] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:16.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:16.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:36:25:2139603 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:36:25:2139603 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:36:27:2139606 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:36:27:2139606 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:36:28:2139605 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:36:28:2139605 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:36:30:2139604 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:36:30:2139604 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [34.3581s] [ 95%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_tp_compile_fullgraph_is_seq_parallel_True_use_ca_False [2025-09-19 13:36:50.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:50.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:50.585] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:36:50.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:37:00:2142239 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:00:2142239 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:01:2142240 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:01:2142240 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:02:2142237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:02:2142237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:04:2142238 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:04:2142238 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [45.3745s] [ 97%]
../../../../test/distributed/tensor/test_dtensor_compile.py::TestDTensorCompileE2E::test_tp_compile_fullgraph_is_seq_parallel_True_use_ca_True [2025-09-19 13:37:35.947] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:37:35.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:37:35.971] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:37:35.976] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:37:45:2144109 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:45:2144109 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:46:2144106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:46:2144106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:47:2144108 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:47:2144108 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:37:49:2144107 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:37:49:2144107 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [48.9867s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_dtensor_compile.py.xml -
================== 40 passed, 2 skipped in 521.49s (0:08:41) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:38:26.624] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/tensor/test_experimental_ops.py::DistOtherOpsTest::test_bernoulli [2025-09-19 13:38:28.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:38:28.708] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:38:29:2146765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:29:2146765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:38:29:2146764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:29:2146764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.2459s] [ 33%]
../../../../test/distributed/tensor/test_experimental_ops.py::DistOtherOpsTest::test_nll [2025-09-19 13:38:43.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:38:43.863] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:38:44:2147113 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:44:2147113 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:38:44:2147112 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:44:2147112 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.1258s] [ 66%]
../../../../test/distributed/tensor/test_experimental_ops.py::DistOtherOpsTest::test_slice [2025-09-19 13:38:58.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:38:58.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:38:59:2147272 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:59:2147272 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:38:59:2147271 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:38:59:2147271 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.9252s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_experimental_ops.py.xml -
============================== 3 passed in 47.29s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:39:14.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/tensor/test_init.py::DTensorInitOpsTest::test_init_ops [2025-09-19 13:39:16.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:16.817] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:16.824] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:16.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:39:17:2147695 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:17:2147695 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:17:2147692 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:17:2147692 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:17:2147694 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:17:2147694 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:17:2147693 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:17:2147693 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8266s] [ 14%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_empty [2025-09-19 13:39:32.541] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:32.545] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:32.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:32.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:39:33:2147993 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:33:2147993 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:33:2147996 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:33:2147996 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:33:2147994 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:33:2147994 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:33:2147995 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:33:2147995 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6275s] [ 28%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_full [2025-09-19 13:39:48.177] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:48.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:48.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:39:48.184] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:39:48:2148297 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:48:2148297 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:48:2148298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:48:2148298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:49:2148296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:49:2148296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:39:49:2148299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:39:49:2148299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5224s] [ 42%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_ones [2025-09-19 13:40:03.687] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:03.687] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:03.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:03.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:40:04:2148598 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:04:2148598 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:04:2148600 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:04:2148600 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:04:2148597 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:04:2148597 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:04:2148599 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:04:2148599 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6270s] [ 57%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_zeros [2025-09-19 13:40:19.308] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:19.309] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:19.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:19.335] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:40:20:2148898 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:20:2148898 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:20:2148901 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:20:2148901 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:20:2148900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:20:2148900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:20:2148899 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:20:2148899 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6264s] [ 71%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_zeros_full_mesh [2025-09-19 13:40:34.947] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:34.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:34.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:34.985] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:40:35:2149200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:35:2149200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:35:2149198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:35:2149198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:35:2149201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:35:2149201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:35:2149199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:35:2149199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9300s] [ 85%]
../../../../test/distributed/tensor/test_init.py::DTensorConstructorTest::test_zeros_submesh [2025-09-19 13:40:50.903] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:50.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:50.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:40:50.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:40:51:2149507 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:51:2149507 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:51:2149509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:51:2149509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:51:2149508 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:51:2149508 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:40:51:2149506 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:40:51:2149506 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8295s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_init.py.xml -
======================== 7 passed in 111.89s (0:01:51) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:41:08.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 20 items
Running 20 items in this shard

../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_conj_complex_dtensor [2025-09-19 13:41:10.213] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:10.229] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:10.234] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:10.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:41:10:2149891 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:10:2149891 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:10:2149889 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:10:2149889 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:10:2149890 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:10:2149890 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:10:2149892 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:10:2149892 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7289s] [  5%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_cumsum [2025-09-19 13:41:25.839] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:25.848] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:25.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:25.853] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:41:26:2150190 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:26:2150190 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:26:2150193 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:26:2150193 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:26:2150192 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:26:2150192 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:26:2150191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:26:2150191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6305s] [ 10%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_foreach_add_different_mesh [2025-09-19 13:41:41.467] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:41.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:41.528] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:41.530] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:41:42:2150490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:42:2150490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:42:2150492 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:42:2150492 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:42:2150493 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:42:2150493 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:42:2150491 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:42:2150491 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9267s] [ 15%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_foreach_norm [2025-09-19 13:41:57.405] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:57.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:57.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:41:57.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:41:57:2150801 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:57:2150801 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:57:2150798 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:57:2150798 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:57:2150799 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:57:2150799 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:41:57:2150800 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:41:57:2150800 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7295s] [ 20%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_foreach_norm_different_mesh [2025-09-19 13:42:13.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:13.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:13.159] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:13.192] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:42:13:2151100 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:13:2151100 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:13:2151102 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:13:2151102 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:13:2151101 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:13:2151101 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:13:2151099 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:13:2151099 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5288s] [ 25%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_histc [2025-09-19 13:42:28.677] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:28.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:28.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:28.741] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:42:28:2151408 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:28:2151408 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:28:2151409 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:28:2151409 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:28:2151410 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:28:2151410 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:29:2151411 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:29:2151411 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1313s] [ 30%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_layer_norm_bwd [2025-09-19 13:42:44.804] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:44.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:44.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:42:44.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:42:45:2152088 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:45:2152088 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:45:2152091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:45:2152091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:45:2152089 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:45:2152089 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:42:45:2152090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:42:45:2152090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1337s] [ 35%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_layer_norm_bwd_req_grad [2025-09-19 13:43:00.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:00.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:00.964] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:01.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:43:01:2152406 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:01:2152406 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:01:2152407 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:01:2152407 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:01:2152405 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:01:2152405 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:01:2152404 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:01:2152404 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.7393s] [ 40%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_layer_norm_fwd [2025-09-19 13:43:32.653] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:32.656] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:32.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:32.702] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:43:32:2152725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:32:2152725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:32:2152723 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:32:2152723 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:32:2152724 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:32:2152724 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:32:2152722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:32:2152722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0301s] [ 45%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_linalg_eigh [2025-09-19 13:43:48.707] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:48.707] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:48.719] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:43:48.719] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:43:48:2153023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:48:2153023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:48:2153025 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:48:2153025 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:48:2153024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:48:2153024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:43:48:2153022 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:43:48:2153022 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6294s] [ 50%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_linear_op_reductions [2025-09-19 13:44:04.333] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:04.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:04.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:04.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:44:04:2153326 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:04:2153326 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:04:2153323 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:04:2153323 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:04:2153324 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:04:2153324 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:04:2153325 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:04:2153325 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3303s] [ 55%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_mean [2025-09-19 13:44:20.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:20.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:20.685] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:20.688] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:44:20:2154005 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:20:2154005 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:20:2154007 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:20:2154007 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:20:2154008 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:20:2154008 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:20:2154006 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:20:2154006 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8302s] [ 60%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_nll_loss_and_cross_entropy [2025-09-19 13:44:36.479] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:36.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:36.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:36.493] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:44:36:2154306 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:36:2154306 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:36:2154307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:36:2154307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:36:2154309 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:36:2154309 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:36:2154308 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:36:2154308 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1305s] [ 65%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_rotary_embedding_complex_ops [2025-09-19 13:44:52.619] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:52.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:52.645] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:44:52.645] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:44:52:2154626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:52:2154626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:52:2154625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:52:2154625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:52:2154624 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:52:2154624 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:44:52:2154623 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:44:52:2154623 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8262s] [ 70%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_shard0_svd [2025-09-19 13:45:08.462] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:08.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:08.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:08.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:45:08:2154943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:08:2154943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:08:2154941 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:08:2154941 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:08:2154940 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:08:2154940 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:08:2154942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:08:2154942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8296s] [ 75%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_shard_math_ops [2025-09-19 13:45:24.273] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:24.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:24.299] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:24.300] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:45:24:2155242 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:24:2155242 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:24:2155244 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:24:2155244 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:24:2155241 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:24:2155241 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:24:2155243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:24:2155243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:25:2155241:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155242:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155244:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155243:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155241:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155244:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155242:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:45:25:2155243:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.3306s] [ 80%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_softmax_fwd [2025-09-19 13:45:40.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:40.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:40.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:40.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:45:40:2155557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:40:2155557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:40:2155559 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:40:2155559 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:40:2155558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:40:2155558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:40:2155560 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:40:2155560 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9303s] [ 85%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_softmax_with_bwd [2025-09-19 13:45:56.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:56.572] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:56.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:45:56.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:45:56:2155857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:56:2155857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:56:2155858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:56:2155858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:56:2155860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:56:2155860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:45:56:2155859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:45:56:2155859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0304s] [ 90%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_topk [2025-09-19 13:46:12.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:12.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:12.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:12.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:46:12:2156174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:12:2156174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:12:2156175 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:12:2156175 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:12:2156176 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:12:2156176 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:12:2156173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:12:2156173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9296s] [ 95%]
../../../../test/distributed/tensor/test_math_ops.py::DistMathOpsTest::test_upsampling [2025-09-19 13:46:28.491] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:28.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:28.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:28.553] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:46:28:2156478 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:28:2156478 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:28:2156477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:28:2156477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:28:2156475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:28:2156475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:28:2156476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:28:2156476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8303s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_math_ops.py.xml -
======================== 20 passed in 336.14s (0:05:36) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:46:45.234] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 13 items
Running 13 items in this shard

../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomInitTest::test_fsdp_tp_model_meta_init [2025-09-19 13:46:47.393] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:47.456] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:47.465] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:46:47.497] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:46:47:2156852 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:47:2156852 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:47:2156851 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:47:2156851 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:47:2156850 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:47:2156850 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:46:47:2156849 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:46:47:2156849 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9309s] [  7%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomInitTest::test_init_ops [2025-09-19 13:47:03.231] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:03.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:03.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:03.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:47:03:2157158 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:03:2157158 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:03:2157160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:03:2157160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:03:2157159 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:03:2157159 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:03:2157157 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:03:2157157 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9297s] [ 15%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomInitTest::test_init_with_user_generator [2025-09-19 13:47:19.083] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:19.120] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:19.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:19.179] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:47:19:2157460 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:19:2157460 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:19:2157459 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:19:2157459 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:19:2157458 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:19:2157458 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:19:2157461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:19:2157461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6273s] [ 23%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomInitTest::test_meta_tensor_init [2025-09-19 13:47:34.807] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:34.807] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:34.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:34.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:47:35:2157761 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:35:2157761 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:35:2157759 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:35:2157759 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:35:2157760 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:35:2157760 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:35:2157758 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:35:2157758 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9297s] [ 30%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomInitTest::test_tp_model_meta_init [2025-09-19 13:47:50.679] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:50.681] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:50.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:47:50.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:47:50:2158058 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:50:2158058 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:50:2158060 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:50:2158060 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:50:2158061 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:50:2158061 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:47:50:2158059 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:47:50:2158059 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8276s] [ 38%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_deterministic_dropout_1d [2025-09-19 13:48:06.492] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:06.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:06.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:06.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:48:06:2158360 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:06:2158360 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:06:2158362 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:06:2158362 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:06:2158359 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:06:2158359 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:06:2158361 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:06:2158361 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7210s] [ 46%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_deterministic_rand_1d [2025-09-19 13:48:22.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:22.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:22.264] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:22.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:48:22:2158661 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:22:2158661 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:22:2158663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:22:2158663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:22:2158660 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:22:2158660 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:22:2158662 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:22:2158662 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9268s] [ 53%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_deterministic_uniform_2d [2025-09-19 13:48:38.170] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:38.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:38.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:38.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:48:38:2158961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:38:2158961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:38:2158963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:38:2158963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:38:2158962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:38:2158962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:38:2158960 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:38:2158960 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:39:2158963:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158962:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158960:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158961:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158960:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158963:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158961:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:48:39:2158962:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.8310s] [ 61%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_manual_seed [2025-09-19 13:48:54.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:54.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:54.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:48:54.993] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:48:55:2159280 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:55:2159280 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:55:2159277 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:55:2159277 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:55:2159278 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:55:2159278 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:48:55:2159279 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:48:55:2159279 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6306s] [ 69%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_manual_seed_submesh [2025-09-19 13:49:10.600] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:10.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:10.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:10.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:49:10:2159579 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:10:2159579 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:10:2159577 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:10:2159577 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:10:2159578 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:10:2159578 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:10:2159580 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:10:2159580 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6300s] [ 76%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_pipeline_parallel_manual_seed [2025-09-19 13:49:26.240] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:26.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:26.260] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:26.260] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:49:26:2159881 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:26:2159881 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:26:2159878 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:26:2159878 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:26:2159879 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:26:2159879 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:26:2159880 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:26:2159880 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8303s] [ 84%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpTest::test_rng_tracker_init [2025-09-19 13:49:42.052] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:42.063] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:42.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:42.107] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:49:42:2160187 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:42:2160187 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:42:2160189 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:42:2160189 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:42:2160188 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:42:2160188 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:49:42:2160186 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:49:42:2160186 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [ 92%]
../../../../test/distributed/tensor/test_random_ops.py::DistTensorRandomOpsTest3D::test_hsdp_tp_model_meta_init [2025-09-19 13:49:57.983] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:57.997] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.000] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.011] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.027] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:49:58.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.1124s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_random_ops.py.xml -
================== 12 passed, 1 skipped in 195.71s (0:03:15) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:50:01.987] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 21 items
Running 21 items in this shard

../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_partial_to_replicate_forward_backward_complex64 [2025-09-19 13:50:04.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:04.096] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:04.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:04.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:50:04:2161130 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:04:2161130 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:04:2161133 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:04:2161133 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:04:2161132 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:04:2161132 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:04:2161131 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:04:2161131 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8507s] [  4%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_partial_to_replicate_forward_backward_float32 [2025-09-19 13:50:19.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:19.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:19.902] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:19.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:50:20:2161448 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:20:2161448 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:20:2161447 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:20:2161447 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:20:2161450 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:20:2161450 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:20:2161449 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:20:2161449 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6288s] [  9%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_partial_to_shard_complex64 [2025-09-19 13:50:35.438] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:35.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:35.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:50:35.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:50:35:2161765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:35:2161765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:35:2161766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:35:2161766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:35:2161767 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:35:2161767 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:50:35:2161768 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:50:35:2161768 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.9551s] [ 14%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_partial_to_shard_float32 [2025-09-19 13:51:06.450] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:06.455] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:06.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:06.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:51:06:2162068 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:06:2162068 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:06:2162067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:06:2162067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:06:2162065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:06:2162065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:06:2162066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:06:2162066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [30.8552s] [ 19%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_redistribute_negative_shard_dim [2025-09-19 13:51:37.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:37.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:37.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:37.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:51:37:2162369 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:37:2162369 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:37:2162367 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:37:2162367 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:37:2162368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:37:2162368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:37:2162370 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:37:2162370 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7295s] [ 23%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_redistribute_shard_dim_change_complex64 [2025-09-19 13:51:53.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:53.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:53.069] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:51:53.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:51:53:2162669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:53:2162669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:53:2162671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:53:2162671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:53:2162668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:53:2162668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:53:2162670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:51:53:2162670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:51:54:2162668:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:54:2162671:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:54:2162670:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:54:2162669:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:55:2162668:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:55:2162669:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:55:2162670:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:51:55:2162671:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.8312s] [ 28%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_redistribute_shard_dim_change_float32 [2025-09-19 13:52:09.831] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:09.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:09.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:09.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:52:10:2162984 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:10:2162984 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:10:2162985 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:10:2162985 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:10:2162986 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:10:2162986 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:10:2162987 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:10:2162987 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:11:2162987:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162985:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162984:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162986:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162986:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162987:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162984:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:11:2162985:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.7313s] [ 33%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_redistribute_uneven_sharding [2025-09-19 13:52:26.548] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:26.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:26.565] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:26.600] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:52:26:2163302 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:26:2163302 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:26:2163305 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:26:2163305 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:26:2163304 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:26:2163304 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:26:2163303 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:26:2163303 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:27:2163303:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:27:2163302:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:27:2163305:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:27:2163304:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:28:2163302:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:28:2163304:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:28:2163305:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:52:28:2163303:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.4308s] [ 38%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_local_partial_grad_complex64 [2025-09-19 13:52:43.004] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:43.004] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:43.005] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:43.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:52:43:2163620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:43:2163620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:43:2163619 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:43:2163619 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:43:2163622 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:43:2163622 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:43:2163621 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:43:2163621 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 42%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_local_partial_grad_float32 [2025-09-19 13:52:58.537] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:58.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:58.550] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:52:58.570] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:52:58:2163935 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:58:2163935 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:58:2163936 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:58:2163936 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:58:2163937 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:58:2163937 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:52:58:2163938 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:52:58:2163938 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5294s] [ 47%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_partial [2025-09-19 13:53:14.067] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:14.073] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:14.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:14.080] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:53:14:2164253 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:14:2164253 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:14:2164254 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:14:2164254 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:14:2164252 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:14:2164252 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:14:2164255 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:14:2164255 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:15:2164253:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164255:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164252:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164254:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164252:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164253:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164254:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:53:15:2164255:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.4311s] [ 52%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_replicate_forward_backward [2025-09-19 13:53:30.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:30.580] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:30.584] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:30.585] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:53:30:2164569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:30:2164569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:30:2164570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:30:2164570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:30:2164571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:30:2164571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:30:2164568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:30:2164568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8296s] [ 57%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_replicate_forward_backward_datatype_conversion [2025-09-19 13:53:46.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:46.327] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:46.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:53:46.348] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:53:46:2164884 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:46:2164884 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:46:2164886 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:46:2164886 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:46:2164885 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:46:2164885 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:53:46:2164887 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:53:46:2164887 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8296s] [ 61%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_replicate_to_shard_forward_backward [2025-09-19 13:54:02.151] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:02.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:02.172] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:02.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:54:02:2165201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:02:2165201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:02:2165204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:02:2165204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:02:2165202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:02:2165202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:02:2165203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:02:2165203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9301s] [ 66%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_shard_dim_alltoall_complex64 [2025-09-19 13:54:18.087] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:18.110] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:18.113] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:18.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:54:18:2165519 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:18:2165519 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:18:2165520 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:18:2165520 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:18:2165517 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:18:2165517 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:18:2165518 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:18:2165518 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:19:2165517:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:19:2165520:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:19:2165518:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:19:2165519:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [15.9296s] [ 71%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_shard_dim_alltoall_float32 [2025-09-19 13:54:34.015] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:34.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:34.030] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:34.058] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:54:34:2165832 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:34:2165832 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:34:2165833 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:34:2165833 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:34:2165831 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:34:2165831 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:34:2165830 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:34:2165830 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:35:2165831:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:35:2165830:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:35:2165833:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-13:54:35:2165832:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.0306s] [ 76%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_shard_to_replicate_forward_backward_complex64 [2025-09-19 13:54:50.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:50.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:50.153] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:54:50.223] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:54:50:2166143 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:50:2166143 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:50:2166146 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:50:2166146 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:50:2166145 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:50:2166145 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:54:50:2166144 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:54:50:2166144 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9298s] [ 80%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_shard_to_replicate_forward_backward_datatype_conversion [2025-09-19 13:55:05.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:06.030] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:06.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:06.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:55:06:2166459 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:06:2166459 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:06:2166460 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:06:2166460 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:06:2166462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:06:2166462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:06:2166461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:06:2166461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9307s] [ 85%]
../../../../test/distributed/tensor/test_redistribute.py::RedistributeTest::test_shard_to_replicate_forward_backward_float32 [2025-09-19 13:55:21.949] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:21.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:21.955] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:21.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:55:22:2166777 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:22:2166777 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:22:2166779 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:22:2166779 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:22:2166776 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:22:2166776 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:22:2166778 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:22:2166778 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9306s] [ 90%]
../../../../test/distributed/tensor/test_redistribute.py::MultiDimRedistributeTest::test_multi_dim_mesh [2025-09-19 13:55:37.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:37.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:37.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:37.868] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.9088s] [ 95%]
../../../../test/distributed/tensor/test_redistribute.py::MultiDimRedistributeTest::test_redistribute_shard_dim_multi_dim_mesh [2025-09-19 13:55:40.747] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:40.769] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:40.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:40.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [2.9085s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_redistribute.py.xml -
================== 19 passed, 2 skipped in 341.69s (0:05:41) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 13:55:44.627] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 30 items
Running 30 items in this shard

../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_aten_contiguous [2025-09-19 13:55:46.724] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:46.724] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:46.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:55:46.748] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:55:46:2167744 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:46:2167744 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:46:2167743 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:46:2167743 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:46:2167745 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:46:2167745 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:55:46:2167742 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:55:46:2167742 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8432s] [  3%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_clone [2025-09-19 13:56:02.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:02.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:02.470] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:02.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:56:02:2168043 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:02:2168043 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:02:2168044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:02:2168044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:02:2168046 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:02:2168046 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:02:2168045 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:02:2168045 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7228s] [  6%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_contiguous [2025-09-19 13:56:18.228] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:18.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:18.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:18.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:56:18:2168346 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:18:2168346 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:18:2168343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:18:2168343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:18:2168344 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:18:2168344 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:18:2168345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:18:2168345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8297s] [ 10%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_copy_ [2025-09-19 13:56:33.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:34.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:34.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:34.111] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:56:34:2168661 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:34:2168661 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:34:2168662 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:34:2168662 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:34:2168663 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:34:2168663 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:34:2168660 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:34:2168660 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [17.8319s] [ 13%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_detach [2025-09-19 13:56:51.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:51.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:51.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:56:51.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:56:52:2169343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:52:2169343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:52:2169341 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:52:2169341 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:52:2169342 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:52:2169342 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:56:52:2169340 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:56:52:2169340 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7295s] [ 16%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_dtensor_dtype_conversion [2025-09-19 13:57:07.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:07.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:07.600] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:07.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:57:07:2169642 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:07:2169642 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:07:2169640 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:07:2169640 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:07:2169641 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:07:2169641 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:07:2169643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:07:2169643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7294s] [ 20%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_empty_like [2025-09-19 13:57:23.335] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:23.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:23.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:23.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:57:23:2169943 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:23:2169943 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:23:2169945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:23:2169945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:23:2169944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:23:2169944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:23:2169942 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:23:2169942 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5295s] [ 23%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_equal [2025-09-19 13:57:38.895] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:38.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:38.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:38.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:57:39:2170245 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:39:2170245 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:39:2170244 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:39:2170244 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:39:2170242 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:39:2170242 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:39:2170243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:39:2170243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9297s] [ 26%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_fill_inplace [2025-09-19 13:57:54.777] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:54.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:54.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:57:54.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:57:55:2170543 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:55:2170543 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:55:2170545 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:55:2170545 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:55:2170544 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:55:2170544 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:57:55:2170542 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:57:55:2170542 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6300s] [ 30%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_fill_inplace_partial_sum [2025-09-19 13:58:10.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:10.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:10.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:10.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:58:10:2170845 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:10:2170845 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:10:2170843 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:10:2170843 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:10:2170844 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:10:2170844 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:10:2170846 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:10:2170846 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7298s] [ 33%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_full_like [2025-09-19 13:58:26.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:26.167] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:26.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:26.185] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:58:26:2171146 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:26:2171146 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:26:2171144 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:26:2171144 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:26:2171147 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:26:2171147 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:26:2171145 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:26:2171145 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7300s] [ 36%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_gather [2025-09-19 13:58:41.868] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:41.869] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:41.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:41.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:58:42:2171444 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:42:2171444 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:42:2171445 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:42:2171445 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:42:2171446 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:42:2171446 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:42:2171447 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:42:2171447 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9301s] [ 40%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_index [2025-09-19 13:58:57.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:57.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:57.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 13:58:57.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-13:58:57:2171745 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:57:2171745 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:58:2171746 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:58:2171746 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:58:2171747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:58:2171747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-13:58:58:2171748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-13:58:58:2171748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [161.3468s] [ 43%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_index_put_scalar [2025-09-19 14:01:39.219] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:39.244] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:39.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:39.287] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:01:39:2172428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:39:2172428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:39:2172430 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:39:2172430 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:39:2172429 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:39:2172429 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:39:2172427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:39:2172427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:40:2172429:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:40:2172430:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:40:2172427:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:40:2172428:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:41:2172427:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:41:2172428:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:41:2172429:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:41:2172430:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.7312s] [ 46%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_index_put_tensor [2025-09-19 14:01:55.868] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:55.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:55.904] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:01:55.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:01:56:2172747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:56:2172747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:56:2172746 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:56:2172746 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:56:2172744 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:56:2172744 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:56:2172745 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:01:56:2172745 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:01:56:2172746:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:56:2172744:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:56:2172747:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:56:2172745:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:58:2172744:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:58:2172747:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:58:2172745:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:01:58:2172746:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.9315s] [ 50%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_inplace_op [2025-09-19 14:02:12.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:12.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:12.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:12.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:02:12:2173064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:12:2173064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:13:2173066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:13:2173066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:13:2173063 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:13:2173063 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:13:2173065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:13:2173065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.0554s] [ 53%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_new_empty_strided [2025-09-19 14:02:43.865] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:43.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:43.877] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:43.896] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:02:44:2173365 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:44:2173365 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:44:2173368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:44:2173368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:44:2173367 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:44:2173367 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:44:2173366 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:44:2173366 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6300s] [ 56%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_new_full [2025-09-19 14:02:59.505] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:59.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:59.511] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:02:59.523] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:02:59:2173670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:59:2173670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:59:2173668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:59:2173668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:59:2173669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:59:2173669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:02:59:2173671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:02:59:2173671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8301s] [ 60%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_ones_like [2025-09-19 14:03:15.344] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:15.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:15.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:15.374] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:03:15:2173969 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:15:2173969 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:15:2173970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:15:2173970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:15:2173972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:15:2173972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:15:2173971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:15:2173971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4289s] [ 63%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_ones_like_partial_sum [2025-09-19 14:03:30.749] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:30.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:30.780] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:30.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:03:30:2174270 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:30:2174270 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:30:2174269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:30:2174269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:31:2174272 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:31:2174272 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:31:2174271 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:31:2174271 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6293s] [ 66%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_op_out_variant [2025-09-19 14:03:46.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:46.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:46.418] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:03:46.426] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:03:46:2174572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:46:2174572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:46:2174571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:46:2174571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:46:2174569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:46:2174569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:03:46:2174570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:03:46:2174570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6294s] [ 70%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_scatter [2025-09-19 14:04:02.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:02.046] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:02.048] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:02.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:04:02:2174871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:02:2174871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:02:2174870 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:02:2174870 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:02:2174872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:02:2174872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:02:2174873 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:02:2174873 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5288s] [ 73%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_slice [2025-09-19 14:04:17.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:17.639] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:17.696] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:17.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:04:17:2175170 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:17:2175170 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:17:2175172 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:17:2175172 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:17:2175171 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:17:2175171 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:17:2175173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:17:2175173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5289s] [ 76%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_split_on_partial [2025-09-19 14:04:33.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:33.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:33.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:33.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:04:33:2175490 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:33:2175490 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:33:2175487 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:33:2175487 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:33:2175488 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:33:2175488 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:33:2175489 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:33:2175489 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9299s] [ 80%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_stack [2025-09-19 14:04:49.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:49.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:49.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:04:49.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:04:49:2175791 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:49:2175791 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:49:2175792 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:49:2175792 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:49:2175790 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:49:2175790 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:04:49:2175789 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:04:49:2175789 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [ 83%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_unbind [2025-09-19 14:05:04.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:04.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:04.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:04.726] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:05:04:2176099 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:04:2176099 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:04:2176097 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:04:2176097 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:04:2176098 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:04:2176098 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:04:2176100 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:04:2176100 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6291s] [ 86%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_where_type_promotion [2025-09-19 14:05:20.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:20.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:20.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:20.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:05:20:2176398 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:20:2176398 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:20:2176400 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:20:2176400 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:20:2176401 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:20:2176401 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:20:2176399 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:20:2176399 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8244s] [ 90%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_zero_inplace [2025-09-19 14:05:36.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:36.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:36.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:36.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:05:36:2176699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:36:2176699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:36:2176698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:36:2176698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:36:2176701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:36:2176701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:36:2176700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:36:2176700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6302s] [ 93%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_zeros_like [2025-09-19 14:05:51.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:51.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:51.748] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:05:51.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:05:51:2176999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:51:2176999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:51:2176998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:51:2176998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:51:2177000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:51:2177000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:05:52:2177001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:05:52:2177001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6297s] [ 96%]
../../../../test/distributed/tensor/test_tensor_ops.py::DistTensorOpsTest::test_zeros_like_partial_sum [2025-09-19 14:06:07.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:07.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:07.378] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:07.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:06:07:2177299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:07:2177299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:07:2177298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:07:2177298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:07:2177300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:07:2177300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:07:2177301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:07:2177301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6296s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_test_tensor_ops.py.xml -
======================== 30 passed in 638.38s (0:10:38) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:06:23.424] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/tensor/experimental/test_register_sharding.py::TestRegisterSharding::test_argmax [2025-09-19 14:06:25.528] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:25.539] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:25.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:25.553] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:06:26:2177674 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:26:2177674 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:26:2177673 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:26:2177673 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:26:2177675 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:26:2177675 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:26:2177676 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:26:2177676 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9303s] [ 50%]
../../../../test/distributed/tensor/experimental/test_register_sharding.py::TestRegisterSharding::test_softmax_fwd [2025-09-19 14:06:41.325] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:41.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:41.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:06:41.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:06:42:2177975 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:42:2177975 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:42:2177976 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:42:2177976 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:42:2177973 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:42:2177973 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:06:42:2177974 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:06:42:2177974 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0303s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_tensor_experimental_test_register_sharding.py.xml -
============================== 2 passed in 33.97s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:06:58.835] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 5 items
Running 5 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_autograd.py::TestFullyShardAutograd::test_nontensor_activations [2025-09-19 14:07:01.036] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:01.039] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:01.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:01.054] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:07:01:2178350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:01:2178350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:01:2178347 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:01:2178347 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:01:2178348 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:01:2178348 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:01:2178349 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:01:2178349 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [32.9675s] [ 20%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_autograd.py::TestFullyShardAutograd::test_unused_forward_module [2025-09-19 14:07:33.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:33.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:33.879] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:07:33.895] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:07:33:2178665 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:33:2178665 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:34:2178667 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:34:2178667 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:34:2178666 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:34:2178666 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:07:34:2178668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:07:34:2178668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:02:2178969:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:02:2178967:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:02:2178974:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:02:2178980:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [32.0572s] [ 40%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_autograd.py::TestFullyShardAutograd::test_unused_forward_output [2025-09-19 14:08:05.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:08:05.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:08:05.898] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:08:05.901] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:08:06:2179012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:06:2179012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:06:2179011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:06:2179011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:06:2179013 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:06:2179013 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:06:2179010 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:06:2179010 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:34:2179315:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:34:2179325:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:34:2179314:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:34:2179320:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:35:2179325:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:35:2179315:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:35:2179314:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:08:35:2179320:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [32.3567s] [ 60%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_autograd.py::TestFullyShardPostAccGradHookMultiThread::test_post_acc_grad_hook_runs SKIPPED [0.0003s] [ 80%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_autograd.py::TestFullyShardPostAccGradHookMultiProcess::test_post_acc_grad_hook_optim_parity [2025-09-19 14:08:38.197] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:08:38.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:08:38:2179353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:38:2179354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:08:38:2179354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:08:38:2179353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [31.1495s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_autograd.py.xml -
=================== 4 passed, 1 skipped in 130.55s (0:02:10) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:09:10.218] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py::TestClipGradNormWorldSize2::test_clip_grad_norm_1d [2025-09-19 14:09:12.308] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:09:12.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:09:12:2179588 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:12:2179588 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:09:12:2179589 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:12:2179589 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [32.1693s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py::TestClipGradNormWorldSize4::test_clip_grad_norm_2d [2025-09-19 14:09:44.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:09:44.396] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:09:44.397] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:09:44.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:09:44:2179749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:44:2179749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:09:44:2179747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:44:2179747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:09:44:2179748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:44:2179748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:09:44:2179750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:09:44:2179750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:09:57:2179748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179750:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179749:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179749:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:09:57:2179750:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179749:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179750:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179747:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179749:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179748:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:14:2179750:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
XFAIL [32.9558s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_clip_grad_norm_.py.xml -
=================== 1 passed, 1 xfailed in 67.04s (0:01:07) ====================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:10:18.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 21 items / 1 deselected / 20 selected
Running 20 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCollectiveOps::test_all_gather_fp32 SKIPPED [0.0003s] [  5%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCollectiveOps::test_reduce_scatter_fp16 SKIPPED [0.0002s] [ 10%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCollectiveOps::test_reduce_scatter_fp32 SKIPPED [0.0001s] [ 15%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCommunication::test_fully_shard_communication_count [2025-09-19 14:10:20.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:20.581] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:20.581] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:20.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:10:20:2180171 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:20:2180171 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:20:2180174 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:20:2180174 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:20:2180173 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:20:2180173 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:20:2180172 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:20:2180172 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:49:2180487:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180478:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180473:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180480:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180480:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180473:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180478:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:49:2180487:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:50:2180478:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:50:2180487:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:50:2180480:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:10:50:2180473:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [32.8419s] [ 20%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCommunication::test_manual_reshard_with_reshard_after_forward_false [2025-09-19 14:10:53.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:53.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:53.219] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:10:53.225] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:10:53:2180527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:53:2180527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:53:2180526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:53:2180526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:53:2180525 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:53:2180525 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:10:53:2180528 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:10:53:2180528 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [31.6518s] [ 25%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardCommunication::test_set_reshard_after_forward [2025-09-19 14:11:24.825] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:24.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:24.839] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:24.859] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:11:25:2180846 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:25:2180846 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:25:2180847 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:25:2180847 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:25:2180845 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:25:2180845 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:25:2180844 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:25:2180844 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [31.9509s] [ 30%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_backward_misprefetch [2025-09-19 14:11:56.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:56.834] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:56.836] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:11:56.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:11:56:2181160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:56:2181160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:57:2181162 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:57:2181162 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:57:2181163 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:57:2181163 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:11:57:2181161 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:11:57:2181161 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.3496s] [ 35%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_fully_shard_backward_prefetch [2025-09-19 14:12:28.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:12:28.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:12:28.145] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:12:28.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:12:28:2181482 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:12:28:2181482 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:12:28:2181481 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:12:28:2181481 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:12:28:2181484 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:12:28:2181484 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:12:28:2181483 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:12:28:2181483 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:12:57:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:57:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:57:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:57:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:58:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:12:59:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:00:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181794:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181789:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181782:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:01:2181793:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:02:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:02:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:02:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:02:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:03:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:04:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:05:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181481:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181482:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181483:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:13:06:2181484:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [41.5621s] [ 40%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_fully_shard_multi_module_backward_prefetch [2025-09-19 14:13:09.682] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:09.710] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:09.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:09.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:13:09:2182091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:09:2182091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:09:2182092 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:09:2182092 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:09:2182090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:09:2182090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:09:2182089 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:09:2182089 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.9488s] [ 45%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_fully_shard_multi_module_unused_module [2025-09-19 14:13:41.619] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:41.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:41.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:13:41.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:13:41:2182410 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:41:2182410 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:41:2182408 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:41:2182408 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:41:2182407 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:41:2182407 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:13:41:2182409 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:13:41:2182409 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [31.3478s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_set_modules_to_backward_prefetch [2025-09-19 14:14:13.001] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:13.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:13.019] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:13.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:14:13:2182725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:13:2182725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:13:2182727 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:13:2182727 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:13:2182726 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:13:2182726 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:13:2182728 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:13:2182728 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [31.7474s] [ 55%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardPrefetch::test_set_modules_to_forward_prefetch [2025-09-19 14:14:44.739] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:44.755] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:44.755] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:14:44.756] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:14:44:2183046 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:44:2183046 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:44:2183047 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:44:2183047 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:44:2183044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:44:2183044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:14:45:2183045 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:14:45:2183045 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.7483s] [ 60%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardUnshardMultiProcess::test_unshard_async [2025-09-19 14:15:16.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:15:16.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:15:16:2183364 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:16:2183364 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:15:16:2183365 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:16:2183365 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [30.4442s] [ 65%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardUnshardMultiThread::test_unshard_no_param_group SKIPPED [0.0002s] [ 70%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardUnshardMultiThread::test_unshard_without_lazy_init SKIPPED [0.0001s] [ 75%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardAllocFromPG::test_exception_when_used_together_with_comm_hooks [2025-09-19 14:15:46.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:15:46.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:15:46.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:15:46.983] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:15:47:2183526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:47:2183526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:15:47:2183525 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:47:2183525 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:15:47:2183523 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:47:2183523 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:15:47:2183524 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:15:47:2183524 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [15.7255s] [ 80%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardAllocFromPG::test_fully_shard_alloc_from_pg SKIPPED [0.0003s] [ 85%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardForceSumReduction::test_fully_shard_force_sum_both_reductions SKIPPED [0.0003s] [ 90%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardForceSumReduction::test_fully_shard_force_sum_reduce_scatter SKIPPED [0.0001s] [ 95%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_comm.py::TestFullyShardReduceOpWorldSize1::test_size1_reduceop [2025-09-19 14:16:02.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:16:02:2183825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:16:02:2183825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.4063s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_comm.py.xml -
=========== 12 passed, 8 skipped, 1 deselected in 347.76s (0:05:47) ============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:16:07.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 18 items
Running 18 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompileCompute::test_disable_compiling_hooks [2025-09-19 14:16:18.300] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:18.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:18.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:18.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:16:27:2184460 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:16:27:2184460 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:16:29:2184459 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:16:29:2184459 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:16:30:2184458 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:16:30:2184458 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:16:31:2184461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:16:31:2184461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [33.9606s] [  5%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_compiled_autograd_ctx [2025-09-19 14:16:52.287] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:52.297] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:52.297] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:16:52.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:17:01:2186969 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:01:2186969 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:03:2186970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:03:2186970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:04:2186972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:04:2186972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:05:2186971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:05:2186971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [49.6700s] [ 11%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_dynamo_recompiles_on_fsdp_layers [2025-09-19 14:17:41.944] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:17:41.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:17:42.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:17:42.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:17:51:2189659 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:51:2189659 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:52:2189658 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:52:2189658 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:54:2189656 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:54:2189656 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:17:55:2189657 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:17:55:2189657 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
PASSED [36.0622s] [ 16%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_dynamo_trace_use_training_state [2025-09-19 14:18:18.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:18.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:18.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:18.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:18:27:2192425 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:27:2192425 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:18:28:2192424 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:28:2192424 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:18:30:2192423 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:30:2192423 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:18:31:2192426 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:31:2192426 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [29.4520s] [ 22%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_nested_fully_shard_backend_aot_eager [2025-09-19 14:18:47.479] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:47.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:47.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:18:47.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:18:56:2194278 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:56:2194278 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:18:58:2194275 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:58:2194275 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:18:59:2194276 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:18:59:2194276 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:19:00:2194277 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:19:00:2194277 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [52.4747s] [ 27%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_nested_fully_shard_backend_aot_eager_decomp_partition [2025-09-19 14:19:39.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:19:39.958] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:19:39.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:19:39.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:19:49:2196147 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:19:49:2196147 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:19:50:2196148 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:19:50:2196148 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:19:52:2196146 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:19:52:2196146 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:19:53:2196149 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:19:53:2196149 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [53.0907s] [ 33%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_nested_fully_shard_backend_inductor_fullgraph_False SKIPPED [0.0003s] [ 38%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_nested_fully_shard_backend_inductor_fullgraph_True [2025-09-19 14:20:33.028] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:20:33.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:20:33.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:20:33.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:20:42:2198020 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:20:42:2198020 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:20:43:2198018 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:20:43:2198018 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:20:45:2198019 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:20:45:2198019 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:20:46:2198021 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:20:46:2198021 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=3, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=1, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=2, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
PASSED [67.5001s] [ 44%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_nested_fully_shard_backend_inductor_fullgraph_True_graph_partition [2025-09-19 14:21:40.579] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:21:40.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:21:40.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:21:40.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:21:50:2200963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:21:50:2200963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:21:51:2200964 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:21:51:2200964 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:21:52:2200962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:21:52:2200962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:21:54:2200961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:21:54:2200961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=2, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=1, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
dist init r=3, world=4
final peak_memory=3676
final peak_memory=3676
final peak_memory=3676
final peak_memory=10352
final peak_memory=10352
final peak_memory=10352
PASSED [68.5180s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_aot_eager [2025-09-19 14:22:49.171] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:22:49.171] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:22:49.191] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:22:49.203] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:22:58:2203910 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:22:58:2203910 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:22:59:2203909 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:22:59:2203909 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:23:01:2203908 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:01:2203908 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:23:02:2203907 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:02:2203907 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [46.9800s] [ 55%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_aot_eager_decomp_partition [2025-09-19 14:23:36.125] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:23:36.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:23:36.165] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:23:36.179] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:23:45:2205780 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:45:2205780 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:23:46:2205781 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:46:2205781 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:23:48:2205778 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:48:2205778 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:23:49:2205779 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:23:49:2205779 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [47.3627s] [ 61%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_simple_mlp_fullgraph_backend_inductor [2025-09-19 14:24:23.447] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:24:23.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:24:23.466] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:24:23.472] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:24:32:2207650 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:24:32:2207650 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:24:34:2207652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:24:34:2207652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:24:35:2207653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:24:35:2207653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:24:37:2207651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:24:37:2207651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
final peak_memory=7084
final peak_memory=7084
final peak_memory=7084
final peak_memory=9792
final peak_memory=9792
final peak_memory=9792
dist init r=3, world=4
final peak_memory=7084
final peak_memory=7084
final peak_memory=7084
final peak_memory=9792
final peak_memory=9792
final peak_memory=9792
dist init r=2, world=4
final peak_memory=7084
final peak_memory=7084
final peak_memory=7084
final peak_memory=9792
final peak_memory=9792
final peak_memory=9792
dist init r=0, world=4
final peak_memory=7084
final peak_memory=7084
final peak_memory=7084
final peak_memory=9792
final peak_memory=9792
final peak_memory=9792
PASSED [58.9030s] [ 66%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_trace_fsdp_copy_ [2025-09-19 14:25:22.329] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:22.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:22.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:22.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:25:32:2210475 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:25:32:2210475 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:25:33:2210474 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:25:33:2210474 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:25:34:2210476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:25:34:2210476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:25:36:2210473 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:25:36:2210473 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [29.8527s] [ 72%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_backend_aot_eager [2025-09-19 14:25:52.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:52.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:52.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:25:52.282] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:26:01:2212326 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:26:01:2212326 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:26:03:2212327 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:26:03:2212327 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:26:04:2212328 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:26:04:2212328 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:26:05:2212329 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:26:05:2212329 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [66.4098s] [ 77%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_backend_aot_eager_decomp_partition [2025-09-19 14:26:58.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:26:58.641] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:26:58.641] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:26:58.641] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:27:08:2214199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:27:08:2214199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:27:09:2214198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:27:09:2214198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:27:10:2214196 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:27:10:2214196 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:27:12:2214197 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:27:12:2214197 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [68.8119s] [ 83%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_backend_inductor_fullgraph_False SKIPPED [0.0003s] [ 88%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_backend_inductor_fullgraph_True SKIPPED [0.0003s] [ 94%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_compile.py::TestFullyShardCompile::test_transformer_backend_inductor_fullgraph_True_graph_partition SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_compile.py.xml -
================== 14 passed, 4 skipped in 720.17s (0:12:00) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:28:08.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 5 items
Running 5 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_extensions.py::TestFullyShardAllGatherExtensionsMultiProcess::test_all_gather_extensions_train_parity [2025-09-19 14:28:10.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:28:10.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:28:10:2216141 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:10:2216141 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:28:11:2216142 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:11:2216142 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [34.5313s] [ 20%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_extensions.py::TestFullyShardAllGatherExtensionsMultiThread::test_all_gather_extension_hsdp_mesh SKIPPED [0.0003s] [ 40%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_extensions.py::TestFullyShardAllGatherExtensionsMultiThread::test_all_gather_extension_outer_size_stride SKIPPED [0.0002s] [ 60%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_extensions.py::TestFullyShardAllGatherExtensionsMultiThread::test_all_gather_extensions_end_to_end SKIPPED [0.0001s] [ 80%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_extensions.py::TestFullyShardAllGatherExtensionsMultiThread::test_all_gather_extensions_monkey_patch SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_extensions.py.xml -
======================== 1 passed, 4 skipped in 36.44s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:28:45.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_frozen.py::TestFullyShardFrozen::test_multi_forward_mixed_requires_grad [2025-09-19 14:28:48.062] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:28:48.064] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:28:48.087] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:28:48.087] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:28:48:2216377 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:48:2216377 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:28:48:2216375 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:48:2216375 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:28:48:2216376 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:48:2216376 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:28:48:2216378 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:28:48:2216378 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:16:2216376:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:16:2216375:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:16:2216378:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:16:2216377:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216375:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216376:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216377:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216378:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216680:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216689:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216682:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:17:2216691:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [32.8392s] [ 33%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_frozen.py::TestFullyShardFrozen::test_train_mixed_requires_grad_across_groups [2025-09-19 14:29:20.723] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:20.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:20.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:20.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:29:20:2216743 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:20:2216743 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:20:2216742 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:20:2216742 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:20:2216741 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:20:2216741 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:21:2216740 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:21:2216740 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:49:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:49:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:49:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:49:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:50:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2216742:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2216743:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2216740:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2216741:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:51:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:52:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2217053:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2217050:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2217043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2217048:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2216741:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2216740:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2216742:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:29:53:2216743:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [35.7628s] [ 66%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_frozen.py::TestFullyShardFrozen::test_train_mixed_requires_grad_per_group [2025-09-19 14:29:56.468] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:56.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:56.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:29:56.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:29:56:2217206 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:56:2217206 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:56:2217204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:56:2217204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:56:2217207 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:56:2217207 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:29:56:2217205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:29:56:2217205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:30:27:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:27:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:27:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:27:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:28:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:29:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:30:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:31:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:32:2217506:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:32:2217511:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:32:2217512:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:32:2217517:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [38.8665s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_frozen.py.xml -
======================== 3 passed in 109.36s (0:01:49) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:30:36.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_grad_scaler.py::TestFullyShardGradientScaler::test_gradient_scaler [2025-09-19 14:30:38.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:30:38.368] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:30:38.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:30:38.409] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:30:38:2217777 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:30:38:2217777 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:30:38:2217774 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:30:38:2217774 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:30:38:2217775 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:30:38:2217775 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:30:38:2217776 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:30:38:2217776 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:30:51:2217776:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217777:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217774:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217774:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217777:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:30:51:2217776:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217774:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217776:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217777:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217775:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217777:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217774:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:31:08:2217776:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [33.5349s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_grad_scaler.py.xml -
============================== 1 passed in 35.43s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:31:12.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_ignore_params.py::TestFullyShardIgnoreParams::test_ddp_A_fsdp_B_ddp_C [2025-09-19 14:31:14.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:31:14.808] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:31:14.808] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:31:14.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:31:14:2218198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:14:2218198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:31:15:2218201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:15:2218201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:31:15:2218199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:15:2218199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:31:15:2218200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:15:2218200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
PASSED [31.2300s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_ignore_params.py.xml -
============================== 1 passed in 33.13s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:31:46.775] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 42 items
Running 42 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardDeviceTensor::test_move_states_to_device_ignored_param_device SKIPPED [0.0003s] [  2%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardDeviceTensor::test_move_states_to_device_tensor SKIPPED [0.0002s] [  4%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardDeviceDTensor::test_move_states_to_device_dtensor_invalid SKIPPED [0.0001s] [  7%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardDeviceDTensor::test_move_states_to_device_dtensor_valid SKIPPED [0.0001s] [  9%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMeshArg::test_2d_mesh_without_mesh_dim_names SKIPPED [0.0001s] [ 11%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMeshArg::test_invalid_mesh_ndim SKIPPED [0.0001s] [ 14%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_modules_duplicate SKIPPED [0.0001s] [ 16%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_modules_list_of_mlps SKIPPED [0.0001s] [ 19%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_modules_nested SKIPPED [0.0002s] [ 21%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_modules_nested_fully_shard_and_replicate SKIPPED [0.0001s] [ 23%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_modules_single SKIPPED [0.0001s] [ 26%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_states_list_of_mlps SKIPPED [0.0001s] [ 28%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_states_nested_fully_shard SKIPPED [0.0001s] [ 30%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardManagedModulesAndStates::test_managed_states_shared_params_and_buffers SKIPPED [0.0001s] [ 33%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardParamModuleInfos::test_get_param_module_infos_duplicates SKIPPED [0.0001s] [ 35%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardParamModuleInfos::test_get_param_module_infos_list_of_mlps SKIPPED [0.0001s] [ 38%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardParamModuleInfos::test_get_param_module_infos_shared_params SKIPPED [0.0001s] [ 40%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardedParameterTensor::test_raise_noncontiguous_parameter SKIPPED [0.0001s] [ 42%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardedParameterTensor::test_raise_scalar_parameter SKIPPED [0.0002s] [ 45%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardedParameterTensor::test_shard_tensor_parameters SKIPPED [0.0001s] [ 47%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardedParameterDTensor::test_shard_dtensor_parameters SKIPPED [0.0002s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardLazyInit::test_fully_shard_double_lazy_init SKIPPED [0.0001s] [ 52%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardLazyInit::test_fully_shard_is_root SKIPPED [0.0001s] [ 54%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardLazyInit::test_fully_shard_module_and_param_fqns SKIPPED [0.0001s] [ 57%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardLazyInit::test_fully_shard_multi_module_root SKIPPED [0.0001s] [ 59%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardLazyInit::test_reset_sharded_param_in_lazy_init SKIPPED [0.0001s] [ 61%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMetaDeviceInit::test_invalid_meta_device_init SKIPPED [0.0001s] [ 64%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMetaDeviceInit::test_meta_device_1d_init SKIPPED [0.0001s] [ 66%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMetaDeviceInit::test_meta_device_2d_init SKIPPED [0.0001s] [ 69%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMetaDeviceInit::test_rank0_broadcast_meta_device_init SKIPPED [0.0001s] [ 71%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardProcessGroupInit::test_1d_process_group_init SKIPPED [0.0001s] [ 73%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardProcessGroupInit::test_2d_process_group_init SKIPPED [0.0001s] [ 76%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardHSDPBroadcast::test_hsdp_broadcast_across_replicas SKIPPED [0.0001s] [ 78%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestHSDPWithCustomHook::test_custom_hook_custom_stream SKIPPED [0.0001s] [ 80%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestHSDPWithCustomHook::test_custom_hsdp_all_reduce_hook SKIPPED [0.0001s] [ 83%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardPlacementFn::test_init_1d_transformer_shard_dim_neg1 SKIPPED [0.0001s] [ 85%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardPlacementFn::test_init_1d_transformer_shard_largest_dim SKIPPED [0.0001s] [ 88%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardPlacementFn::test_init_1d_uneven_shard_largest_dim SKIPPED [0.0001s] [ 90%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardPlacementFn::test_init_2d_transformer_shard_diff_dim SKIPPED [0.0003s] [ 92%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardShardPlacementFn::test_invalid_shard_dim SKIPPED [0.0001s] [ 95%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardOldImport::test_old_import_training SKIPPED [0.0001s] [ 97%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_init.py::TestFullyShardMixedDtypeParam::test_mixed_dtypes_no_grad_param SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_init.py.xml -
============================= 42 skipped in 2.05s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:31:49.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 0 items
Running 0 items in this shard

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_logging.py.xml -
============================ no tests ran in 1.89s =============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:31:52.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_memory.py::TestFullyShardMemory::test_fully_shard_del_memory [2025-09-19 14:31:54.748] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:31:54.762] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:31:54:2218737 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:54:2218737 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:31:55:2218736 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:31:55:2218736 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=2
dist init r=0, world=2
PASSED [15.3107s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_memory.py::TestFullyShardMemory::test_fully_shard_training_memory [2025-09-19 14:32:09.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:32:09.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:32:09:2218886 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:09:2218886 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:32:09:2218887 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:09:2218887 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
FAILED [24.1350s] [100%]

=================================== FAILURES ===================================
____________ TestFullyShardMemory.test_fully_shard_training_memory _____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 35, in test_fully_shard_training_memory
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 210, in _test_fully_shard_training_memory
    self.assertLessEqual(mem_mb - base_mem_mb, expected_mem_mb)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1238, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 177 not less than or equal to 163.904256

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 35, in test_fully_shard_training_memory
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 210, in _test_fully_shard_training_memory
    self.assertLessEqual(mem_mb - base_mem_mb, expected_mem_mb)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1238, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 177 not less than or equal to 163.904256

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:32:07.943000 2218664 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2218886
I0919 14:32:07.944000 2218664 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2218887
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_memory.py.xml -
=========================== short test summary info ============================
FAILED [24.1350s] ../../../../test/distributed/_composable/fsdp/test_fully_shard_memory.py::TestFullyShardMemory::test_fully_shard_training_memory - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 35, in test_fully_shard_training_memory
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 210, in _test_fully_shard_training_memory
    self.assertLessEqual(mem_mb - base_mem_mb, expected_mem_mb)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1238, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 177 not less than or equal to 163.904256

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 35, in test_fully_shard_training_memory
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_memory.py", line 210, in _test_fully_shard_training_memory
    self.assertLessEqual(mem_mb - base_mem_mb, expected_mem_mb)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1238, in assertLessEqual
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 177 not less than or equal to 163.904256

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
========================= 1 failed, 1 passed in 41.45s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:32:34.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 9 items
Running 9 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionTraining::test_compute_dtype [2025-09-19 14:32:37.029] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:32:37.037] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:32:37.052] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:32:37.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:32:37:2219315 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:37:2219315 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:32:37:2219313 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:37:2219313 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:32:37:2219312 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:37:2219312 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:32:37:2219314 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:32:37:2219314 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:06:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:06:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:06:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:06:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:07:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219623:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219628:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:33:09:2219618:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [36.0403s] [ 11%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionTraining::test_grad_acc_with_reduce_dtype [2025-09-19 14:33:12.833] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:12.840] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:12.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:12.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:33:13:2219720 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:13:2219720 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:13:2219722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:13:2219722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:13:2219721 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:13:2219721 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:13:2219719 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:13:2219719 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [32.2580s] [ 22%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionTraining::test_reduce_dtype [2025-09-19 14:33:45.067] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:45.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:45.103] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:33:45.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:33:45:2220039 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:45:2220039 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:45:2220038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:45:2220038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:45:2220037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:45:2220037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:33:45:2220036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:33:45:2220036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:14:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:14:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:14:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:14:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:15:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:16:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:16:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:16:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:16:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220338:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220350:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220341:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:17:2220349:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=0, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
PASSED [35.7612s] [ 33%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_clamp_reduce_dtype SKIPPED [0.0003s] [ 44%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_dataclass_input SKIPPED [0.0001s] [ 55%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_float16_on_one_submodule SKIPPED [0.0001s] [ 66%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_norm_modules_bf16 SKIPPED [0.0001s] [ 77%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_norm_modules_fp16 SKIPPED [0.0001s] [ 88%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py::TestFullyShardMixedPrecisionCasts::test_submodules_with_external_inputs SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_mixed_precision.py.xml -
=================== 3 passed, 6 skipped in 105.97s (0:01:45) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:34:21.795] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items / 1 deselected / 1 selected
Running 1 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_overlap.py::TestFullyShardOverlap::test_fully_shard_post_optim_event_overlap SKIPPED [0.0003s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_overlap.py.xml -
======================= 1 skipped, 1 deselected in 2.01s =======================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:34:24.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_2d_state_dict_correctness [2025-09-19 14:34:26.896] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:26.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:26.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:26.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:34:27:2220592 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:27:2220592 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:27:2220593 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:27:2220593 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:27:2220590 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:27:2220590 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:27:2220591 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:27:2220591 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:40:2220590:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220591:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220592:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220593:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220591:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220590:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220593:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:34:40:2220592:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [16.6400s] [ 14%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_cached_state_dict [2025-09-19 14:34:43.313] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:43.313] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:43.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:43.343] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:34:43:2220909 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:43:2220909 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:43:2220910 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:43:2220910 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:43:2220907 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:43:2220907 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:43:2220908 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:43:2220908 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
FAILED [15.4287s] [ 28%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_dp_state_dict_cpu_offload [2025-09-19 14:34:58.781] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:58.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:58.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:34:58.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:34:58:2221208 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:58:2221208 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:58:2221210 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:58:2221210 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:59:2221211 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:59:2221211 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:34:59:2221209 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:34:59:2221209 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [15.8293s] [ 42%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_dp_state_dict_save_load [2025-09-19 14:35:14.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:14.587] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:14.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:14.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:35:14:2221509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:14:2221509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:14:2221511 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:14:2221511 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:14:2221512 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:14:2221512 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:14:2221510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:14:2221510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:29:2221511:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:29:2221512:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:29:2221509:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:29:2221510:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [17.9353s] [ 57%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_dp_tp_state_dict_save_load [2025-09-19 14:35:32.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:32.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:32.537] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:32.545] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:35:32:2221823 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:32:2221823 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:32:2221824 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:32:2221824 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:32:2221825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:32:2221825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:32:2221826 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:32:2221826 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:45:2221823:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:45:2221824:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:45:2221825:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:35:45:2221826:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [16.4300s] [ 71%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_hsdp_tp_state_dict_save_load [2025-09-19 14:35:48.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:48.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:48.964] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:35:48.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:35:49:2222138 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:49:2222138 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:49:2222136 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:49:2222136 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:49:2222137 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:49:2222137 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:35:49:2222139 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:35:49:2222139 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:02:2222137:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:36:02:2222136:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:36:02:2222139:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:36:02:2222138:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [16.3297s] [ 85%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiThread::test_rank0_offload_full_state_dict SKIPPED [0.0003s] [100%]

=================================== FAILURES ===================================
__________ TestFullyShardStateDictMultiProcess.test_cached_state_dict __________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_state_dict.py", line 123, in test_cached_state_dict
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_state_dict.py", line 145, in _test_cached_state_dict
    model = model.cuda()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1084, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_fully_shard/_fully_shard.py", line 631, in _apply
    ret = super()._apply(*args, **kwargs)  # type: ignore[misc]
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1084, in <lambda>
    return self._apply(lambda t: t.cuda(device))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/cuda/__init__.py", line 403, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_state_dict.py TestFullyShardStateDictMultiProcess.test_cached_state_dict

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:34:41.464000 2220518 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2220907
I0919 14:34:41.464000 2220518 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2220908
I0919 14:34:41.465000 2220518 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2220909
I0919 14:34:41.465000 2220518 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2220910
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_state_dict.py.xml -
=========================== short test summary info ============================
FAILED [15.4287s] ../../../../test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_cached_state_dict - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_state_dict.py", line 123, in test_cached_state_dict
    self.run_subtests(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py", line 1188, in run_subtests
    return run_subtests(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1184, in run_subtests
    test_fn(*test_args, **test_kwargs, **subtest_kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_state_dict.py", line 145, in _test_cached_state_dict
    model = model.cuda()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1084, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/fsdp/_fully_shard/_fully_shard.py", line 631, in _apply
    ret = super()._apply(*args, **kwargs)  # type: ignore[misc]
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1084, in <lambda>
    return self._apply(lambda t: t.cuda(device))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/cuda/__init__.py", line 403, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_state_dict.py TestFullyShardStateDictMultiProcess.test_cached_state_dict

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
============== 1 failed, 5 passed, 1 skipped in 100.59s (0:01:40) ==============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:36:06.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 5 items
Running 5 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_state.py::TestFullyShardState::test_fully_shard_cls SKIPPED [0.0003s] [ 20%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state.py::TestFullyShardState::test_fully_shard_deepcopy SKIPPED [0.0002s] [ 40%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state.py::TestFullyShardState::test_fully_shard_reapply SKIPPED [0.0002s] [ 60%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state.py::TestFullyShardState::test_fully_shard_state SKIPPED [0.0001s] [ 80%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_state.py::TestFullyShardState::test_fully_shard_unsupported_module_cls SKIPPED [0.0001s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_state.py.xml -
============================== 5 skipped in 1.92s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:36:08.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 24 items
Running 24 items in this shard

../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardForwardInputs::test_root_move_forward_input_to_device SKIPPED [0.0003s] [  4%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardRegisteredParams::test_param_registration_after_backward SKIPPED [0.0002s] [  8%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardRegisteredParams::test_param_registration_after_forward SKIPPED [0.0001s] [ 12%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardCastAfterInit::test_to_float64_after_init SKIPPED [0.0001s] [ 16%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_explicit_prefetching [2025-09-19 14:36:11.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:11.174] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:11.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:11.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:36:11:2222603 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:11:2222603 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:11:2222602 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:11:2222602 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:11:2222601 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:11:2222601 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:11:2222604 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:11:2222604 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [32.8528s] [ 20%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_multi_forward_module [2025-09-19 14:36:43.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:43.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:43.898] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:36:43.902] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:36:44:2222923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:44:2222923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:44:2222922 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:44:2222922 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:44:2222921 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:44:2222921 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:36:44:2222920 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:36:44:2222920 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [31.9568s] [ 25%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_non_root_forward_backward SKIPPED [0.0002s] [ 29%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_post_optim_event SKIPPED [0.0003s] [ 33%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_train_parity_multi_group SKIPPED [0.0001s] [ 37%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_train_parity_multi_group_cpu_offload_eager SKIPPED [0.0001s] [ 41%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_train_parity_multi_group_unshard_async_op SKIPPED [0.0001s] [ 45%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_train_parity_single_group_shard_dim0 [2025-09-19 14:37:15.768] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:15.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:15.807] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:15.822] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:37:15:2223237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:15:2223237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:15:2223236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:15:2223236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:16:2223238 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:16:2223238 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:16:2223239 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:16:2223239 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
PASSED [31.6546s] [ 50%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCore::test_train_parity_single_group_shard_largest_dim [2025-09-19 14:37:47.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:47.420] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:47.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:37:47.454] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:37:47:2223553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:47:2223553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:47:2223555 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:47:2223555 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:47:2223554 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:47:2223554 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:37:47:2223556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:37:47:2223556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
PASSED [31.6389s] [ 54%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShard1DTrainingCompose::test_train_parity_with_activation_checkpointing [2025-09-19 14:38:19.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:38:19.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:38:19:2223871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:38:19:2223871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:38:19:2223872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:38:19:2223872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
XFAIL [42.5655s] [ 58%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardShardPlacementFnMultiProcess::test_train_parity_shard_placement_fn_shard_largest_dim [2025-09-19 14:39:01.623] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:01.628] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:01.628] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:01.629] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:39:01:2224034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:01:2224034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:01:2224031 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:01:2224031 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:01:2224032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:01:2224032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:01:2224033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:01:2224033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
PASSED [32.0572s] [ 62%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardShardPlacementFnMultiThread::test_shard_placement_fn_contiguous_params_grads SKIPPED [0.0004s] [ 66%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardSharedParams::test_train_parity_with_shared_params [2025-09-19 14:39:33.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:33.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:33.702] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:39:33.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:39:33:2224353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:33:2224353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:33:2224351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:33:2224351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:33:2224352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:33:2224352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:39:33:2224350 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:39:33:2224350 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [33.0580s] [ 70%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardGradientAccumulation::test_1f1b_microbatching [2025-09-19 14:40:06.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:06.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:06.736] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:06.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:40:06:2224668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:06:2224668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:06:2224669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:06:2224669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:06:2224670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:06:2224670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:06:2224671 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:06:2224671 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
dist init r=1, world=4
PASSED [31.8564s] [ 75%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardGradientAccumulation::test_gradient_accumulation [2025-09-19 14:40:38.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:38.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:38.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:40:38.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:40:38:2224986 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:38:2224986 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:38:2224987 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:38:2224987 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:38:2224988 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:38:2224988 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:40:38:2224989 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:40:38:2224989 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:41:13:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:13:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:13:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:13:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:14:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:15:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:16:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:17:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:18:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:19:2224988:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:19:2224986:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:19:2224987:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:19:2224989:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:31:2225309:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:31:2225299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:31:2225294:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:41:31:2225304:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [72.3182s] [ 79%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardNDTraining::test_2d_mlp_with_nd_mesh [2025-09-19 14:41:50.940] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:41:50.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:41:50.952] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:41:50.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:41:51:2225468 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:41:51:2225468 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:41:51:2225465 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:41:51:2225465 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:41:51:2225466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:41:51:2225466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:41:51:2225467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:41:51:2225467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:04:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:04:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:20:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:20:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:20:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:20:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:21:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:22:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:23:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:24:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:25:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:26:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:27:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:28:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:29:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:30:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:31:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:32:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:33:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225465:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225467:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225466:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:42:34:2225468:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
dist init r=2, world=4
PASSED [46.5796s] [ 83%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardHSDP3DTraining::test_3d_mlp_with_nd_mesh [2025-09-19 14:42:37.484] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:37.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:37.509] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:37.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:42:37:2226119 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:37:2226119 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:37:2226117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:37:2226117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:37:2226120 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:37:2226120 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:37:2226118 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:37:2226118 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
SKIPPED [15.5294s] [ 87%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardHSDPTraining::test_train_parity_hsdp [2025-09-19 14:42:53.115] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:53.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:53.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:42:53.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:42:53:2226418 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:53:2226418 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:53:2226417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:53:2226417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:53:2226420 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:53:2226420 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:42:53:2226419 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:42:53:2226419 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:43:06:2226420:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226418:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226419:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226417:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226419:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226420:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226418:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:43:06:2226417:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=3, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [49.4809s] [ 91%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardCustomForwardMethod::test_register_fsdp_forward_method [2025-09-19 14:43:42.563] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:43:42.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:43:42:2226751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:43:42:2226751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:43:42:2226752 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:43:42:2226752 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=2
dist init r=1, world=2
PASSED [41.8654s] [ 95%]
../../../../test/distributed/_composable/fsdp/test_fully_shard_training.py::TestFullyShardWorldSize1::test_train_parity_single_worldsize1 [2025-09-19 14:44:24.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:44:24:2227101 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:44:24:2227101 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=0, world=1
PASSED [3.7068s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_fsdp_test_fully_shard_training.py.xml -
============ 12 passed, 11 skipped, 1 xfailed in 499.06s (0:08:19) =============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:44:29.075] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 10 items
Running 10 items in this shard

../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_bucketing_coalesced_op [2025-09-19 14:44:40.166] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:44:40.177] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
PASSED [24.8419s] [ 10%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_bucketing_concat_op [2025-09-19 14:45:05.029] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:45:05.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
PASSED [25.1424s] [ 20%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_backward_only [2025-09-19 14:45:30.285] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:45:30.302] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:45:43:2232132 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:45:43:2232132 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:45:44:2232131 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:45:44:2232131 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [59.8949s] [ 30%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_bf16 [2025-09-19 14:46:30.076] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:46:30.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:46:43:2233724 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:46:43:2233724 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:46:44:2233723 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:46:44:2233723 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [57.2918s] [ 40%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_cpu [2025-09-19 14:47:27.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:47:27.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
PASSED [24.2405s] [ 50%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_cpu_no_sync [2025-09-19 14:47:51.601] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:47:51.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
PASSED [25.6436s] [ 60%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_fp16 [2025-09-19 14:48:17.261] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:48:17.262] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:48:30:2241739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:48:30:2241739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:48:31:2241738 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:48:31:2241738 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [24.3398s] [ 70%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_gpu [2025-09-19 14:48:41.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:48:41.617] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:48:55:2243195 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:48:55:2243195 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:48:56:2243194 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:48:56:2243194 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [62.0975s] [ 80%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::ReplicateTest::test_compile_gpu_ac [2025-09-19 14:49:43.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:49:43.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:49:57:2244802 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:49:57:2244802 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:49:58:2244803 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:49:58:2244803 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [62.0978s] [ 90%]
../../../../test/distributed/_composable/test_replicate_with_compiler.py::DDP_TP_Test::test_ddp_tp SKIPPED [0.0003s] SymInt`) [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__composable_test_replicate_with_compiler.py.xml -
=================== 9 passed, 1 skipped in 376.72s (0:06:16) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:50:46.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/_shard/test_sharder.py::TestCustomSharder::test_custom_sharder [2025-09-19 14:50:49.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:50:49.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:50:49.174] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:50:49.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:50:49:2246487 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:50:49:2246487 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:50:49:2246488 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:50:49:2246488 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:50:49:2246486 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:50:49:2246486 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:50:49:2246485 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:50:49:2246485 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [31.8306s] [ 50%]
../../../../test/distributed/_shard/test_sharder.py::TestCustomSharder::test_custom_sharder_errors [2025-09-19 14:51:20.768] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:20.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:20.790] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:20.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:51:20:2246787 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:20:2246787 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:21:2246788 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:21:2246788 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:21:2246790 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:21:2246790 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:21:2246789 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:21:2246789 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6211s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_test_sharder.py.xml -
============================== 2 passed in 49.35s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:51:36.767] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/_shard/sharded_tensor/test_logger.py::ShardingSpecLoggerTest::test_get_or_create_logger PASSED [0.1645s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_sharded_tensor_test_logger.py.xml -
============================== 1 passed in 2.05s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 14:51:40.359] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 74 items
Running 74 items in this shard

../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorMetadata::test_serialize_and_deserialize PASSED [0.2051s] [  1%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestCreateTensorFromParams::test_empty PASSED [0.0016s] [  2%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardParameter::test_shard_parameter [2025-09-19 14:51:42.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:42.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:42.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:42.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:51:42:2247238 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:42:2247238 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:42:2247237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:42:2247237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:42:2247240 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:42:2247240 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:42:2247239 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:42:2247239 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7306s] [  4%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardParameter::test_shard_parameter_errors [2025-09-19 14:51:58.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:58.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:58.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:51:58.391] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:51:58:2247537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:58:2247537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:58:2247540 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:58:2247540 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:58:2247538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:58:2247538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:51:58:2247539 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:51:58:2247539 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7271s] [  5%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardTensor::test_shard_tensor [2025-09-19 14:52:14.024] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:14.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:14.045] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:14.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:52:14:2247840 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:14:2247840 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:14:2247838 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:14:2247838 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:14:2247839 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:14:2247839 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:14:2247841 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:14:2247841 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9275s] [  6%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardTensor::test_shard_tensor_errors [2025-09-19 14:52:29.919] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:29.968] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:29.995] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:30.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:52:30:2248140 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:30:2248140 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:30:2248141 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:30:2248141 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:30:2248142 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:30:2248142 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:30:2248143 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:30:2248143 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [  8%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardTensor::test_shard_tensor_with_empty_shard [2025-09-19 14:52:45.761] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:45.766] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:45.774] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:52:45.784] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:52:45:2248444 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:45:2248444 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:46:2248441 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:46:2248441 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:46:2248443 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:46:2248443 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:52:46:2248442 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:52:46:2248442 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8285s] [  9%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestModuleHookApi::test_collect_local_shard [2025-09-19 14:53:01.615] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:01.620] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:01.621] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:01.625] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:53:01:2248744 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:01:2248744 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:01:2248742 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:01:2248742 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:02:2248741 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:02:2248741 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:02:2248743 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:02:2248743 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6292s] [ 10%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestModuleHookApi::test_reshard_output [2025-09-19 14:53:17.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:17.261] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:17.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:17.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:53:17:2249044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:17:2249044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:17:2249045 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:17:2249045 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:17:2249043 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:17:2249043 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:17:2249042 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:17:2249042 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1302s] [ 12%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestLocalTensor::test_local_tensor [2025-09-19 14:53:33.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:33.386] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:33.398] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:33.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:53:33:2249342 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:33:2249342 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:33:2249345 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:33:2249345 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:33:2249343 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:33:2249343 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:33:2249344 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:33:2249344 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7296s] [ 13%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestLocalTensor::test_local_tensor_error [2025-09-19 14:53:49.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:49.207] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:49.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:53:49.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-14:53:49:2249643 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:49:2249643 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:49:2249644 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:49:2249644 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:49:2249645 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:49:2249645 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:53:49:2249642 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:53:49:2249642 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6292s] [ 14%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_cleanup [2025-09-19 14:54:04.727] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:04.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:04.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:04.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:54:05:2249946 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:05:2249947 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:05:2249947 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:05:2249946 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:05:2249945 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:05:2249944 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:05:2249945 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:05:2249944 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2310s] [ 16%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_complete_world_size [2025-09-19 14:54:20.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:20.990] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:21.065] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:21.087] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.2091s] [ 17%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_create_sharded_tensor_like [2025-09-19 14:54:24.184] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:24.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:24.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:24.242] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:54:24:2250960 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:24:2250960 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:24:2250962 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:24:2250962 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:24:2250963 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:24:2250963 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:24:2250961 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:24:2250961 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9296s] [ 18%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_create_sharded_tensor_with_full [2025-09-19 14:54:40.099] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:40.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:40.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:40.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:54:40:2251445 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:40:2251445 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:40:2251446 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:40:2251446 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:40:2251447 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:40:2251448 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:40:2251447 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:40:2251448 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7297s] [ 20%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_create_sharded_tensor_with_ones [2025-09-19 14:54:55.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:55.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:55.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:54:55.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:54:56:2251930 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:56:2251930 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:56:2251932 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:56:2251932 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:56:2251931 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:56:2251931 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:54:56:2251933 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:54:56:2251933 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7295s] [ 21%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_create_sharded_tensor_with_rand [2025-09-19 14:55:11.653] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:11.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:11.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:11.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:55:12:2252416 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:12:2252416 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:12:2252417 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:12:2252417 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:12:2252418 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:12:2252419 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:12:2252419 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:12:2252418 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0296s] [ 22%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_create_sharded_tensor_with_zeros [2025-09-19 14:55:27.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:27.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:27.689] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:27.694] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:55:28:2252903 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:28:2252903 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:28:2252905 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:28:2252905 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:28:2252906 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:28:2252904 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:28:2252906 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:28:2252904 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9296s] [ 24%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_gather_even [2025-09-19 14:55:43.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:43.556] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:43.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:43.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:55:44:2253387 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:44:2253387 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:44:2253389 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:44:2253389 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:44:2253390 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:44:2253390 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:55:44:2253388 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:55:44:2253388 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1297s] [ 25%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_gather_uneven [2025-09-19 14:55:59.699] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:59.717] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:59.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:55:59.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:56:00:2253874 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:00:2253874 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:00:2253871 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:00:2253871 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:00:2253872 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:00:2253872 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:00:2253873 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:00:2253873 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9295s] [ 27%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_insufficient_sharding_dims [2025-09-19 14:56:15.577] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:15.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:15.626] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:15.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9085s] [ 28%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_invalid_pg_rpc_ranks [2025-09-19 14:56:18.487] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:18.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:18.538] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:18.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
PASSED [3.0087s] [ 29%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_invalid_sharding [2025-09-19 14:56:21.535] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:21.535] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:21.539] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:21.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
PASSED [3.1088s] [ 31%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_load_state_dict_errors [2025-09-19 14:56:24.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:24.660] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:24.717] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:24.782] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
PASSED [3.2093s] [ 32%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_multiple_local_shards [2025-09-19 14:56:27.839] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:27.844] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:27.844] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:27.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.0090s] [ 33%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_new_group [2025-09-19 14:56:30.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:30.897] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:30.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:30.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.2092s] [ 35%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_partial_world_size [2025-09-19 14:56:34.075] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:34.081] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:34.081] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:34.085] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.0087s] [ 36%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_sharded_tensor_metadata [2025-09-19 14:56:37.067] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:37.076] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:37.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:37.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:56:37:2257498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:37:2257498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:37:2257501 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:37:2257501 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:37:2257500 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:37:2257499 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:37:2257500 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:37:2257499 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0299s] [ 37%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_sharded_tensor_sizes [2025-09-19 14:56:53.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:53.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:53.101] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:56:53.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:56:53:2257984 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:53:2257984 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:53:2257983 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:53:2257983 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:53:2257986 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:53:2257986 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:56:53:2257985 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:56:53:2257985 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9296s] [ 39%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_sharding_columns [2025-09-19 14:57:09.004] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:09.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:09.028] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:09.028] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8087s] [ 40%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_state_dict [2025-09-19 14:57:11.909] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:11.912] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:11.915] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:11.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:57:12:2258757 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:12:2258757 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:12:2258756 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:12:2258756 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:12:2258758 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:12:2258758 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:12:2258755 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:12:2258755 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3287s] [ 41%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_state_dict_new_group [2025-09-19 14:57:28.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:28.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:28.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:28.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:57:28:2259241 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:28:2259241 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:28:2259240 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:28:2259240 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:28:2259242 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:28:2259242 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:28:2259243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:28:2259243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:41:2259242:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-14:57:41:2259243:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [28.1466s] [ 43%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_state_dict_no_sharded_tensors [2025-09-19 14:57:56.308] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:56.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:56.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:57:56.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:57:56:2259728 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:56:2259728 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:56:2259729 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:56:2259729 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:56:2259731 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:56:2259731 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:57:56:2259730 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:57:56:2259730 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2306s] [ 44%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_create_sharded_tensor_with_ones [2025-09-19 14:58:12.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:12.591] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:12.592] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:12.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:58:13:2260212 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:13:2260215 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:13:2260212 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:13:2260215 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:13:2260214 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:13:2260214 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:13:2260213 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:13:2260213 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2310s] [ 45%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_gather_even [2025-09-19 14:58:28.787] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:28.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:28.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:28.800] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:58:29:2260697 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:29:2260697 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:29:2260700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:29:2260700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:29:2260699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:29:2260698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:29:2260699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:29:2260698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2307s] [ 47%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_gather_uneven [2025-09-19 14:58:45.003] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:45.036] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:45.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:45.043] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:58:45:2261183 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:45:2261183 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:45:2261184 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:45:2261184 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:45:2261182 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:45:2261182 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:58:45:2261181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:58:45:2261181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [4.0109s] [ 48%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_grid_sharding [2025-09-19 14:58:49.030] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:49.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:49.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:49.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.2092s] [ 50%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_multiple_local_shards [2025-09-19 14:58:52.219] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:52.273] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:52.284] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:52.297] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.0087s] [ 51%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_new_group [2025-09-19 14:58:55.215] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:55.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:55.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:55.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.1092s] [ 52%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_partial_world_size [2025-09-19 14:58:58.387] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:58.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:58.391] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:58:58.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.0091s] [ 54%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_sharded_tensor_device [2025-09-19 14:59:01.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:01.409] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:01.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:01.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:59:01:2263565 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:01:2263565 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:01:2263566 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:01:2263566 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:01:2263568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:01:2263568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:01:2263567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:01:2263567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2315s] [ 55%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_sharded_tensor_metadata [2025-09-19 14:59:17.587] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:17.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:17.617] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:17.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:59:18:2264052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:18:2264052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:18:2264053 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:18:2264053 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:18:2264051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:18:2264051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:18:2264050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:18:2264050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0297s] [ 56%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_sharded_tensor_to_accelerator [2025-09-19 14:59:33.613] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:33.618] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:33.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:33.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:59:34:2264535 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:34:2264535 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:34:2264537 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:34:2264537 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:34:2264538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:34:2264536 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:34:2264538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:34:2264536 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1266s] [ 58%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_sharded_tensor_to_cpu [2025-09-19 14:59:49.715] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:49.768] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:49.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 14:59:49.779] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-14:59:50:2265032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:50:2265032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:50:2265031 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:50:2265031 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:50:2265034 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:50:2265034 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-14:59:50:2265033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-14:59:50:2265033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0230s] [ 59%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_sharded_tensor_to_test [2025-09-19 15:00:05.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:05.795] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:05.795] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:05.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:00:06:2265534 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:06:2265534 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:00:06:2265531 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:06:2265532 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:06:2265531 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:06:2265533 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:06:2265532 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:06:2265533 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9284s] [ 60%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_uneven_shards [2025-09-19 15:00:21.699] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:21.713] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:21.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:21.715] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8084s] [ 62%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_with_rpc_names [2025-09-19 15:00:24.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:24.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:24.600] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:24.611] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.1089s] [ 63%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalTensor::test_init_from_local_tensor [2025-09-19 15:00:27.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:27.692] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:27.692] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:27.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.3094s] [ 64%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalTensor::test_init_from_local_tensor_errors [2025-09-19 15:00:30.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:30.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:30.955] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:30.956] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:00:31:2267264 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:31:2267264 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:31:2267265 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:31:2267266 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:31:2267266 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:31:2267265 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:31:2267267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:31:2267267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1290s] [ 66%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards [2025-09-19 15:00:47.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:47.082] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:47.129] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:47.143] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:00:47:2267748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:47:2267748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:47:2267749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:47:2267749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:47:2267751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:47:2267751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:47:2267750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:47:2267750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
write: error: buf 0x557289fbb650, size 394, shift 0
write: error: buf 0x55ff358f7410, size 394, shift 0
FAILED [4.3106s] [ 67%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_and_global_metadata [2025-09-19 15:00:51.381] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:51.381] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:51.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:51.449] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
FAILED [3.2088s] [ 68%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_and_global_metadata_invalid_shards [2025-09-19 15:00:54.604] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:54.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:54.627] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:00:54.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:00:55:2268711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:55:2268711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:55:2268712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:55:2268712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:55:2268714 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:55:2268714 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:00:55:2268713 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:00:55:2268713 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7300s] [ 70%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_and_global_metadata_with_all_zeros [2025-09-19 15:01:10.321] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:10.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:10.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:10.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:01:10:2269198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:10:2269198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:10:2269195 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:10:2269195 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:10:2269196 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:10:2269196 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:10:2269197 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:10:2269197 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5293s] [ 71%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_and_global_metadata_with_local_view [2025-09-19 15:01:25.863] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:25.863] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:25.864] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:25.887] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:01:26:2269496 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:26:2269496 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:26:2269495 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:26:2269495 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:26:2269498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:26:2269498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:26:2269497 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:26:2269497 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6295s] [ 72%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_invalid_local_shards [2025-09-19 15:01:41.483] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:41.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:41.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:41.500] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:01:41:2269796 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:41:2269796 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:41:2269798 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:41:2269798 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:41:2269799 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:41:2269799 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:41:2269797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:41:2269797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1300s] [ 74%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_invalid_pin_memory [2025-09-19 15:01:57.603] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:57.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:57.640] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:01:57.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:01:57:2270284 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:57:2270284 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:57:2270282 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:57:2270282 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:57:2270283 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:57:2270283 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:01:57:2270281 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:01:57:2270281 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9301s] [ 75%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_invalid_property_cross_ranks [2025-09-19 15:02:13.518] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:13.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:13.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:13.602] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:02:14:2270581 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:14:2270581 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:14:2270583 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:14:2270583 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:14:2270584 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:14:2270584 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:14:2270582 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:14:2270582 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2310s] [ 77%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_invalid_shards_gaps [2025-09-19 15:02:29.759] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:29.776] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:29.776] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:29.776] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:02:30:2271067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:30:2271067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:30:2271069 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:30:2271069 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:30:2271068 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:30:2271068 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:30:2271070 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:30:2271070 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0302s] [ 78%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_invalid_shards_overlap [2025-09-19 15:02:45.786] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:45.862] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:45.876] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:02:45.876] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:02:46:2271552 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:46:2271552 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:46:2271551 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:46:2271551 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:46:2271554 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:46:2271554 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:02:46:2271553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:02:46:2271553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.4315s] [ 79%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_new_group [2025-09-19 15:03:02.343] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:02.344] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:02.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:02.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:03:02:2272035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:02:2272035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:02:2272036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:02:2272036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:02:2272037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:02:2272037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:02:2272038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:02:2272038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:04:2272037:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:03:04:2272036:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:03:04:2272038:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.7321s] [ 81%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_with_different_glb_size [2025-09-19 15:03:18.935] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:19.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:19.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:19.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:03:19:2272529 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:19:2272529 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:19:2272527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:19:2272527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:19:2272526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:19:2272526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:19:2272528 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:19:2272528 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.7302s] [ 82%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_local_shards [2025-09-19 15:03:34.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:34.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:34.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:34.770] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:03:34:2272826 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:34:2272826 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:34:2272828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:34:2272828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:34:2272827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:34:2272827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:35:2272829 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:35:2272829 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6294s] [ 83%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_non_rw_sharded_recalc_for_metadata [2025-09-19 15:03:50.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:50.372] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:50.390] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:03:50.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:03:50:2273126 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:50:2273126 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:50:2273129 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:50:2273129 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:50:2273128 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:50:2273128 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:03:50:2273127 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:03:50:2273127 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8298s] [ 85%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_recalc_for_metadata [2025-09-19 15:04:06.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:06.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:06.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:06.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:04:06:2273427 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:06:2273427 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:06:2273428 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:06:2273428 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:06:2273429 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:06:2273429 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:06:2273430 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:06:2273430 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8334s] [ 86%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_st_base_init_from_local_shards_and_global_metadata [2025-09-19 15:04:22.072] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:22.072] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:22.072] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:22.090] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8089s] [ 87%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorCustomOps::test_custom_op [2025-09-19 15:04:24.817] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:24.835] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:24.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:24.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:04:25:2274011 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:25:2274011 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:25:2274012 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:25:2274012 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:25:2274014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:25:2274014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:25:2274013 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:25:2274013 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0261s] [ 89%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorCustomOps::test_custom_op_errors [2025-09-19 15:04:40.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:40.846] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:40.856] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:40.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:04:41:2274497 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:41:2274497 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:41:2274498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:41:2274498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:41:2274500 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:41:2274500 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:41:2274499 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:41:2274499 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9295s] [ 90%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorCustomOps::test_custom_op_override [2025-09-19 15:04:56.771] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:56.794] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:56.802] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:04:56.817] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:04:57:2274982 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:57:2274982 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:57:2274983 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:57:2274983 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:57:2274984 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:57:2274984 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:04:57:2274985 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:04:57:2274985 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.4300s] [ 91%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardMetadata::test_create_shard_with_no_placement [2025-09-19 15:05:13.183] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:13.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:13.248] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:13.259] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:05:13:2275467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:13:2275467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:13:2275468 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:13:2275469 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:13:2275468 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:13:2275469 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:13:2275470 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:13:2275470 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1294s] [ 93%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardMetadata::test_shard_metadata_init [2025-09-19 15:05:29.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:29.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:29.385] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:29.414] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:05:29:2275953 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:29:2275953 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:29:2275954 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:29:2275954 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:29:2275956 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:29:2275956 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:29:2275955 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:29:2275955 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.4309s] [ 94%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorSubGroupInit::test_sub_process_group_placement_validation PASSED [0.0238s] [ 95%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorSubGroupInit::test_sub_process_group_sharded_tensor_init PASSED [0.0066s] [ 97%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestCreateTensorNoProcessGroupMode::test_init_from_local_shards_and_global_metadata PASSED [0.0005s] [ 98%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestCreateTensorNoProcessGroupMode::test_non_contiguous_local_shards PASSED [0.0005s] [100%]

=================================== FAILURES ===================================
______________ TestShardedTensorChunked.test_complete_world_size _______________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 566, in test_complete_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_complete_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:54:19.109000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2250487
I0919 14:54:19.110000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2250488
I0919 14:54:19.110000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2250489
I0919 14:54:19.111000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2250490
_____________ TestShardedTensorChunked.test_multiple_local_shards ______________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 999, in test_multiple_local_shards
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_multiple_local_shards

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:56:25.976000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2256064
I0919 14:56:25.976000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2256065
I0919 14:56:25.977000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2256066
I0919 14:56:25.977000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2256067
___________________ TestShardedTensorChunked.test_new_group ____________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 942, in test_new_group
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_new_group

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:56:28.986000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2256537
I0919 14:56:28.987000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2256538
I0919 14:56:28.987000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2256539
I0919 14:56:28.988000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2256540
_______________ TestShardedTensorChunked.test_partial_world_size _______________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 886, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 886, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: ECONNRESET: connection reset by peer (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 0 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:56:32.197000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2257024
I0919 14:56:32.197000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2257025
I0919 14:56:32.198000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2257026
I0919 14:56:32.198000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2257027
________________ TestShardedTensorEnumerable.test_gather_uneven ________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 103, in wrapper
    self.destroy_comms(destroy_rpc=init_rpc)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 63, in destroy_comms
    dist.barrier()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875, in barrier
    work = group.barrier(opts=opts)
RuntimeError: oneCCL: ze_fd_manager.cpp:390 convert_fd_pidfd: EXCEPTION: pidfd_getfd failed: convert_from_fd: 318, fd: -1, handle: 297, errno: Bad file descriptor

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_gather_uneven

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 103, in wrapper
    self.destroy_comms(destroy_rpc=init_rpc)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 63, in destroy_comms
    dist.barrier()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875, in barrier
    work = group.barrier(opts=opts)
RuntimeError: oneCCL: ze_fd_manager.cpp:390 convert_fd_pidfd: EXCEPTION: pidfd_getfd failed: convert_from_fd: 12, fd: -1, handle: 297, errno: Bad file descriptor

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_gather_uneven

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:58:43.151000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2261181
I0919 14:58:43.152000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2261182
I0919 14:58:43.153000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2261183
I0919 14:58:43.153000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2261184
________________ TestShardedTensorEnumerable.test_grid_sharding ________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:58:47.164000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2261666
I0919 14:58:47.165000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2261667
I0919 14:58:47.165000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2261668
I0919 14:58:47.166000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2261669
____________ TestShardedTensorEnumerable.test_multiple_local_shards ____________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2233, in test_multiple_local_shards
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_multiple_local_shards

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:58:50.375000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2262141
I0919 14:58:50.375000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2262142
I0919 14:58:50.376000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2262143
I0919 14:58:50.376000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2262144
__________________ TestShardedTensorEnumerable.test_new_group __________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2148, in test_new_group
    shard = remote_shard.to_here()
RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_new_group

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:58:53.385000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2262615
I0919 14:58:53.385000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2262616
I0919 14:58:53.386000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2262617
I0919 14:58:53.386000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2262618
_____________ TestShardedTensorEnumerable.test_partial_world_size ______________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 14:58:56.496000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2263092
I0919 14:58:56.496000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2263093
I0919 14:58:56.497000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2263094
I0919 14:58:56.497000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2263095
_______________ TestShardedTensorEnumerable.test_with_rpc_names ________________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2305, in test_with_rpc_names
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_with_rpc_names

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 15:00:22.660000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2266315
I0919 15:00:22.661000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2266316
I0919 15:00:22.662000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2266317
I0919 15:00:22.662000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2266318
_________ TestShardedTensorFromLocalTensor.test_init_from_local_tensor _________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2379, in test_init_from_local_tensor
    self._generate_st_from_chunk_local_tensor([20, 10], spec)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2367, in _generate_st_from_chunk_local_tensor
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalTensor.test_init_from_local_tensor

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2379, in test_init_from_local_tensor
    self._generate_st_from_chunk_local_tensor([20, 10], spec)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2367, in _generate_st_from_chunk_local_tensor
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalTensor.test_init_from_local_tensor

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 1 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 15:00:25.771000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2266790
I0919 15:00:25.772000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2266791
I0919 15:00:25.773000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2266792
I0919 15:00:25.773000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2266793
_________ TestShardedTensorFromLocalShards.test_init_from_local_shards _________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1093, in _check_return_codes
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4180, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not equal!

Expected 0 but got -6.
Absolute difference: 6
Relative difference: inf
Expected exit code 0 but got -6 for pid: 2267748
----------------------------- Captured stderr call -----------------------------
I0919 15:00:45.212000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2267748
I0919 15:00:45.213000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2267749
I0919 15:00:45.213000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2267750
I0919 15:00:45.214000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2267751
_ TestShardedTensorFromLocalShards.test_init_from_local_shards_and_global_metadata _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2950, in test_init_from_local_shards_and_global_metadata
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalShards.test_init_from_local_shards_and_global_metadata

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 3 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 15:00:49.524000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2268237
I0919 15:00:49.525000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2268238
I0919 15:00:49.525000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2268239
I0919 15:00:49.526000 2247165 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2268240
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_sharded_tensor_test_sharded_tensor.py.xml -
=========================== short test summary info ============================
FAILED [3.2091s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_complete_world_size - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 566, in test_complete_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_complete_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0090s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_multiple_local_shards - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 999, in test_multiple_local_shards
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_multiple_local_shards

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.2092s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_new_group - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 942, in test_new_group
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_new_group

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0087s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorChunked::test_partial_world_size - RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 886, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 886, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: ECONNRESET: connection reset by peer (this error originated at tensorpipe/transport/uv/connection_impl.cc:132)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorChunked.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [4.0109s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_gather_uneven - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 103, in wrapper
    self.destroy_comms(destroy_rpc=init_rpc)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 63, in destroy_comms
    dist.barrier()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875, in barrier
    work = group.barrier(opts=opts)
RuntimeError: oneCCL: ze_fd_manager.cpp:390 convert_fd_pidfd: EXCEPTION: pidfd_getfd failed: convert_from_fd: 318, fd: -1, handle: 297, errno: Bad file descriptor

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_gather_uneven

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 103, in wrapper
    self.destroy_comms(destroy_rpc=init_rpc)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 63, in destroy_comms
    dist.barrier()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875, in barrier
    work = group.barrier(opts=opts)
RuntimeError: oneCCL: ze_fd_manager.cpp:390 convert_fd_pidfd: EXCEPTION: pidfd_getfd failed: convert_from_fd: 12, fd: -1, handle: 297, errno: Bad file descriptor

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_gather_uneven

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.2092s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_grid_sharding - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 1561, in test_grid_sharding
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_grid_sharding

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0087s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_multiple_local_shards - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2233, in test_multiple_local_shards
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_multiple_local_shards

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.1092s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_new_group - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2148, in test_new_group
    shard = remote_shard.to_here()
RuntimeError: async error on socket: Connection reset by peer (this error originated at tensorpipe/transport/shm/connection_impl.cc:187)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_new_group

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.0091s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_partial_world_size - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2078, in test_partial_world_size
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_partial_world_size

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.1089s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorEnumerable::test_with_rpc_names - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2305, in test_with_rpc_names
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorEnumerable.test_with_rpc_names

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [3.3094s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalTensor::test_init_from_local_tensor - RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2379, in test_init_from_local_tensor
    self._generate_st_from_chunk_local_tensor([20, 10], spec)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2367, in _generate_st_from_chunk_local_tensor
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalTensor.test_init_from_local_tensor

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2379, in test_init_from_local_tensor
    self._generate_st_from_chunk_local_tensor([20, 10], spec)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2367, in _generate_st_from_chunk_local_tensor
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalTensor.test_init_from_local_tensor

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
FAILED [4.3106s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards - AssertionError: Scalars are not equal!

Expected 0 but got -6.
Absolute difference: 6
Relative difference: inf
Expected exit code 0 but got -6 for pid: 2267748
FAILED [3.2088s] ../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor.py::TestShardedTensorFromLocalShards::test_init_from_local_shards_and_global_metadata - RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1918, in wrapper
    return fn(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102, in wrapper
    func(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 222, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharded_tensor/test_sharded_tensor.py", line 2950, in test_init_from_local_shards_and_global_metadata
    shard = remote_shard.to_here()
RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_shard/sharded_tensor/test_sharded_tensor.py TestShardedTensorFromLocalShards.test_init_from_local_shards_and_global_metadata

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
================== 13 failed, 61 passed in 845.46s (0:14:05) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:05:46.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor_reshard.py::TestReshard::test_sharded_tensor_reshard [2025-09-19 15:05:48.936] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:48.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:48.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:05:48.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:05:49:2276524 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:49:2276524 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:49:2276526 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:49:2276526 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:49:2276527 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:49:2276527 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:05:49:2276525 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:05:49:2276525 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3246s] [ 50%]
../../../../test/distributed/_shard/sharded_tensor/test_sharded_tensor_reshard.py::TestReshard::test_sharded_tensor_reshard_errors [2025-09-19 15:06:05.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:05.095] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:05.112] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:05.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:06:05:2276828 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:05:2276828 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:05:2276826 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:05:2276826 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:05:2276827 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:05:2276827 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:05:2276825 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:05:2276825 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6293s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_sharded_tensor_test_sharded_tensor_reshard.py.xml -
============================== 2 passed in 33.93s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:06:21.635] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/_shard/sharding_plan/test_sharding_plan.py::TestShardingPlan::test_custom_sharding_planner [2025-09-19 15:06:23.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:23.908] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:23.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:23.937] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:06:24:2277200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:24:2277200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:24:2277199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:24:2277199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:24:2277202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:24:2277202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:24:2277201 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:24:2277201 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.2359s] [ 33%]
../../../../test/distributed/_shard/sharding_plan/test_sharding_plan.py::TestShardingPlan::test_shard_module_sub_process_group [2025-09-19 15:06:39.827] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:39.871] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:39.882] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:39.887] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:06:40:2277500 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:40:2277500 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:40:2277502 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:40:2277502 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:40:2277503 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:40:2277503 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:40:2277501 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:40:2277501 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:41:2277503:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:06:41:2277502:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.4310s] [ 66%]
../../../../test/distributed/_shard/sharding_plan/test_sharding_plan.py::TestShardingPlan::test_sharding_plan_errors [2025-09-19 15:06:56.375] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:56.379] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:56.401] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:06:56.405] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:06:56:2277806 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:56:2277806 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:56:2277809 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:56:2277809 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:56:2277807 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:56:2277807 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:06:56:2277808 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:06:56:2277808 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8226s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_sharding_plan_test_sharding_plan.py.xml -
============================== 3 passed in 50.48s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:07:12.576] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 11 items
Running 11 items in this shard

../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_check_overlapping PASSED [0.0730s] [  9%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_chunked_sharding_spec SKIPPED [0.0002s] [ 18%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_device_placement SKIPPED [0.0001s] [ 27%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_enumerable_sharding_spec SKIPPED [0.0001s] [ 36%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_get_chunk_sharding_params PASSED [0.0008s] [ 45%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_get_chunked_dim_size PASSED [0.0006s] [ 54%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_get_split_size PASSED [0.0006s] [ 63%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestShardingSpec::test_infer_sharding_spec_from_shards_metadata PASSED [0.0050s] [ 72%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestCustomShardingSpec::test_custom_sharding_spec [2025-09-19 15:07:14.760] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:14.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:14.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:14.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1090s] [ 81%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestCustomShardingSpec::test_custom_sharding_spec_shard_tensor SKIPPED [0.0002s] [ 90%]
../../../../test/distributed/_shard/sharding_spec/test_sharding_spec.py::TestCustomShardingSpec::test_custom_sharding_spec_tensor_ctor SKIPPED [0.0003s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__shard_sharding_spec_test_sharding_spec.py.xml -
========================= 6 passed, 5 skipped in 5.30s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:07:19.407] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/_tools/test_fsdp2_mem_tracker.py::TestTrackerFullyShard1DTrainingCore::test_tracker_multi_group_eager [2025-09-19 15:07:21.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:21.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:21.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:21.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:07:21:2278541 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:21:2278541 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:21:2278539 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:21:2278539 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:21:2278540 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:21:2278540 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:21:2278538 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:21:2278538 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=2, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [28.2526s] [ 33%]
../../../../test/distributed/_tools/test_fsdp2_mem_tracker.py::TestTrackerFullyShard1DTrainingCore::test_tracker_non_root_forward_backward [2025-09-19 15:07:49.659] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:49.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:49.668] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:07:49.714] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:07:49:2279239 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:49:2279239 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:49:2279236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:49:2279236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:49:2279238 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:49:2279238 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:07:49:2279237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:07:49:2279237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
dist init r=1, world=4
PASSED [21.9399s] [ 66%]
../../../../test/distributed/_tools/test_fsdp2_mem_tracker.py::TestTrackerFullyShard1DTrainingCompose::test_tracker_with_activation_checkpointing [2025-09-19 15:08:11.619] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:08:11.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:08:11.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:08:11.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:08:11:2279552 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:08:11:2279552 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:08:11:2279555 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:08:11:2279555 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:08:11:2279554 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:08:11:2279554 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:08:11:2279553 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:08:11:2279553 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
PASSED [36.4649s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__tools_test_fsdp2_mem_tracker.py.xml -
========================= 3 passed in 88.66s (0:01:28) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:08:49.027] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/_tools/test_mem_tracker.py::TestMemTracker::test_accelerator_tracker_equivalence PASSED [0.4314s] [ 33%]
../../../../test/distributed/_tools/test_mem_tracker.py::TestMemTracker::test_tracker_attribution PASSED [0.0528s] [ 66%]
../../../../test/distributed/_tools/test_mem_tracker.py::TestMemTracker::test_tracker_with_activation_checkpointing PASSED [0.3616s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__tools_test_mem_tracker.py.xml -
============================== 3 passed in 2.84s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:08:52.895] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/_tools/test_memory_tracker.py::TestMemoryTracker::test_local_model PASSED [0.5043s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__tools_test_memory_tracker.py.xml -
============================== 1 passed in 2.50s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:08:56.423] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/_tools/test_mod_tracker.py::TestModTracker::test_ac PASSED [0.1934s] [ 25%]
../../../../test/distributed/_tools/test_mod_tracker.py::TestModTracker::test_bw_detection PASSED [0.0012s] [ 50%]
../../../../test/distributed/_tools/test_mod_tracker.py::TestModTracker::test_module_hierarchy PASSED [0.0023s] [ 75%]
../../../../test/distributed/_tools/test_mod_tracker.py::TestModTracker::test_user_hooks PASSED [0.0025s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed__tools_test_mod_tracker.py.xml -
============================== 4 passed in 2.19s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:08:59.147] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 19 items
Running 19 items in this shard

../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_different_ordered_state_dict_keys [2025-09-19 15:09:01.442] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:01.446] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:01.447] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:01.469] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:09:02:2280725 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:02:2280724 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:02:2280722 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:02:2280722 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:02:2280725 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:02:2280724 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:02:2280723 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:02:2280723 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp25bsvfd6
PASSED [15.9941s] [  5%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_False_async_checkpointer_type0_zoc_False [2025-09-19 15:09:17.190] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:17.207] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:17.215] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:17.215] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:09:18:2281037 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:18:2281037 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:18:2281036 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:18:2281036 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:18:2281038 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:18:2281038 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:18:2281035 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:18:2281035 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
still waiting... 1.0010207389714196
still waiting... 1.0010041180066764
still waiting... 1.0010091880103573
Using temp directory: /tmp/tmpfi70mlss
still waiting... 1.0010161779355258
PASSED [32.7593s] [ 10%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_False_async_checkpointer_type2_zoc_False [2025-09-19 15:09:49.942] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:49.953] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:49.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:09:49.956] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:09:51:2281372 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:51:2281372 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:51:2281370 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:51:2281370 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:51:2281371 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:51:2281371 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:09:51:2281373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:09:51:2281373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp414ilu1v
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-09-19 15:10:09.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:09.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:09.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:09.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
still waiting... 1.001023679971695
still waiting... 2.002070724964142
still waiting... 3.003096858970821
still waiting... 1.0010751300724223
still waiting... 2.002157666021958
still waiting... 3.003193264012225
still waiting... 1.0010245050070807
still waiting... 2.0020781829953194
still waiting... 3.003126777941361
still waiting... 1.0010303989984095
still waiting... 2.002083324943669
still waiting... 3.0031056609004736
PASSED [34.7601s] [ 15%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_False_async_checkpointer_type4_zoc_True [2025-09-19 15:10:24.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:24.750] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:24.754] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:24.758] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:10:25:2282003 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:25:2282003 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:25:2282001 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:25:2282001 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:25:2282002 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:25:2282002 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:25:2282000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:25:2282000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpa5s4_hfd
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-09-19 15:10:43.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:43.962] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:43.963] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:43.963] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
still waiting... 1.0010295070242137
still waiting... 2.0021049780771136
still waiting... 1.0010226119775325
still waiting... 2.002113203983754
still waiting... 1.001036386936903
still waiting... 2.002109471941367
still waiting... 1.0010296180844307
still waiting... 2.002157708047889
PASSED [33.8603s] [ 21%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_False_async_checkpointer_type5_zoc_True [2025-09-19 15:10:58.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:58.575] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:58.575] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:10:58.582] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:10:59:2282632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:59:2282632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:59:2282630 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:59:2282630 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:59:2282633 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:59:2282633 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:10:59:2282631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:10:59:2282631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
still waiting... 1.0006524439668283
Using temp directory: /tmp/tmpiwjvwhql
still waiting... 1.000986899016425
still waiting... 1.0009995941072702
still waiting... 1.0010079559870064
PASSED [32.7582s] [ 26%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_True_async_checkpointer_type1_zoc_False [2025-09-19 15:11:31.318] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:11:31.360] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:11:31.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:11:31.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:11:32:2282972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:11:32:2282972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:11:32:2282971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:11:32:2282971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:11:32:2282970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:11:32:2282970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:11:32:2282969 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:11:32:2282969 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
still waiting... 1.0010081719374284
still waiting... 1.0010016730520874
Using temp directory: /tmp/tmp3rh1s0w3
still waiting... 1.0010480229975656
still waiting... 1.001016384921968
PASSED [32.9592s] [ 31%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_async_cached_cache_staged_state_dict_True_async_checkpointer_type3_zoc_False [2025-09-19 15:12:04.320] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:04.331] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:04.332] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:04.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:12:05:2283306 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:05:2283306 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:05:2283304 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:05:2283304 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:05:2283305 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:05:2283305 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:05:2283307 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:05:2283307 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp_ujahck2
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-09-19 15:12:23.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:23.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:23.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:23.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
still waiting... 1.0002867210423574
still waiting... 2.001359994057566
still waiting... 1.0010257350513712
still waiting... 2.0020954749779776
still waiting... 1.000577147002332
still waiting... 2.0016328509664163
still waiting... 1.0001746279885992
still waiting... 2.0012707359855995
PASSED [33.8603s] [ 36%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_False_model_type0 [2025-09-19 15:12:38.146] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:38.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:38.157] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:12:38.157] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:12:39:2283936 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:39:2283936 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:39:2283935 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:39:2283935 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:39:2283937 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:39:2283937 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:12:39:2283934 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:12:39:2283934 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp_e8lrmry
PASSED [31.8571s] [ 42%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_False_model_type1 [2025-09-19 15:13:10.007] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:10.008] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:10.008] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:10.014] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:13:11:2284263 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:11:2284263 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:11:2284264 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:11:2284264 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:11:2284265 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:11:2284265 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:11:2284266 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:11:2284266 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:27:2284577:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:27:2284575:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:27:2284566:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:27:2284568:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:13:39:2284263:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:39:2284264:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:39:2284266:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:39:2284265:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284568:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284577:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284566:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284575:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284264:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284263:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284266:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:13:40:2284265:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpu7s34fss
PASSED [33.0591s] [ 47%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_False_model_type2 [2025-09-19 15:13:43.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:43.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:43.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:43.135] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:13:44:2284676 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:44:2284676 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:44:2284674 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:44:2284674 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:44:2284675 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:44:2284675 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:13:44:2284677 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:13:44:2284677 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpbr4bhahm
PASSED [16.2308s] [ 52%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_True_model_type0 [2025-09-19 15:13:59.333] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:59.337] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:59.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:13:59.360] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:14:06:2285004 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:06:2285004 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:06:2285005 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:06:2285005 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:06:2285003 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:06:2285003 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:06:2285002 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:06:2285002 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpsj38a_v_
PASSED [40.7630s] [ 57%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_True_model_type1 [2025-09-19 15:14:40.070] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:14:40.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:14:40.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:14:40.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:14:45:2287237 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:45:2287237 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:45:2287235 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:45:2287235 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:45:2287234 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:45:2287234 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:14:45:2287236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:14:45:2287236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:01:2287893:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:01:2287878:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:01:2287806:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:01:2288030:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:15:14:2287237:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287236:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287234:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287235:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287878:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287893:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2287806:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:14:2288030:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:15:2287234:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:15:2287236:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:15:2287235:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:15:15:2287237:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmp3kjdvckj
PASSED [38.5621s] [ 63%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_e2e_compile_True_model_type2 [2025-09-19 15:15:18.632] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:18.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:18.641] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:18.678] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:15:21:2288266 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:21:2288266 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:21:2288268 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:21:2288268 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:21:2288269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:21:2288269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:21:2288267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:21:2288267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmplhxnuwj9
PASSED [19.7306s] [ 68%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_no_dist [2025-09-19 15:15:38.377] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:38.377] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:38.379] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:38.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
No process group initialized, using temp directory: /tmp/tmpsdllg1np
No process group initialized, using temp directory: /tmp/tmp7hl6zdp6
No process group initialized, using temp directory: /tmp/tmpxrrnk16q
No process group initialized, using temp directory: /tmp/tmpwouezbgp
PASSED [2.9088s] [ 73%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_overwrite [2025-09-19 15:15:41.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:41.279] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:41.288] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:41.289] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:15:42:2289435 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:42:2289435 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:42:2289436 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:42:2289437 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:42:2289437 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:42:2289436 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:42:2289438 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:42:2289438 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpf0zqxvw3
PASSED [16.0291s] [ 78%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_partial_load [2025-09-19 15:15:57.307] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:57.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:57.332] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:15:57.337] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:15:58:2289750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:58:2289750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:58:2289749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:58:2289749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:58:2289751 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:58:2289751 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:15:58:2289748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:15:58:2289748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp76p_djo6
PASSED [31.6552s] [ 84%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestE2ESaveAndLoad::test_stateful_and_non_stateful_loads [2025-09-19 15:16:28.968] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:28.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:28.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:28.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
No process group initialized, using temp directory: /tmp/tmp27yv9um0
No process group initialized, using temp directory: /tmp/tmpza7mubhf
No process group initialized, using temp directory: /tmp/tmp8thsu6w4
No process group initialized, using temp directory: /tmp/tmpl45i8l02
PASSED [3.0087s] [ 89%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestNoCPU::test_no_cpu [2025-09-19 15:16:31.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:31.982] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:31.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:32.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:16:32:2290365 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:32:2290365 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:32:2290364 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:32:2290364 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:32:2290362 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:32:2290362 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:32:2290363 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:32:2290363 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [ 94%]
../../../../test/distributed/checkpoint/e2e/test_e2e_save_and_load.py::TestInitStateDict::test_init_state_dict [2025-09-19 15:16:47.775] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:47.797] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:47.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:47.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
No process group initialized, using temp directory: /tmp/tmpug6ytwn8
No process group initialized, using temp directory: /tmp/tmp48604kx4
No process group initialized, using temp directory: /tmp/tmpfm4eaqx_
No process group initialized, using temp directory: /tmp/tmpk6knbnfo
PASSED [2.9084s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_e2e_test_e2e_save_and_load.py.xml -
======================== 19 passed in 471.56s (0:07:51) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:16:52.151] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/checkpoint/e2e/test_fine_tuning.py::TestFineTuning::test_fine_tuning [2025-09-19 15:16:54.338] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:54.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:54.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:16:54.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:16:54:2291021 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:54:2291021 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:54:2291022 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:54:2291022 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:54:2291024 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:54:2291023 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:16:54:2291024 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:16:54:2291023 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
/tmp/tmpstchkjlu/pretrain /tmp/tmpstchkjlu/finetune
/tmp/tmpstchkjlu/pretrain /tmp/tmpstchkjlu/finetune
Using temp directory: /tmp/tmpstchkjlu
/tmp/tmpstchkjlu/pretrain /tmp/tmpstchkjlu/finetune
/tmp/tmpstchkjlu/pretrain /tmp/tmpstchkjlu/finetune
PASSED [22.3303s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_e2e_test_fine_tuning.py.xml -
============================== 1 passed in 24.37s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:17:16.917] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/checkpoint/e2e/test_fsdp_ep.py::TestFSDPWithEP::test_e2e [2025-09-19 15:17:19.120] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:19.120] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:19.120] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:19.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
SKIPPED [3.1756s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_e2e_test_fsdp_ep.py.xml -
============================== 1 skipped in 5.15s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:17:23.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 6 items
Running 6 items in this shard

../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_1d_fsdp_cpu_offload_full_model_state_dict [2025-09-19 15:17:25.687] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:25.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:25.720] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:25.742] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:17:25:2291789 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:25:2291789 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:25:2291788 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:25:2291788 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:25:2291790 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:25:2291790 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:25:2291791 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:25:2291791 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=2, world=4
dist init r=3, world=4
dist init r=1, world=4
dist init r=0, world=4
PASSED [15.8099s] [ 16%]
../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_1d_fsdp_get_model_state_dict [2025-09-19 15:17:41.335] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:41.357] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:41.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:41.365] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:17:41:2292088 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:41:2292088 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:41:2292089 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:41:2292089 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:41:2292090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:41:2292090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:41:2292091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:41:2292091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
dist init r=3, world=4
PASSED [16.3240s] [ 33%]
../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_save_with_fsdp1_and_load_with_fsdp2 [2025-09-19 15:17:57.667] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:57.671] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:57.671] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:17:57.713] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:17:57:2292394 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:57:2292394 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:57:2292391 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:57:2292391 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:57:2292392 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:57:2292392 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:17:57:2292393 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:17:57:2292393 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
dist init r=1, world=4
dist init r=2, world=4
dist init r=3, world=4
dist init r=0, world=4
Using temp directory: /tmp/tmp8k7wwjv1
Using temp directory: /tmp/tmpkcm94lu_
PASSED [31.9569s] [ 50%]
../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_save_with_fsdp1_and_load_with_fsdp2_tp [2025-09-19 15:18:29.598] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:18:29.675] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:18:29.702] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:18:29.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:18:29:2292712 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:18:29:2292712 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:18:29:2292709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:18:29:2292709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:18:29:2292710 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:18:29:2292710 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:18:29:2292711 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:18:29:2292711 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:18:43:2292711:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:43:2292709:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:43:2292710:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:43:2292712:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:58:2292710:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:58:2292709:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:58:2292711:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:18:58:2292712:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=3, world=4
dist init r=1, world=4
dist init r=2, world=4
dist init r=0, world=4
Using temp directory: /tmp/tmpi96wvp2g
PASSED [32.2568s] [ 66%]
../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_save_with_fsdp2_tp_and_load_with_tp [2025-09-19 15:19:01.873] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:01.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:01.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:01.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:19:02:2293041 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:02:2293041 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:02:2293043 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:02:2293043 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:02:2293044 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:02:2293044 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:02:2293042 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:02:2293042 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:14:2293042:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:14:2293041:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:14:2293044:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:14:2293043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:15:2293042:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:15:2293044:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:15:2293043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:15:2293041:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293041:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293042:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293044:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293042:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293044:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293041:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:31:2293043:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=1, world=4
dist init r=0, world=4
Using temp directory: /tmp/tmp5462dg9z
Using temp directory: /tmp/tmp4twr6xwj
dist init r=3, world=4
dist init r=2, world=4
PASSED [33.0581s] [ 83%]
../../../../test/distributed/checkpoint/fsdp/test_fsdp_dsd.py::TestFullyShardWithDistributedStateDict::test_save_with_tp_and_load_with_fsdp2_tp [2025-09-19 15:19:34.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:34.966] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:34.971] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:34.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:19:35:2293394 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:35:2293394 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:35:2293393 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:35:2293393 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:35:2293391 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:35:2293391 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:35:2293392 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:35:2293392 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:48:2293391:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:48:2293392:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:48:2293394:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:48:2293393:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:49:2293394:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:49:2293392:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:49:2293393:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:19:49:2293391:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
dist init r=2, world=4
dist init r=1, world=4
dist init r=3, world=4
dist init r=0, world=4
Using temp directory: /tmp/tmp_u9stglp
PASSED [17.0313s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_fsdp_test_fsdp_dsd.py.xml -
======================== 6 passed in 148.38s (0:02:28) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:19:52.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedCheckpointing::test_default_metadata [2025-09-19 15:19:55.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:19:55.110] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:19:55:2293797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:55:2293797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:19:55:2293798 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:19:55:2293798 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.1046s] [ 12%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedCheckpointing::test_tensor_metadata_with_missing_rank_spec [2025-09-19 15:20:09.973] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:09.986] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:20:10:2293949 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:10:2293949 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:10:2293950 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:10:2293950 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.8195s] [ 25%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_dummy_reader_works [2025-09-19 15:20:24.795] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:24.815] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:24.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:24.839] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:20:25:2294102 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:25:2294102 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:25:2294099 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:25:2294099 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:25:2294101 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:25:2294101 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:25:2294100 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:25:2294100 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8281s] [ 37%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_dummy_writer_works [2025-09-19 15:20:40.625] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:40.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:40.662] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:40.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:20:40:2294401 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:40:2294401 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:40:2294400 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:40:2294400 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:40:2294403 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:40:2294403 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:40:2294402 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:40:2294402 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9259s] [ 50%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_load_error_handling [2025-09-19 15:20:56.576] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:56.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:56.578] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:20:56.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:20:56:2294701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:56:2294701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:56:2294702 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:56:2294702 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:56:2294704 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:56:2294704 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:20:56:2294703 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:20:56:2294703 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.6313s] [ 62%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_load_error_handling_no_dist [2025-09-19 15:21:13.176] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:13.194] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:13.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:13.222] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9092s] [ 75%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_save_error_handling [2025-09-19 15:21:16.136] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:16.145] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:16.148] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:16.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:21:16:2295285 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:16:2295285 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:16:2295288 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:16:2295288 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:16:2295287 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:16:2295287 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:16:2295286 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:16:2295286 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.4271s] [ 87%]
../../../../test/distributed/checkpoint/test_checkpoint.py::TestDistributedFailure::test_save_error_handling_no_dist [2025-09-19 15:21:32.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:32.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:32.536] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:32.574] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.9073s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_checkpoint.py.xml -
======================== 8 passed in 102.53s (0:01:42) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:21:36.096] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/checkpoint/test_compatibility.py::TestDCPCompatbility::test_metadata PASSED [0.1883s] [ 25%]
../../../../test/distributed/checkpoint/test_compatibility.py::TestDCPCompatbility::test_sharded_tensor_dependency PASSED [0.0080s] [ 50%]
../../../../test/distributed/checkpoint/test_compatibility.py::TestDCPCompatbility::test_storage_meta PASSED [0.0026s] [ 75%]
../../../../test/distributed/checkpoint/test_compatibility.py::TestDCPCompatbility::test_with_v_2_3 PASSED [0.0125s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_compatibility.py.xml -
============================== 4 passed in 2.24s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:21:39.313] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/checkpoint/test_dedup_tensors.py::TestDedupTensor::test_dedup_shards PASSED [0.1655s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_dedup_tensors.py.xml -
============================== 1 passed in 2.19s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:21:42.425] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/checkpoint/test_dtensor_checkpoint.py::DTensorPlanner::test_distributed_tensor_planner [2025-09-19 15:21:44.616] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:44.623] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:44.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:21:44.657] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:21:45:2296089 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:45:2296089 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:45:2296088 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:45:2296088 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:45:2296091 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:45:2296091 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:21:45:2296090 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:21:45:2296090 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpxx_xbmm_
PASSED [16.2856s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_dtensor_checkpoint.py.xml -
============================== 1 passed in 18.33s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:22:01.636] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardPlacementChange::test_1d_to_1d_reshard_placement_change_extensions0 [2025-09-19 15:22:03.921] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:03.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:04.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:04.040] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:22:04:2296466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:04:2296466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:04:2296468 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:04:2296468 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:04:2296469 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:04:2296469 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:04:2296467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:04:2296467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp9qe0pnnt
PASSED [16.3956s] [ 14%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardPlacementChange::test_1d_to_1d_reshard_placement_change_extensions1 [2025-09-19 15:22:20.081] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:20.083] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:20.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:20.092] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:22:20:2296769 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:20:2296769 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:20:2296768 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:20:2296768 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:20:2296767 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:20:2296767 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:20:2296766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:20:2296766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmph9c2n5eh
PASSED [16.2302s] [ 28%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardPlacementChange::test_1d_to_1d_reshard_placement_change_extensions2 [2025-09-19 15:22:36.305] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:36.319] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:36.440] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:36.451] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:22:37:2297069 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:37:2297069 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:37:2297070 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:37:2297070 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:37:2297068 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:37:2297068 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:37:2297071 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:37:2297071 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp11layirq
PASSED [16.0279s] [ 42%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardPlacementChange::test_2d_to_2d_reshard_placement_change [2025-09-19 15:22:52.314] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:52.341] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:52.345] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:22:52.372] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:22:53:2297371 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:53:2297371 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:53:2297368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:53:2297368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:53:2297370 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:53:2297370 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:53:2297369 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:22:53:2297369 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:22:54:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:54:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:55:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:56:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:57:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:58:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:22:59:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:00:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:01:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:02:2297370:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:02:2297371:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:02:2297368:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:02:2297369:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpqxo5l5i4
PASSED [24.0394s] [ 57%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardMeshChange::test_1d_to_2d_reshard_mesh_change [2025-09-19 15:23:16.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:16.394] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:16.394] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:16.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:23:17:2297864 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:17:2297864 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:17:2297863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:17:2297863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:17:2297866 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:17:2297866 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:17:2297865 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:17:2297865 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:18:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:18:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:19:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297864:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297866:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297863:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:20:2297865:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmp_ws4e163
PASSED [18.7276s] [ 71%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardMeshChange::test_2d_to_1d_reshard_mesh_change [2025-09-19 15:23:35.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:35.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:35.129] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:35.129] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:23:35:2298260 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:35:2298260 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:35:2298261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:35:2298261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:35:2298259 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:35:2298259 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:35:2298262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:35:2298262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:37:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:37:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:38:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298259:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298260:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298262:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:39:2298261:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpd6vu664z
PASSED [18.9258s] [ 85%]
../../../../test/distributed/checkpoint/test_dtensor_resharding.py::TestDTensorReshardMeshChange::test_dtensor_checkpoint_resharding_with_empty_shard [2025-09-19 15:23:54.015] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:54.015] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:54.049] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:23:54.068] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:23:54:2298627 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:54:2298627 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:54:2298626 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:54:2298626 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:54:2298625 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:54:2298625 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:54:2298624 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:23:54:2298624 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:23:56:2298624:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298625:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298626:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298627:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298626:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298624:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298625:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:23:56:2298627:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpc0j77eq5
PASSED [16.6308s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_dtensor_resharding.py.xml -
======================== 7 passed in 128.94s (0:02:08) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:24:11.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 9 items
Running 9 items in this shard

../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedStateDictSaveLoad::test_read_write_only_tensor PASSED [0.1921s] [ 11%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedStateDictSaveLoadWithSharedTensor::test_read_write_shard_tensor_extensions0 [2025-09-19 15:24:14.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:24:14.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:24:14:2299015 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:14:2299015 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:24:14:2299014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:14:2299014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4210s] [ 22%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedStateDictSaveLoadWithSharedTensor::test_read_write_shard_tensor_extensions1 [2025-09-19 15:24:29.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:24:29.658] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:24:29:2299166 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:29:2299166 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:24:29:2299165 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:29:2299165 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.3260s] [ 33%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedStateDictSaveLoadWithSharedTensor::test_read_write_shard_tensor_extensions2 [2025-09-19 15:24:44.952] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:24:44.954] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:24:45:2299316 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:45:2299316 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:24:45:2299315 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:24:45:2299315 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.2257s] [ 44%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedReshardOnLoad::test_load_rowwise_to_colwise [2025-09-19 15:25:00.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:25:00.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:25:00:2299467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:00:2299467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:25:00:2299466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:00:2299466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.2191s] [ 55%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedReshardOnLoad::test_load_with_different_shard_plan [2025-09-19 15:25:15.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:25:15.354] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:25:15:2299617 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:15:2299617 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:25:15:2299616 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:15:2299616 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5258s] [ 66%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedReshardOnLoad::test_save_load_bytes [2025-09-19 15:25:30.894] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:25:30.906] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:25:31:2299768 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:31:2299768 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:25:31:2299767 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:31:2299767 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.2257s] [ 77%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedReshardOnLoad::test_switch_between_sharded_tensor_to_tensor [2025-09-19 15:25:46.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:25:46.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:25:46:2299919 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:46:2299919 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:25:46:2299918 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:25:46:2299918 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5256s] [ 88%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint.py::TestDistributedStateDictSaveLoadWithCaching::test_read_write_shard_tensor [2025-09-19 15:26:01.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:26:01.734] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:26:01:2300069 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:01:2300069 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:26:01:2300070 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:01:2300070 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmprx980izn
PASSED [15.3195s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_file_system_checkpoint.py.xml -
======================== 9 passed in 124.93s (0:02:04) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:26:17.397] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 16 items
Running 16 items in this shard

../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoad::test_read_write_only_tensor_thread_count_1 PASSED [0.1737s] [  6%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoad::test_read_write_only_tensor_thread_count_2 PASSED [0.0058s] [ 12%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadRot13::test_read_write_tensor_and_blob_thread_count_1 PASSED [0.0099s] [ 18%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadRot13::test_read_write_tensor_and_blob_thread_count_2 PASSED [0.0107s] [ 25%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadZStandard::test_read_write_only_tensor_thread_count_1 PASSED [0.0063s] [ 31%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadZStandard::test_read_write_only_tensor_thread_count_2 PASSED [0.0064s] [ 37%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadWithSharedTensor::test_read_write_shard_tensor_thread_count_1 [2025-09-19 15:26:19.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:26:19.642] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:26:20:2300296 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:20:2300296 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:26:20:2300295 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:20:2300295 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.2229s] [ 43%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedStateDictSaveLoadWithSharedTensor::test_read_write_shard_tensor_thread_count_2 [2025-09-19 15:26:34.829] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:26:34.830] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:26:35:2300447 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:35:2300447 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:26:35:2300446 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:35:2300446 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.3238s] [ 50%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_rowwise_to_colwise_thread_count_1 [2025-09-19 15:26:50.172] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:26:50.185] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:26:50:2300599 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:50:2300599 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:26:50:2300598 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:26:50:2300598 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [300.0238s] [ 56%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_rowwise_to_colwise_thread_count_2 [2025-09-19 15:31:50.211] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:31:50.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:31:51:2300758 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:31:51:2300758 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:31:51:2300757 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:31:51:2300757 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [300.0240s] [ 62%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_with_different_shard_plan_thread_count_1 [2025-09-19 15:36:50.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:36:50.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:36:51:2300924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:36:51:2300924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:36:51:2300923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:36:51:2300923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [300.0266s] [ 68%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_with_different_shard_plan_thread_count_2 [2025-09-19 15:41:50.274] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:41:50.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:41:51:2301083 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:41:51:2301083 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:41:51:2301082 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:41:51:2301082 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [300.0205s] [ 75%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_save_load_bytes_thread_count_1 [2025-09-19 15:46:50.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:46:50.315] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:46:51:2301243 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:46:51:2301243 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:46:51:2301242 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:46:51:2301242 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.3259s] [ 81%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_save_load_bytes_thread_count_2 [2025-09-19 15:47:05.595] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:05.609] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:47:06:2301397 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:06:2301397 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:06:2301396 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:06:2301396 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.1259s] [ 87%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_switch_between_sharded_tensor_to_tensor_thread_count_1 SKIPPED [0.0002s] [ 93%]
../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_switch_between_sharded_tensor_to_tensor_thread_count_2 SKIPPED [0.0001s] [100%]

=================================== FAILURES ===================================
___ TestDistributedReshardOnLoad.test_load_rowwise_to_colwise_thread_count_1 ___
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.02076864242554 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 15:26:48.762000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2300598
I0919 15:26:48.762000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2300599
E0919 15:31:48.780000 2300220 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f32b7c4f640 (most recent call first):
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f33b86c8e00 (most recent call first):
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4258 in gather
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py", line 477 in gather
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 295 in load_tensor
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 451 in test_load_rowwise_to_colwise
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102 in wrapper
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552 in instantiated_test
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 15:31:48.783000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
___ TestDistributedReshardOnLoad.test_load_rowwise_to_colwise_thread_count_2 ___
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.0214264392853 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 15:31:48.787000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2300757
I0919 15:31:48.787000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2300758
E0919 15:36:48.806000 2300220 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007fd39d50f640 (most recent call first):
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fd49df8be00 (most recent call first):
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4258 in gather
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py", line 477 in gather
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 295 in load_tensor
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 451 in test_load_rowwise_to_colwise
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102 in wrapper
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552 in instantiated_test
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 15:36:48.808000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
_ TestDistributedReshardOnLoad.test_load_with_different_shard_plan_thread_count_1 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.02337098121643 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 15:36:48.812000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2300923
I0919 15:36:48.813000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2300924
E0919 15:41:48.834000 2300220 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f0b1437f640 (most recent call first):
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f0c14dfae00 (most recent call first):
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4258 in gather
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py", line 477 in gather
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 295 in load_tensor
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 399 in test_load_with_different_shard_plan
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102 in wrapper
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552 in instantiated_test
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 15:41:48.836000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
_ TestDistributedReshardOnLoad.test_load_with_different_shard_plan_thread_count_2 _
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.0174992084503 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 15:41:48.841000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2301082
I0919 15:41:48.841000 2300220 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2301083
E0919 15:46:48.856000 2300220 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f4684c0f640 (most recent call first):
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f4785687e00 (most recent call first):
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4258 in gather
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/_shard/sharded_tensor/api.py", line 477 in gather
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 295 in load_tensor
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/checkpoint/test_file_system_checkpoint_cpu.py", line 399 in test_load_with_different_shard_plan
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py", line 102 in wrapper
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552 in instantiated_test
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 15:46:48.859000 2300220 site-packages/torch/testing/_internal/common_distributed.py:965] 
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_file_system_checkpoint_cpu.py.xml -
=========================== short test summary info ============================
FAILED [300.0238s] ../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_rowwise_to_colwise_thread_count_1 - RuntimeError: Process 0 terminated or timed out after 300.02076864242554 seconds
FAILED [300.0240s] ../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_rowwise_to_colwise_thread_count_2 - RuntimeError: Process 0 terminated or timed out after 300.0214264392853 seconds
FAILED [300.0266s] ../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_with_different_shard_plan_thread_count_1 - RuntimeError: Process 0 terminated or timed out after 300.02337098121643 seconds
FAILED [300.0205s] ../../../../test/distributed/checkpoint/test_file_system_checkpoint_cpu.py::TestDistributedReshardOnLoad::test_load_with_different_shard_plan_thread_count_2 - RuntimeError: Process 0 terminated or timed out after 300.0174992084503 seconds
============= 4 failed, 10 passed, 2 skipped in 1263.26s (0:21:03) =============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:47:22.243] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/checkpoint/test_format_utils.py::TestFormatUtils::test_dcp_to_torch_save [2025-09-19 15:47:24.436] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:24.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:24.479] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:24.479] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
No process group initialized, using temp directory: /tmp/tmpnjunddzp
No process group initialized, using temp directory: /tmp/tmpa4llf_ae
No process group initialized, using temp directory: /tmp/tmp9mc5y69m
No process group initialized, using temp directory: /tmp/tmpmojt74lg
PASSED [3.1984s] [ 33%]
../../../../test/distributed/checkpoint/test_format_utils.py::TestFormatUtils::test_online_torch_save_to_dcp [2025-09-19 15:47:27.479] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:27.491] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:27.491] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:27.498] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:47:27:2301909 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:27:2301909 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:27:2301910 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:27:2301910 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:27:2301908 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:27:2301908 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:27:2301911 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:27:2301911 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp9y4j_u48
PASSED [16.0257s] [ 66%]
../../../../test/distributed/checkpoint/test_format_utils.py::TestFormatUtils::test_torch_save_to_dcp [2025-09-19 15:47:43.464] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:43.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:43.502] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:43.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
No process group initialized, using temp directory: /tmp/tmpdqeze_gt
No process group initialized, using temp directory: /tmp/tmp886yx7p8
No process group initialized, using temp directory: /tmp/tmpnn0wj0qc
No process group initialized, using temp directory: /tmp/tmpyyrxo7kd
PASSED [2.9086s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_format_utils.py.xml -
============================== 3 passed in 24.16s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:47:47.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/test_fsdp_model_state.py::FsdpModelStateCheckpoint::test_fsdp_model_state_no_resharding [2025-09-19 15:47:49.443] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:49.443] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:49.475] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:47:49.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:47:49:2302572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:49:2302572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:49:2302570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:49:2302570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:49:2302571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:49:2302571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:47:49:2302569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:47:49:2302569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpwfw0cnyt
PASSED [31.6181s] [ 50%]
../../../../test/distributed/checkpoint/test_fsdp_model_state.py::FsdpModelStateCheckpoint::test_fsdp_model_state_with_resharding [2025-09-19 15:48:20.910] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:20.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:20.926] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:20.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:48:21:2302901 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:21:2302903 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:21:2302903 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:21:2302901 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:21:2302900 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:21:2302900 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:21:2302902 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:21:2302902 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:37:2302901:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:48:37:2302903:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:48:37:2302902:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:48:37:2302900:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpzuzhghqa
PASSED [31.7560s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_fsdp_model_state.py.xml -
========================= 2 passed in 65.31s (0:01:05) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:48:53.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/test_fsdp_optim_state.py::FsdpOptimStateCheckpoint::test_load_sharded_optimizer_state_dict_pass_planner_False [2025-09-19 15:48:55.851] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:55.851] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:55.853] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:48:55.860] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:48:56:2303325 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:56:2303325 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:56:2303323 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:56:2303323 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:56:2303324 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:56:2303324 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:48:56:2303322 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:48:56:2303322 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp2lxxe9by
PASSED [31.9619s] [ 50%]
../../../../test/distributed/checkpoint/test_fsdp_optim_state.py::FsdpOptimStateCheckpoint::test_load_sharded_optimizer_state_dict_pass_planner_True [2025-09-19 15:49:27.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:49:27.622] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:49:27.627] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:49:27.650] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
2025:09:19-15:49:27:2303652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:49:27:2303652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:49:27:2303654 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:49:27:2303654 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:49:27:2303653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:49:27:2303653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:49:27:2303651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:49:27:2303651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpj4p3dqco
PASSED [31.6561s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_fsdp_optim_state.py.xml -
========================= 2 passed in 65.65s (0:01:05) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:49:59.637] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 1 item
Running 1 items in this shard

../../../../test/distributed/checkpoint/test_fsdp_tp_checkpoint_conversion.py::TestFsdpTpCheckpointConversion::test_fsdp_to_tp [2025-09-19 15:50:01.931] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:50:01.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:50:01.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:50:01.950] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:50:02:2304058 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:02:2304058 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:50:02:2304056 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:02:2304056 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:50:02:2304059 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:02:2304059 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:50:02:2304057 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:02:2304057 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp1e42xqa_
PASSED [16.8206s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_fsdp_tp_checkpoint_conversion.py.xml -
============================== 1 passed in 18.76s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:50:19.951] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/checkpoint/test_fsspec.py::TestFSSpec::test_fsspec [2025-09-19 15:50:22.198] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:50:22.214] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:50:22:2304449 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:22:2304449 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:50:22:2304448 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:22:2304448 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp5e3eiyzu
PASSED [30.6511s] [ 33%]
../../../../test/distributed/checkpoint/test_fsspec.py::TestFSSpec::test_overwrite [2025-09-19 15:50:52.698] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:50:52.701] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:50:52:2304608 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:52:2304608 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:50:52:2304607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:50:52:2304607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpz9rssvcy
PASSED [15.3262s] [ 66%]
../../../../test/distributed/checkpoint/test_fsspec.py::TestFileSystem::test_remove_on_fail PASSED [0.0027s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_fsspec.py.xml -
============================== 3 passed in 48.03s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:51:08.814] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/checkpoint/test_hsdp_checkpoint.py::TestHSDPCheckpoint::test_hsdp_checkpoint_is_even_sharded_model_False [2025-09-19 15:51:11.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:11.074] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:11.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:11.086] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:51:11:2304832 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:11:2304832 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:11:2304835 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:11:2304835 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:11:2304833 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:11:2304833 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:11:2304834 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:11:2304834 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:27:2305177:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:27:2305183:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:28:2305190:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:28:2305188:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:40:2304832:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:40:2304834:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:40:2304833:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:51:40:2304835:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpvkw4a6h2
PASSED [32.6222s] [ 25%]
../../../../test/distributed/checkpoint/test_hsdp_checkpoint.py::TestHSDPCheckpoint::test_hsdp_checkpoint_is_even_sharded_model_True [2025-09-19 15:51:43.513] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:43.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:43.534] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:51:43.537] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:51:43:2305202 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:43:2305202 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:43:2305203 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:43:2305203 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:43:2305204 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:43:2305204 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:51:43:2305205 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:51:43:2305205 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:00:2305558:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:00:2305552:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:00:2305549:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:00:2305555:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:12:2305203:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:12:2305202:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:12:2305204:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:12:2305205:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmph5puu1m1
PASSED [32.3576s] [ 50%]
../../../../test/distributed/checkpoint/test_hsdp_checkpoint.py::TestHSDPCheckpoint::test_hsdp_fsdp_checkpoint_conversion_is_even_sharded_model_False [2025-09-19 15:52:15.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:15.857] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:15.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:15.878] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:52:16:2305570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:16:2305570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:16:2305572 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:16:2305572 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:16:2305573 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:16:2305573 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:16:2305571 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:16:2305571 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:17:2305570:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:17:2305572:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:17:2305573:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:17:2305571:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmpqmg8r4ee
PASSED [16.4222s] [ 75%]
../../../../test/distributed/checkpoint/test_hsdp_checkpoint.py::TestHSDPCheckpoint::test_hsdp_fsdp_checkpoint_conversion_is_even_sharded_model_True [2025-09-19 15:52:32.290] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:32.329] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:32.329] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:32.329] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
2025:09:19-15:52:32:2305924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:32:2305924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:32:2305926 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:32:2305926 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:32:2305923 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:32:2305923 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:32:2305925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:32:2305925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:33:2305923:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:33:2305926:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:33:2305925:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:52:33:2305924:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
Using temp directory: /tmp/tmp2gcv8skd
PASSED [16.5293s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_hsdp_checkpoint.py.xml -
========================= 4 passed in 99.89s (0:01:39) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:52:49.275] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/test_nested_dict.py::TestFlattening::test_flattening_round_trip PASSED [0.1658s] [ 50%]
../../../../test/distributed/checkpoint/test_nested_dict.py::TestFlattening::test_mapping PASSED [0.0012s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_nested_dict.py.xml -
============================== 2 passed in 2.18s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:52:52.312] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 15 items
Running 15 items in this shard

../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_dedup_plans PASSED [0.1909s] [  6%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_finish_plan_with_caching PASSED [0.0010s] [ 13%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_global_plan PASSED [0.0027s] [ 20%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_global_plan_with_caching PASSED [0.0047s] [ 26%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_load_with_resharding PASSED [0.0019s] [ 33%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_load_with_world_size_diff_by_one PASSED [0.0014s] [ 40%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_local_load_plan PASSED [0.0014s] [ 46%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_local_plan PASSED [0.0015s] [ 53%]
../../../../test/distributed/checkpoint/test_planner.py::TestSavePlan::test_local_plan_with_caching PASSED [0.0009s] [ 60%]
../../../../test/distributed/checkpoint/test_planner.py::TestPlannerHelpers::test_compare_save_plans PASSED [0.0009s] [ 66%]
../../../../test/distributed/checkpoint/test_planner.py::TestPlannerHelpers::test_create_read_item_from_chunks PASSED [0.0009s] [ 73%]
../../../../test/distributed/checkpoint/test_planner.py::TestPlannerHelpers::test_merge_delta_local_plans PASSED [0.0039s] [ 80%]
../../../../test/distributed/checkpoint/test_planner.py::TestLoadPlanner::test_load_different_sizes_throws PASSED [0.0136s] [ 86%]
../../../../test/distributed/checkpoint/test_planner.py::TestLoadPlanner::test_strict PASSED [0.0046s] [ 93%]
../../../../test/distributed/checkpoint/test_planner.py::TestLoadPlanner::test_version_key_in_planner_data PASSED [0.0038s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_planner.py.xml -
============================== 15 passed in 2.18s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:52:55.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/test_save_load_api.py::TestSaveAndLoadAPI::test_assert_same_keys [2025-09-19 15:52:57.809] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:52:57.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:52:58:2306488 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:58:2306488 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:52:58:2306489 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:52:58:2306489 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.3881s] [ 50%]
../../../../test/distributed/checkpoint/test_save_load_api.py::TestSaveAndLoadAPI::test_auto_detect [2025-09-19 15:53:12.955] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:12.970] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:53:13:2306639 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:13:2306639 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:13:2306638 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:13:2306638 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpeg8nvp0f
PASSED [15.3243s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_save_load_api.py.xml -
============================== 2 passed in 32.74s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 15:53:29.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 25 items
Running 25 items in this shard

../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_activation_ckpt_fqns_ddp [2025-09-19 15:53:32.103] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:32.113] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:32.125] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:32.133] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:53:32:2306865 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:32:2306865 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:32:2306863 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:32:2306863 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:32:2306864 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:32:2306864 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:32:2306866 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:32:2306866 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1090s] [  4%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_activation_ckpt_fqns_fsdp1 [2025-09-19 15:53:48.077] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:48.095] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:48.195] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:53:48.208] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:53:48:2307166 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:48:2307164 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:48:2307166 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:48:2307164 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:48:2307165 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:48:2307165 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:53:48:2307167 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:53:48:2307167 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8255s] [  8%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_broadcast_from_rank0 [2025-09-19 15:54:03.875] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:03.879] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:03.880] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:03.892] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:54:04:2307466 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:04:2307466 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:04:2307465 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:04:2307465 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:04:2307464 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:04:2307464 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:04:2307467 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:04:2307467 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:20:2307466:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307467:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307465:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307464:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307768:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307762:[2] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307765:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-15:54:20:2307775:[3] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [32.3545s] [ 12%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_compiled_fsdp [2025-09-19 15:54:36.221] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:36.225] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:36.226] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:54:36.234] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:54:36:2308181 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:36:2308181 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:36:2308183 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:36:2308183 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:36:2308180 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:36:2308180 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:54:36:2308182 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:54:36:2308182 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [28.6508s] [ 16%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_cpu_offload_full_state_dict [2025-09-19 15:55:04.917] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:04.936] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:04.947] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:04.947] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:55:05:2308500 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:05:2308500 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:05:2308499 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:05:2308499 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:05:2308497 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:05:2308497 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:05:2308498 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:05:2308498 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.3314s] [ 20%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_ddp [2025-09-19 15:55:21.230] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:21.236] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:21.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:21.263] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:55:21:2308798 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:21:2308798 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:21:2308797 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:21:2308797 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:21:2308800 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:21:2308800 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:21:2308799 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:21:2308799 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.0332s] [ 24%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_deprecate_api [2025-09-19 15:55:39.247] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:39.256] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:39.256] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:39.275] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:55:39:2309117 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:39:2309117 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:39:2309115 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:39:2309115 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:39:2309116 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:39:2309116 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:39:2309114 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:39:2309114 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9295s] [ 28%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_extra_state [2025-09-19 15:55:55.133] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:55.154] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:55.169] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:55.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.2090s] [ 32%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_flattened_osd [2025-09-19 15:55:58.363] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:58.382] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:58.391] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:55:58.391] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:55:58:2309701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:58:2309701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:58:2309700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:58:2309700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:58:2309699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:58:2309699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:55:58:2309698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:55:58:2309698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [18.9347s] [ 36%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_fsdp [2025-09-19 15:56:17.319] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:56:17.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:56:17.333] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:56:17.335] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:56:17:2310016 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:56:17:2310016 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:56:17:2310014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:56:17:2310014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:56:17:2310015 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:56:17:2310015 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:56:17:2310017 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:56:17:2310017 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [44.6762s] [ 40%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_fsdp2 [2025-09-19 15:57:01.989] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:02.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:02.030] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:02.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:57:03:2310334 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:03:2310334 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:03:2310333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:03:2310333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:03:2310332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:03:2310332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:03:2310331 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:03:2310331 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [21.9385s] [ 44%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_fsdp_ddp [2025-09-19 15:57:23.899] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:23.974] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:23.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:23.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:57:24:2310650 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:24:2310650 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:24:2310651 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:24:2310651 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:24:2310649 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:24:2310649 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:24:2310648 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:24:2310648 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [28.5507s] [ 48%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_fsdp_root_not_initialized [2025-09-19 15:57:52.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:52.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:52.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:57:52.539] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:57:52:2310966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:52:2310966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:52:2310968 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:52:2310968 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:52:2310965 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:52:2310965 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:57:52:2310967 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:57:52:2310967 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0294s] [ 52%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_multi_device_load_model_state_dict [2025-09-19 15:58:08.505] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:08.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:08.542] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:08.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:58:08:2311267 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:08:2311267 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:08:2311268 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:08:2311268 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:08:2311266 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:08:2311266 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:08:2311269 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:08:2311269 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9301s] [ 56%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_multi_param_groups [2025-09-19 15:58:24.435] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:24.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:24.465] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:24.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:58:24:2311568 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:24:2311568 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:24:2311567 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:24:2311567 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:24:2311569 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:24:2311569 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:24:2311570 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:24:2311570 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.6308s] [ 60%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_non_persistent_buffers [2025-09-19 15:58:41.076] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:41.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:41.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:41.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.2089s] [ 64%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_optim_state_dict_param_matching [2025-09-19 15:58:44.291] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:44.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:44.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:58:44.334] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:58:44:2312169 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:44:2312169 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:44:2312167 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:44:2312167 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:44:2312168 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:44:2312168 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:58:44:2312170 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:58:44:2312170 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8296s] [ 68%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_set_cpu_model_state_dict_broadcast_from_rank0 [2025-09-19 15:59:00.100] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:00.114] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:00.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:00.137] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:59:00:2312470 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:00:2312470 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:00:2312469 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:00:2312469 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:00:2312468 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:00:2312468 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:00:2312471 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:00:2312471 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8298s] [ 72%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_setting_meta_device_model [2025-09-19 15:59:15.931] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:15.991] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:15.997] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:16.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:59:16:2312770 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:16:2312770 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:16:2312769 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:16:2312769 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:16:2312771 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:16:2312771 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:16:2312772 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:16:2312772 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9296s] [ 76%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_setting_meta_device_model_broadcasting_and_memory [2025-09-19 15:59:31.876] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:31.890] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:31.893] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 15:59:31.919] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-15:59:33:2313071 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:33:2313071 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:33:2313073 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:33:2313073 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:33:2313070 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:33:2313070 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-15:59:33:2313072 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-15:59:33:2313072 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [300.0659s] [ 80%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_shared_weight [2025-09-19 16:04:32.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:32.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:32.022] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:32.032] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:04:32:2313760 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:04:32:2313760 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:04:32:2313762 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:04:32:2313762 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:04:32:2313759 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:04:32:2313759 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:04:32:2313761 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:04:32:2313761 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [22.8379s] [ 84%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_single_gpu [2025-09-19 16:04:54.848] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:54.849] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:54.859] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:54.871] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [5.0102s] [ 88%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_state_dict_with_hook_on_keys [2025-09-19 16:04:59.796] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:59.818] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:59.832] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:04:59.855] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:05:00:2314764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:00:2314764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:00:2314765 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:00:2314765 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:00:2314766 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:00:2314766 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:00:2314763 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:00:2314763 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9300s] [ 92%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_strict [2025-09-19 16:05:15.718] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:15.763] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:15.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:15.796] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:05:15:2315067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:15:2315067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:15:2315068 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:15:2315068 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:15:2315066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:15:2315066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:16:2315065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:16:2315065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0314s] [ 96%]
../../../../test/distributed/checkpoint/test_state_dict.py::TestNoComm::test_no_dist [2025-09-19 16:05:31.738] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:31.815] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:31.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:31.820] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.3090s] [100%]

=================================== FAILURES ===================================
_____ TestStateDict.test_setting_meta_device_model_broadcasting_and_memory _____
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1062, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 300.06123185157776 seconds
----------------------------- Captured stdout call -----------------------------
Timing out after 300 seconds and killing subprocesses.
----------------------------- Captured stderr call -----------------------------
I0919 15:59:29.844000 2306790 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2313070
I0919 15:59:29.845000 2306790 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2313071
I0919 15:59:29.845000 2306790 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2313072
I0919 15:59:29.846000 2306790 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2313073
E0919 16:04:29.901000 2306790 site-packages/torch/testing/_internal/common_distributed.py:946] Encountered error while trying to get traceback for process 0: [Errno 32] Broken pipe
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Process 1 timed out with traceback: 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f3c8ffff640 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f3da0ca1e00 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875 in barrier
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 438 in destroy_pg
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 508 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Process 2 timed out with traceback: 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007fe584a0f640 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007fe690a85e00 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875 in barrier
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 438 in destroy_pg
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 508 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Process 3 timed out with traceback: 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Current thread 0x00007f2b8afff640 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 862 in _event_listener
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 953 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/threading.py", line 973 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] Thread 0x00007f2c983a1e00 (most recent call first):
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4875 in barrier
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 438 in destroy_pg
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 508 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755 in wrapper
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901 in run_test
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 880 in _run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 108 in run
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/process.py", line 314 in _bootstrap
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 129 in _main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/multiprocessing/spawn.py", line 116 in spawn_main
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965]   File "<string>", line 1 in <module>
E0919 16:04:29.904000 2306790 site-packages/torch/testing/_internal/common_distributed.py:965] 
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_state_dict.py.xml -
=========================== short test summary info ============================
FAILED [300.0659s] ../../../../test/distributed/checkpoint/test_state_dict.py::TestStateDict::test_setting_meta_device_model_broadcasting_and_memory - RuntimeError: Process 0 terminated or timed out after 300.06123185157776 seconds
=================== 1 failed, 24 passed in 725.24s (0:12:05) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:05:35.424] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_complicated_dict [2025-09-19 16:05:37.539] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:37.547] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:37.558] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:37.559] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:05:38:2315731 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:38:2315731 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:38:2315730 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:38:2315730 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:38:2315732 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:38:2315732 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:38:2315729 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:38:2315729 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1110s] [ 14%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_cpu_and_ranks_only [2025-09-19 16:05:53.481] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:53.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:53.500] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:05:53.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:05:54:2316031 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:54:2316031 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:54:2316030 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:54:2316030 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:54:2316033 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:54:2316033 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:05:54:2316032 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:05:54:2316032 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.6298s] [ 28%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_cpu_offload_for_dtensor [2025-09-19 16:06:09.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:09.123] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:09.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:09.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:06:09:2316332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:09:2316332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:09:2316333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:09:2316333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:10:2316331 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:10:2316331 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:10:2316330 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:10:2316330 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0302s] [ 42%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_create_cpu_state_dict SKIPPED [0.0003s] [ 57%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_gather_state_dict_dtensor [2025-09-19 16:06:25.147] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:25.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:25.166] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:25.179] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:06:25:2316633 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:25:2316633 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:25:2316630 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:25:2316630 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:26:2316631 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:26:2316631 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:26:2316632 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:26:2316632 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8300s] [ 71%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_gather_with_cpu_and_ranks_only [2025-09-19 16:06:40.985] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:40.994] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:40.995] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:40.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:06:41:2316934 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:41:2316934 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:41:2316935 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:41:2316935 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:41:2316933 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:41:2316933 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:41:2316932 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:41:2316932 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.8302s] [ 85%]
../../../../test/distributed/checkpoint/test_state_dict_utils.py::TestStateDictUtils::test_state_dict_util_distribute_tensors [2025-09-19 16:06:56.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:56.839] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:56.840] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:06:56.842] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:06:57:2317233 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:57:2317233 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:57:2317234 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:57:2317234 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:57:2317235 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:57:2317235 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:06:57:2317236 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:06:57:2317236 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.1303s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_state_dict_utils.py.xml -
=================== 6 passed, 1 skipped in 97.56s (0:01:37) ====================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:07:13.866] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/test_tp_checkpoint.py::TestTpCheckpoint::test_tp_checkpoint [2025-09-19 16:07:16.055] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:16.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:16.066] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:16.067] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:07:16:2317609 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:16:2317609 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:16:2317610 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:16:2317610 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:16:2317608 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:16:2317608 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:16:2317607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:16:2317607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmpic9ge21m
PASSED [16.5988s] [ 50%]
../../../../test/distributed/checkpoint/test_tp_checkpoint.py::TestTpCheckpoint::test_tp_checkpoint_load_on_meta_device [2025-09-19 16:07:32.490] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:32.506] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:32.514] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:32.516] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:07:33:2317926 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:33:2317926 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:33:2317927 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:33:2317927 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:33:2317924 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:33:2317924 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:33:2317925 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:33:2317925 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Using temp directory: /tmp/tmp_0c0f5er
PASSED [15.9220s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_tp_checkpoint.py.xml -
============================== 2 passed in 34.46s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:07:49.351] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_get_element PASSED [0.1631s] [ 14%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_set_element PASSED [0.0009s] [ 28%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_traverse_doesnt_ignore_intermediate_collections PASSED [0.0031s] [ 42%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_traverse_nested_dict PASSED [0.0007s] [ 57%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_traverse_nested_list PASSED [0.0008s] [ 71%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_traverse_shallow PASSED [0.0007s] [ 85%]
../../../../test/distributed/checkpoint/test_traverse.py::TestTraverse::test_traverse_with_ordered_dict PASSED [0.0006s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_traverse.py.xml -
============================== 7 passed in 2.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:07:52.402] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 16 items
Running 16 items in this shard

../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_dcp_logger PASSED [0.1910s] [  6%]
../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_flat_data PASSED [0.0040s] [ 12%]
../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_index_hint_ignored_on_equals PASSED [0.0006s] [ 18%]
../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_index_hint_ignored_on_hash PASSED [0.0005s] [ 25%]
../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_init_convert_offset PASSED [0.0005s] [ 31%]
../../../../test/distributed/checkpoint/test_utils.py::TestMedatadaIndex::test_sharded_tensor_lookup PASSED [0.0015s] [ 37%]
../../../../test/distributed/checkpoint/test_utils.py::TestReaderView::testAllRead PASSED [0.0005s] [ 43%]
../../../../test/distributed/checkpoint/test_utils.py::TestReaderView::testLongRead PASSED [0.0005s] [ 50%]
../../../../test/distributed/checkpoint/test_utils.py::TestReaderView::testLongReadinto PASSED [0.0010s] [ 56%]
../../../../test/distributed/checkpoint/test_utils.py::TestReaderView::testShortRead PASSED [0.0003s] [ 62%]
../../../../test/distributed/checkpoint/test_utils.py::TestReaderView::testShortReadinto PASSED [0.0004s] [ 68%]
../../../../test/distributed/checkpoint/test_utils.py::TestDistWrapper::test_barrier [2025-09-19 16:07:54.649] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:54.649] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:54.799] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:07:54.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:07:55:2318374 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:55:2318374 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:55:2318371 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:55:2318371 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:55:2318372 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:55:2318372 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:07:55:2318373 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:07:55:2318373 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:08:2318374:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:08:2318372:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:08:2318371:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:08:2318373:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [27.2399s] [ 75%]
../../../../test/distributed/checkpoint/test_utils.py::TestDistWrapper::test_broadcast_object_global_local_mismatch [2025-09-19 16:08:21.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:21.918] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:21.924] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:21.931] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:08:22:2318686 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:22:2318686 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:22:2318685 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:22:2318685 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:22:2318684 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:22:2318684 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:22:2318683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:22:2318683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:23:2318683:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:23:2318685:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:23:2318684:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:23:2318686:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.1262s] [ 81%]
../../../../test/distributed/checkpoint/test_utils.py::TestDistWrapper::test_broadcast_object_with_nonzero_coordinator [2025-09-19 16:08:38.002] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:38.018] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:38.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:38.033] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:08:38:2318999 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:38:2318999 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:38:2318998 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:38:2318998 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:38:2318997 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:38:2318997 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:38:2319000 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:38:2319000 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.5290s] [ 87%]
../../../../test/distributed/checkpoint/test_utils.py::TestDistWrapper::test_gather_object [2025-09-19 16:08:53.529] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:53.546] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:53.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:08:53.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:08:54:2319301 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:54:2319301 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:54:2319298 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:54:2319298 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:54:2319300 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:54:2319300 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:54:2319299 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:08:54:2319299 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:08:55:2319298:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:55:2319300:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:55:2319301:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:08:55:2319299:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [16.0306s] [ 93%]
../../../../test/distributed/checkpoint/test_utils.py::TestDistWrapper::test_scatter_object [2025-09-19 16:09:09.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:09:09.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:09:09.596] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:09:09.596] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:09:10:2319612 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:09:10:2319612 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:09:10:2319614 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:09:10:2319614 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:09:10:2319611 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:09:10:2319611 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:09:10:2319613 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:09:10:2319613 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:09:11:2319613:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:09:11:2319611:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:09:11:2319612:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:09:11:2319614:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [15.9306s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint_test_utils.py.xml -
======================== 16 passed in 93.02s (0:01:33) =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:09:26.440] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_barriers.py::TestBarriers::test_execute_barrier PASSED [0.1662s] [ 50%]
../../../../test/distributed/checkpoint/_experimental/test_barriers.py::TestBarriers::test_tcpstore_barrier_initialization PASSED [0.0010s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_barriers.py.xml -
============================== 2 passed in 2.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:09:29.482] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 4 items
Running 4 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_builder.py::TestMakeCheckpointer::test_make_async_checkpointer PASSED [1.5689s] [ 25%]
../../../../test/distributed/checkpoint/_experimental/test_builder.py::TestMakeCheckpointer::test_make_sync_checkpointer PASSED [0.0021s] [ 50%]
../../../../test/distributed/checkpoint/_experimental/test_builder.py::TestMakeCheckpointer::test_make_sync_checkpointer_with_config_first PASSED [0.0010s] [ 75%]
../../../../test/distributed/checkpoint/_experimental/test_builder.py::TestMakeCheckpointer::test_make_sync_checkpointer_with_custom_config PASSED [0.0014s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_builder.py.xml -
============================== 4 passed in 3.52s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:09:33.993] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 15 items
Running 15 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestRequestTypes::test_request_type_enum PASSED [0.1645s] [  6%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestRequestTypes::test_worker_request PASSED [0.0007s] [ 13%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestRequestTypes::test_worker_response PASSED [0.0006s] [ 20%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcessConfig::test_custom_options PASSED [0.0006s] [ 26%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcessConfig::test_default_options PASSED [0.0005s] [ 33%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_checkpoint_process_initialization [2025-09-19 16:09:36.241] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5655s] [ 40%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_checkpoint_write_future_state_dict [2025-09-19 16:09:38.740] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5573s] [ 46%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_checkpoint_write_sync_state_dict [2025-09-19 16:09:41.303] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4867s] [ 53%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_checkpoint_write_with_kwargs [2025-09-19 16:09:43.798] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4990s] [ 60%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_communication_error_handling [2025-09-19 16:09:46.292] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.0518s] [ 66%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_forced_termination [2025-09-19 16:09:48.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.1384s] [ 73%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_graceful_termination [2025-09-19 16:09:50.499] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5172s] [ 80%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_shared_memory_tensor_ipc [2025-09-19 16:09:52.998] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5139s] [ 86%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_subprocess_initialization_failure [2025-09-19 16:09:55.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.0243s] [ 93%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_process.py::TestCheckpointProcess::test_subprocess_initialization_timeout PASSED [1.0036s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_checkpoint_process.py.xml -
============================= 15 passed in 24.57s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:09:59.495] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_partial_read PASSED [0.0268s] [ 14%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_partial_read_different_dtypes PASSED [0.0057s] [ 28%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_partial_read_missing_keys PASSED [0.0035s] [ 42%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_read_checkpoint PASSED [0.0014s] [ 57%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_read_nonexistent_checkpoint PASSED [0.0009s] [ 71%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_read_with_kwargs PASSED [0.0012s] [ 85%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_reader.py::TestCheckpointReader::test_read_with_map_location PASSED [0.0020s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_checkpoint_reader.py.xml -
============================== 7 passed in 1.99s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:02.472] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriterConfig::test_custom_values PASSED [0.1660s] [ 12%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriterConfig::test_default_values PASSED [0.0006s] [ 25%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_close PASSED [0.0029s] [ 37%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_write_calls_barrier PASSED [0.0022s] [ 50%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_write_calls_commit_hooks PASSED [0.0015s] [ 62%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_write_creates_checkpoint_file PASSED [0.0019s] [ 75%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_write_without_barrier PASSED [0.0009s] [ 87%]
../../../../test/distributed/checkpoint/_experimental/test_checkpoint_writer.py::TestCheckpointWriter::test_write_without_commit_hook PASSED [0.0012s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_checkpoint_writer.py.xml -
============================== 8 passed in 2.23s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:05.556] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 11 items
Running 11 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_load_strict_mode [2025-09-19 16:10:07.649] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5764s] [  9%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_load_with_map_location [2025-09-19 16:10:10.158] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4639s] [ 18%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_nested_dict_partial_load [2025-09-19 16:10:12.634] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4837s] [ 27%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_partial_load [2025-09-19 16:10:15.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5191s] [ 36%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_save_and_load_basic [2025-09-19 16:10:17.635] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4747s] [ 45%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestCheckpointer::test_save_with_kwargs [2025-09-19 16:10:20.108] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4973s] [ 54%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestAsyncCheckpointerSpecific::test_async_error_handling PASSED [0.0016s] [ 63%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestAsyncCheckpointerSpecific::test_async_future_results [2025-09-19 16:10:22.608] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5015s] [ 72%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestAsyncCheckpointerSpecific::test_async_multiple_saves_ordering [2025-09-19 16:10:25.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4952s] [ 81%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestAsyncCheckpointerSpecific::test_async_returns_futures [2025-09-19 16:10:27.605] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4734s] [ 90%]
../../../../test/distributed/checkpoint/_experimental/test_checkpointer.py::TestAsyncCheckpointerSpecific::test_async_sequential_saves_wait [2025-09-19 16:10:30.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.5120s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_checkpointer.py.xml -
============================= 11 passed in 26.98s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:33.522] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 7 items
Running 7 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_async_staging PASSED [0.0210s] [ 14%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_cuda_non_blocking_without_cuda SKIPPED [0.0004s] [ 28%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_cuda_tensors_staging PASSED [0.0014s] [ 42%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_different_option_combinations PASSED [0.0010s] [ 57%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_multiple_staging_operations PASSED [0.0009s] [ 71%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_resource_cleanup PASSED [0.0003s] [ 85%]
../../../../test/distributed/checkpoint/_experimental/test_staging.py::TestDefaultStager::test_sync_staging PASSED [0.0008s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_staging.py.xml -
========================= 6 passed, 1 skipped in 2.07s =========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:36.459] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/checkpoint/_experimental/test_types.py::TestRankInfo::test_rank_info_default_initialization PASSED [0.1640s] [ 33%]
../../../../test/distributed/checkpoint/_experimental/test_types.py::TestRankInfo::test_rank_info_initialization PASSED [0.0006s] [ 66%]
../../../../test/distributed/checkpoint/_experimental/test_types.py::TestRankInfo::test_state_dict_type_alias PASSED [0.0007s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_checkpoint__experimental_test_types.py.xml -
============================== 3 passed in 2.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:39.492] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/elastic/events/lib_test.py::EventLibTest::test_event_created PASSED [0.1631s] [ 12%]
../../../../test/distributed/elastic/events/lib_test.py::EventLibTest::test_event_deser PASSED [0.0010s] [ 25%]
../../../../test/distributed/elastic/events/lib_test.py::EventLibTest::test_get_or_create_logger PASSED [0.0009s] [ 37%]
../../../../test/distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_construct_and_record_rdzv_event PASSED [0.0020s] [ 50%]
../../../../test/distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_construct_and_record_rdzv_event_does_not_run_if_invalid_dest PASSED [0.0008s] [ 62%]
../../../../test/distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_created PASSED [0.0008s] [ 75%]
../../../../test/distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_deserialize PASSED [0.0010s] [ 87%]
../../../../test/distributed/elastic/events/lib_test.py::RdzvEventLibTest::test_rdzv_event_str PASSED [0.0007s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_events_lib_test.py.xml -
============================== 8 passed in 2.16s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:42.580] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 3 items
Running 3 items in this shard

../../../../test/distributed/elastic/metrics/api_test.py::MetricsApiTest::test_get_metric_name PASSED [0.1849s] [ 33%]
../../../../test/distributed/elastic/metrics/api_test.py::MetricsApiTest::test_inheritance PASSED [0.0009s] [ 66%]
../../../../test/distributed/elastic/metrics/api_test.py::MetricsApiTest::test_profile PASSED [0.0010s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_metrics_api_test.py.xml -
============================== 3 passed in 2.18s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:10:45.679] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 25 items
Running 25 items in this shard

../../../../test/distributed/elastic/multiprocessing/api_test.py::RunProcResultsTest::test_get_failures PASSED [0.1866s] [  4%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::RunProcResultsTest::test_is_failed PASSED [0.0007s] [  8%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StdTest::test_from_str_bad_input PASSED [0.0007s] [ 12%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StdTest::test_from_value PASSED [0.0007s] [ 16%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StdTest::test_from_value_map PASSED [0.0006s] [ 20%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_args_env_len_mismatch PASSED [0.0007s] [ 24%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_function_large_ret_val [2025-09-19 16:10:47.840] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:10:47.841] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:10:47.848] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:10:47.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1464s] [ 28%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_function_raise [2025-09-19 16:10:50.965] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:10:50.978] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.7357s] [ 32%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_function_with_tensor [2025-09-19 16:10:53.713] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.4518s] [ 36%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_invalid_log_dir PASSED [0.0011s] [ 40%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_multiprocess_context_close PASSED [0.0025s] [ 44%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_multiprocessing_context_poll_raises_exception PASSED [0.0030s] [ 48%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_pcontext_wait PASSED [2.4848s] [ 52%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_pcontext_wait_on_a_child_thread PASSED [2.5910s] [ 56%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_to_map PASSED [0.0012s] [ 60%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_void_function [2025-09-19 16:11:02.245] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:11:02.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
hello
world
PASSED [2.7042s] [ 64%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsFuncTest::test_wait_for_all_child_procs_to_exit PASSED [0.0034s] [ 68%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsBinaryTest::test_binary_exit PASSED [0.1048s] [ 72%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsBinaryTest::test_binary_incorrect_entrypoint PASSED [0.0019s] [ 76%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsBinaryTest::test_binary_raises bar from 1
PASSED [0.1026s] [ 80%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsBinaryTest::test_subprocess_context_close PASSED [0.0026s] [ 84%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesAsBinaryTest::test_validate_full_rank PASSED [0.0006s] [ 88%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesListAsFuncTest::test_function [2025-09-19 16:11:05.138] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:11:05.144] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
hello stdout from 0
hello stdout from 1
PASSED [2.7480s] [ 92%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesListAsBinaryTest::test_binary hello stdout from 0
hello stdout from 1
PASSED [1000.1405s] [ 96%]
../../../../test/distributed/elastic/multiprocessing/api_test.py::StartProcessesListAsBinaryTest::test_binary_redirect_and_tee world stdout from 1
PASSED [1001.0835s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_multiprocessing_api_test.py.xml -
======================= 25 passed in 2023.52s (0:33:43) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:44:30.037] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 9 items / 1 deselected / 8 selected
Running 8 items in this shard

../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_dump_nccl_trace_pickle SKIPPED [0.0002s] [ 12%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_dump_nccl_trace_pickle_with_json SKIPPED [0.0002s] [ 25%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_dump_nccl_trace_pickle_with_params SKIPPED [0.0001s] [ 37%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_dump_traceback PASSED [0.1839s] [ 50%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_get_handler_names PASSED [0.0007s] [ 62%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_get_handler_nonexistant PASSED [0.0009s] [ 75%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_run_handler PASSED [0.0009s] [ 87%]
../../../../test/distributed/elastic/test_control_plane.py::WorkerServerTest::test_worker_server PASSED [0.0191s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_test_control_plane.py.xml -
================== 5 passed, 3 skipped, 1 deselected in 2.14s ==================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... collected 1 item
Running 1 items in this shard

../../../../test/distributed/elastic/timer/api_test.py::TimerApiTest::test_run_watchdog PASSED [0.0017s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_timer_api_test.py.xml -
============================== 1 passed in 1.18s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:44:34.999] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/elastic/timer/local_timer_example.py::LocalTimerExample::test_example_start_method_spawn [2025-09-19 16:44:37.163] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.163] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.180] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.180] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.200] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:37.205] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.6333s] [ 50%]
../../../../test/distributed/elastic/timer/local_timer_example.py::LocalTimerExample::test_torch_mp_example [2025-09-19 16:44:40.639] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.644] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.646] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.652] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.653] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.654] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.665] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:40.666] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.176] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.179] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.197] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.205] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.212] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:44:44.213] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [6.7722s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_timer_local_timer_example.py.xml -
============================== 2 passed in 12.39s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:44:48.263] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 14 items
Running 14 items in this shard

../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_client_interaction PASSED [0.1938s] [  7%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_exception_propagation PASSED [0.0109s] [ 14%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_get_timer_recursive [2025-09-19 16:44:50.689] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.4280s] [ 21%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_happy_path PASSED [0.1029s] [ 28%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_no_client PASSED [0.0109s] [ 35%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerTest::test_timer PASSED [0.1482s] [ 42%]
../../../../test/distributed/elastic/timer/local_timer_test.py::MultiprocessingRequestQueueTest::test_get PASSED [0.0215s] [ 50%]
../../../../test/distributed/elastic/timer/local_timer_test.py::MultiprocessingRequestQueueTest::test_get_less_than_size PASSED [0.5076s] [ 57%]
../../../../test/distributed/elastic/timer/local_timer_test.py::MultiprocessingRequestQueueTest::test_get_size PASSED [0.9091s] [ 64%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerServerTest::test_acquire_release PASSED [0.0022s] [ 71%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerServerTest::test_expired_timers PASSED [0.0016s] [ 78%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerServerTest::test_valid_timers PASSED [0.0015s] [ 85%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerServerTest::test_watchdog_call_count PASSED [0.1024s] [ 92%]
../../../../test/distributed/elastic/timer/local_timer_test.py::LocalTimerServerTest::test_watchdog_empty_queue PASSED [0.0108s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_timer_local_timer_test.py.xml -
============================== 14 passed in 6.36s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:44:55.743] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_multi PASSED [0.2097s] [ 12%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_no_port_multi PASSED [0.0010s] [ 25%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_single_server PASSED [0.0017s] [ 37%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_timeout_on_server PASSED [2.0045s] [ 50%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_timeout_on_worker PASSED [2.6803s] [ 62%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_create_store_with_libuv_support PASSED [0.0015s] [ 75%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_port_already_in_use_on_server PASSED [0.0013s] [ 87%]
../../../../test/distributed/elastic/utils/distributed_test.py::DistributedUtilTest::test_port_already_in_use_on_worker PASSED [2.3031s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_utils_distributed_test.py.xml -
============================== 8 passed in 9.20s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:45:05.945] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 2 items
Running 2 items in this shard

../../../../test/distributed/elastic/utils/logging_test.py::LoggingTest::test_derive_module_name PASSED [0.1655s] [ 50%]
../../../../test/distributed/elastic/utils/logging_test.py::LoggingTest::test_logger_name PASSED [0.0018s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_utils_logging_test.py.xml -
============================== 2 passed in 2.17s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:45:08.949] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 12 items
Running 12 items in this shard

../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_barrier PASSED [0.1679s] [  8%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_barrier_hash_store PASSED [0.0045s] [ 16%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_barrier_timeout_operations PASSED [0.0010s] [ 25%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_barrier_timeout_rank_tracing PASSED [0.1126s] [ 33%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_get_all_rank_0 PASSED [0.0005s] [ 41%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_get_all_rank_n PASSED [0.0004s] [ 50%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_synchronize PASSED [0.0005s] [ 58%]
../../../../test/distributed/elastic/utils/util_test.py::StoreUtilTest::test_synchronize_hash_store PASSED [0.0021s] [ 66%]
../../../../test/distributed/elastic/utils/util_test.py::UtilTest::test_get_logger PASSED [0.0014s] [ 75%]
../../../../test/distributed/elastic/utils/util_test.py::UtilTest::test_get_logger_custom_name PASSED [0.0006s] [ 83%]
../../../../test/distributed/elastic/utils/util_test.py::UtilTest::test_get_logger_different PASSED [0.0007s] [ 91%]
../../../../test/distributed/elastic/utils/util_test.py::UtilTest::test_get_logger_none PASSED [0.0010s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_elastic_utils_util_test.py.xml -
============================== 12 passed in 2.22s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... collected 5 items
Running 5 items in this shard

../../../../test/distributed/optim/test_apply_optimizer_in_backward.py::ApplyOverlappedOptimizerTest::test_apply_optimizer_in_backward [2025-09-19 16:45:12.776] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [0.6642s] [ 20%]
../../../../test/distributed/optim/test_apply_optimizer_in_backward.py::ApplyOverlappedOptimizerTest::test_apply_optimizer_in_backward_shared_params PASSED [0.0057s] [ 40%]
../../../../test/distributed/optim/test_apply_optimizer_in_backward.py::ApplyOverlappedOptimizerTest::test_get_optimizers_in_backward PASSED [0.0004s] [ 60%]
../../../../test/distributed/optim/test_apply_optimizer_in_backward.py::ApplyOverlappedOptimizerTest::test_multiple_optim_for_params PASSED [0.0064s] [ 80%]
../../../../test/distributed/optim/test_apply_optimizer_in_backward.py::ApplyOverlappedOptimizerTest::test_no_register_hook PASSED [0.0026s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_optim_test_apply_optimizer_in_backward.py.xml -
============================== 5 passed in 2.06s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... collected 8 items
Running 8 items in this shard

../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_add_param_group [2025-09-19 16:45:15.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [0.6300s] [ 12%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_add_param_group_error PASSED [0.0015s] [ 25%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_init_state PASSED [0.0011s] [ 37%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_load_state_dict PASSED [0.0026s] [ 50%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_load_state_dict_conditional_training PASSED [0.0024s] [ 62%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_load_state_dict_error PASSED [0.0017s] [ 75%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_state_dict PASSED [0.0033s] [ 87%]
../../../../test/distributed/optim/test_named_optimizer.py::NamedOptimizerTest::test_state_dict_multi_param_group PASSED [0.0044s] [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_optim_test_named_optimizer.py.xml -
============================== 8 passed in 1.94s ===============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 16:45:18.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 42 items
Running 42 items in this shard

../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_constructor [2025-09-19 16:45:20.886] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8048s] [  2%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_lr_scheduler [2025-09-19 16:45:23.614] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:23:2326432 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:23:2326432 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.3069s] [  4%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_same_dense_param_type [2025-09-19 16:45:26.922] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.7060s] [  7%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_state_dict [2025-09-19 16:45:29.638] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:29:2326582 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:29:2326582 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.4069s] [  9%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_step_with_extra_inner_key [2025-09-19 16:45:33.050] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:33:2326660 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:33:2326660 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.3069s] [ 11%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_step_with_kwargs [2025-09-19 16:45:36.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:36:2326739 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:36:2326739 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.3083s] [ 14%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_step_without_closure [2025-09-19 16:45:39.670] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:39:2326817 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:39:2326817 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [3.4070s] [ 16%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerSingleRank::test_zero_grad [2025-09-19 16:45:43.078] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [2.8061s] [ 19%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_add_param_group [2025-09-19 16:45:45.914] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:45.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:45.934] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:45.951] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1089s] [ 21%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_collect_shards [2025-09-19 16:45:49.016] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:49.020] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:49.038] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:49.042] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:49:2327262 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:49:2327262 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:49:2327259 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:49:2327259 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:49:2327261 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:49:2327261 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:49:2327260 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:49:2327260 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.3115s] [ 23%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_False_static_graph_False_shard_buckets_False [2025-09-19 16:45:53.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:53.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:53.362] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:45:53.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:45:53:2327578 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:53:2327578 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:53:2327575 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:53:2327575 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:53:2327577 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:53:2327577 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:45:53:2327576 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:45:53:2327576 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.7803s] [ 26%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_False_static_graph_False_shard_buckets_True [2025-09-19 16:46:34.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:46:34.130] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:46:34.199] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:46:34.239] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:46:34:2328276 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:46:34:2328276 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:46:34:2328273 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:46:34:2328273 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:46:34:2328274 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:46:34:2328274 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:46:34:2328275 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:46:34:2328275 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.7740s] [ 28%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_False_static_graph_True_shard_buckets_False [2025-09-19 16:47:14.913] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:14.924] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:14.930] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:14.938] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:47:15:2328972 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:15:2328972 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:15:2328973 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:15:2328973 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:15:2328971 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:15:2328971 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:15:2328970 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:15:2328970 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.1792s] [ 30%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_False_static_graph_True_shard_buckets_True [2025-09-19 16:47:56.118] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:56.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:56.235] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:47:56.244] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:47:56:2329667 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:56:2329667 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:56:2329669 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:56:2329669 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:56:2329668 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:56:2329668 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:47:56:2329670 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:47:56:2329670 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.4765s] [ 33%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_True_static_graph_False_shard_buckets_False [2025-09-19 16:48:37.564] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:48:37.565] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:48:37.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:48:37.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:48:38:2330366 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:48:38:2330366 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:48:38:2330367 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:48:38:2330367 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:48:38:2330365 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:48:38:2330365 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:48:38:2330368 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:48:38:2330368 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.9742s] [ 35%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_True_static_graph_False_shard_buckets_True [2025-09-19 16:49:18.540] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:18.554] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:18.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:18.609] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:49:19:2331068 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:19:2331068 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:49:19:2331067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:19:2331067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:49:19:2331066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:19:2331066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:49:19:2331065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:19:2331065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.8805s] [ 38%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_True_static_graph_True_shard_buckets_False [2025-09-19 16:49:59.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:59.408] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:59.430] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:49:59.431] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:49:59:2331762 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:59:2331762 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:49:59:2331763 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:49:59:2331763 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:50:00:2331764 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:00:2331764 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:50:00:2331761 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:00:2331761 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.1743s] [ 40%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_False_gradient_as_bucket_view_True_static_graph_True_shard_buckets_True [2025-09-19 16:50:40.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:50:40.588] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:50:40.603] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:50:40.610] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:50:41:2332461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:41:2332461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:50:41:2332462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:41:2332462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:50:41:2332459 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:41:2332459 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:50:41:2332460 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:50:41:2332460 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.7746s] [ 42%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_False_static_graph_False_shard_buckets_False [2025-09-19 16:51:21.406] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:51:21.411] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:51:21.412] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:51:21.435] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:51:21:2333161 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:51:21:2333161 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:51:21:2333160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:51:21:2333160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:51:22:2333158 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:51:22:2333158 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:51:22:2333159 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:51:22:2333159 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.8753s] [ 45%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_False_static_graph_False_shard_buckets_True [2025-09-19 16:52:02.234] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:02.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:02.245] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:02.264] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:52:02:2333857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:02:2333857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:02:2333860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:02:2333860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:02:2333859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:02:2333859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:02:2333858 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:02:2333858 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.8717s] [ 47%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_False_static_graph_True_shard_buckets_False [2025-09-19 16:52:43.094] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:43.102] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:43.106] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:52:43.141] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:52:43:2334559 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:43:2334559 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:43:2334558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:43:2334558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:43:2334560 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:43:2334560 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:52:43:2334557 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:52:43:2334557 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.0712s] [ 50%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_False_static_graph_True_shard_buckets_True [2025-09-19 16:53:24.200] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:53:24.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:53:24.220] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:53:24.227] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:53:24:2335255 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:53:24:2335255 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:53:24:2335256 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:53:24:2335256 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:53:24:2335257 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:53:24:2335257 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:53:24:2335258 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:53:24:2335258 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.4853s] [ 52%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_True_static_graph_False_shard_buckets_False [2025-09-19 16:54:05.674] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:05.713] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:05.727] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:05.730] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:54:06:2335955 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:06:2335955 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:06:2335957 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:06:2335957 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:06:2335954 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:06:2335954 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:06:2335956 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:06:2335956 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.8711s] [ 54%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_True_static_graph_False_shard_buckets_True [2025-09-19 16:54:46.584] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:46.586] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:46.659] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:54:46.695] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:54:47:2336655 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:47:2336655 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:47:2336654 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:47:2336654 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:47:2336652 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:47:2336652 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:54:47:2336653 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:54:47:2336653 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [41.2727s] [ 57%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_True_static_graph_True_shard_buckets_False [2025-09-19 16:55:27.844] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:55:27.854] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:55:27.858] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:55:27.870] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:55:28:2337352 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:55:28:2337352 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:55:28:2337354 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:55:28:2337354 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:55:28:2337353 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:55:28:2337353 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:55:28:2337351 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:55:28:2337351 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.8730s] [ 59%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_ddp_zero_overlap_use_gpu_True_use_interleaved_hook_True_gradient_as_bucket_view_True_static_graph_True_shard_buckets_True [2025-09-19 16:56:08.690] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:08.706] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:08.709] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:08.717] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:56:09:2338053 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:09:2338053 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:09:2338050 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:09:2338050 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:09:2338051 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:09:2338051 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:09:2338052 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:09:2338052 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [40.7757s] [ 61%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_AdamW_maximize_False [2025-09-19 16:56:49.474] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:49.478] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:49.606] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:56:49.615] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:56:49:2338748 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:49:2338748 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:49:2338749 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:49:2338749 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:49:2338750 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:49:2338750 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:56:49:2338747 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:56:49:2338747 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.8335s] [ 64%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_AdamW_maximize_True [2025-09-19 16:57:06.307] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:06.325] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:06.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:06.350] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:57:06:2339065 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:06:2339065 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:06:2339067 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:06:2339067 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:06:2339066 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:06:2339066 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:06:2339064 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:06:2339064 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.8335s] [ 66%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_Adam_maximize_False [2025-09-19 16:57:23.111] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:23.116] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:23.120] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:23.126] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:57:23:2339381 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:23:2339381 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:23:2339380 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:23:2339380 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:23:2339383 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:23:2339383 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:23:2339382 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:23:2339382 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.7352s] [ 69%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_Adam_maximize_True [2025-09-19 16:57:39.996] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:40.006] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:40.021] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:40.031] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:57:40:2339698 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:40:2339698 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:40:2339699 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:40:2339699 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:40:2339701 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:40:2339701 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:40:2339700 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:40:2339700 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.7428s] [ 71%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_SGD_maximize_False [2025-09-19 16:57:56.608] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:56.624] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:56.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:57:56.630] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:57:56:2340017 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:56:2340017 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:56:2340015 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:56:2340015 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:57:2340014 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:57:2340014 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:57:57:2340016 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:57:57:2340016 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.6335s] [ 73%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_local_optimizer_parity_optimizer_class_str_SGD_maximize_True [2025-09-19 16:58:13.256] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:13.256] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:13.278] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:13.283] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:58:13:2340330 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:13:2340330 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:13:2340332 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:13:2340332 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:13:2340331 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:13:2340331 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:13:2340333 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:13:2340333 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.7338s] [ 76%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_lr_scheduler [2025-09-19 16:58:29.996] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:30.004] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:30.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:30.010] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:58:30:2340647 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:30:2340647 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:30:2340650 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:30:2340650 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:30:2340648 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:30:2340648 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:30:2340649 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:30:2340649 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.1127s] [ 78%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_multiple_param_groups [2025-09-19 16:58:34.156] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:34.164] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:34.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:34.178] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:58:34:2340965 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:34:2340965 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:34:2340968 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:34:2340968 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:34:2340967 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:34:2340967 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:34:2340966 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:34:2340966 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [4.6135s] [ 80%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_nondefault_process_group [2025-09-19 16:58:38.705] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:38.722] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:38.810] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:38.977] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:58:39:2341281 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:39:2341281 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:39:2341283 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:39:2341283 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.4648s] [ 83%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_sharding [2025-09-19 16:58:54.179] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:54.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:54.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:54.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [3.1113s] [ 85%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_step [2025-09-19 16:58:57.342] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:57.346] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:57.363] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:58:57.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:58:57:2341877 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:57:2341877 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:57:2341874 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:57:2341874 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:57:2341876 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:57:2341876 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:58:57:2341875 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:58:57:2341875 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [16.0327s] [ 88%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_step_with_closure [2025-09-19 16:59:13.304] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:13.305] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:13.308] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:13.322] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:59:13:2342191 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:13:2342191 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:13:2342190 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:13:2342190 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:13:2342193 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:13:2342193 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:13:2342192 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:13:2342192 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [15.9323s] [ 90%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_zero_join_cpu [2025-09-19 16:59:29.285] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:29.294] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:29.310] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:29.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
PASSED [3.3121s] [ 92%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_zero_join_gpu [2025-09-19 16:59:32.571] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:32.590] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:32.611] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:32.624] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:59:32:2342820 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:32:2342820 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:32:2342822 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:32:2342822 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:32:2342819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:32:2342819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:33:2342821 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:33:2342821 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
FAILED [16.4337s] [ 95%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_zero_model_parallel_parameters_as_bucket_view_False [2025-09-19 16:59:49.026] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:49.039] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:49.167] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 16:59:49.183] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-16:59:49:2343516 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:49:2343516 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:49:2343515 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-16:59:49:2343515 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-16:59:50:2343515:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-16:59:50:2343516:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [27.4485s] [ 97%]
../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_zero_model_parallel_parameters_as_bucket_view_True [2025-09-19 17:00:16.439] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:16.444] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:16.458] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:16.461] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:00:16:2343819 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:16:2343819 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:00:16:2343818 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:16:2343818 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:00:17:2343818:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:00:17:2343819:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [27.4493s] [100%]

=================================== FAILURES ===================================
__________ TestZeroRedundancyOptimizerDistributed.test_zero_join_gpu ___________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 753, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1017, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1057, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 140, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 1083, in test_zero_join_gpu
    self._test_zero_join(self.device)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 1070, in _test_zero_join
    torch.testing.assert_close(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Parameters differ between using ZeRO and local optimizer

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/optim/test_zero_redundancy_optimizer.py TestZeroRedundancyOptimizerDistributed.test_zero_join_gpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0


----------------------------- Captured stdout call -----------------------------
Process 2 terminated with exit code 10, terminating remaining processes.
----------------------------- Captured stderr call -----------------------------
I0919 16:59:30.548000 2326288 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 0 with pid 2342819
I0919 16:59:30.549000 2326288 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 1 with pid 2342820
I0919 16:59:30.550000 2326288 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 2 with pid 2342821
I0919 16:59:30.551000 2326288 site-packages/torch/testing/_internal/common_distributed.py:826] Started process 3 with pid 2342822
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_optim_test_zero_redundancy_optimizer.py.xml -
=========================== short test summary info ============================
FAILED [16.4337s] ../../../../test/distributed/optim/test_zero_redundancy_optimizer.py::TestZeroRedundancyOptimizerDistributed::test_zero_join_gpu - RuntimeError: Process 2 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 901, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 755, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 140, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 1083, in test_zero_join_gpu
    self._test_zero_join(self.device)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/optim/test_zero_redundancy_optimizer.py", line 1070, in _test_zero_join
    torch.testing.assert_close(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_comparison.py", line 1589, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Parameters differ between using ZeRO and local optimizer

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/optim/test_zero_redundancy_optimizer.py TestZeroRedundancyOptimizerDistributed.test_zero_join_gpu

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
=================== 1 failed, 41 passed in 925.29s (0:15:25) ===================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 17:00:44.255] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 28 items

distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_all_to_all [2025-09-19 17:00:46.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:46.206] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:46.226] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:00:46.229] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:00:46:2344197 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:46:2344197 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:00:46:2344200 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:46:2344200 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:00:47:2344198 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:47:2344198 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:00:47:2344199 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:00:47:2344199 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [  3%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_all_to_all_single PASSED [  7%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_all_to_all_single_none PASSED [ 10%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_all_to_all_single_unequal_split PASSED [ 14%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_allgather_base_basics PASSED [ 17%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_allgather_base_ops PASSED [ 21%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_allgather_ops PASSED [ 25%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_allreduce_ops_complex64 PASSED [ 28%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_allreduce_ops_float32 PASSED [ 32%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_alltoall_ops_with_xpufree_race PASSED [ 35%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_barrier PASSED [ 39%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_batch_isend_irecv PASSED [ 42%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_broadcast_ops_complex64 PASSED [ 46%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_broadcast_ops_float32 PASSED [ 50%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_empty_tensors PASSED [ 53%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_gather_checks PASSED [ 57%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_gather_ops PASSED [ 60%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_gather_stress PASSED [ 64%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_reduce_ops PASSED [ 67%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_reduce_scatter_base_basics PASSED [ 71%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_reduce_scatter_base_ops PASSED [ 75%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_reduce_scatter_ops PASSED [ 78%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_scatter_checks PASSED [ 82%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_scatter_ops PASSED [ 85%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_scatter_stress PASSED [ 89%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_send_recv 2025:09:19-17:01:18:2344198:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:01:18:2344197:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [ 92%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_send_recv_complex PASSED [ 96%]
distributed/test_c10d_ops_xccl.py::ProcessGroupXCCLOpTest::test_send_recv_object_list PASSED [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_distributed_test_c10d_ops_xccl.py.xml -
============================= 28 passed in 36.94s ==============================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 17:01:22.137] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 22 items

distributed/test_c10d_xccl.py::RendezvousEnvTest::test_common_errors PASSED [  4%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_close_multi_pg_unordered [2025-09-19 17:01:24.366] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:24.370] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:25:2344606 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:25:2344606 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:25:2344607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:25:2344607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:37:2344607:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:01:37:2344606:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:01:37:2344606:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:01:37:2344607:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED [  9%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_file_store_check [2025-09-19 17:01:39.850] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:39.874] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [ 13%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_bfloat16 [2025-09-19 17:01:44.566] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:44.575] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:45:2344910 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:45:2344910 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:45:2344909 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:45:2344909 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 18%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_float16 [2025-09-19 17:01:47.484] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:47.493] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:48:2345060 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:48:2345060 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:48:2345059 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:48:2345059 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 22%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_float32 [2025-09-19 17:01:50.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:50.330] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:51:2345209 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:51:2345209 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:51:2345210 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:51:2345210 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 27%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_float64 [2025-09-19 17:01:53.307] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:53.332] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:54:2345360 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:54:2345360 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:54:2345359 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:54:2345359 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 31%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_float8_e4m3fn [2025-09-19 17:01:56.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:56.142] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:01:57:2345510 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:57:2345510 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:01:57:2345509 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:01:57:2345509 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 36%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_nan_assert_float8_e5m2 [2025-09-19 17:01:59.816] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:01:59.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:00:2345660 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:00:2345660 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:00:2345659 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:00:2345659 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 40%]
distributed/test_c10d_xccl.py::ProcessGroupXCCLTest::test_set_process_group_desc [2025-09-19 17:02:03.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:03.434] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
PASSED [ 45%]
distributed/test_c10d_xccl.py::CommTest::test_all_gather_into_tensor [2025-09-19 17:02:06.152] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:06.162] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:06:2345958 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:06:2345958 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:07:2345957 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:07:2345957 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:07:2345958:[1] |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
2025:09:19-17:02:07:2345957:[0] |CCL_WARN| topology recognition shows PCIe connection between devices. If this is not correct, you can disable topology recognition, with CCL_TOPO_FABRIC_VERTEX_CONNECTION_CHECK=0. This will assume XeLinks across devices
PASSED [ 50%]
distributed/test_c10d_xccl.py::CommTest::test_all_reduce_coalesced_manager_xccl [2025-09-19 17:02:09.631] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:09.633] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:10:2346106 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:10:2346106 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:10:2346105 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:10:2346105 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 54%]
distributed/test_c10d_xccl.py::CommTest::test_all_reduce_coalesced_xccl [2025-09-19 17:02:24.676] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:24.686] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:25:2346256 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:25:2346256 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:25:2346257 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:25:2346257 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 59%]
distributed/test_c10d_xccl.py::CommTest::test_broadcast_coalesced_xccl [2025-09-19 17:02:39.510] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:39.526] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:40:2346408 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:40:2346408 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:40:2346409 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:40:2346409 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 63%]
distributed/test_c10d_xccl.py::CommTest::test_reduce_scatter_base_k [2025-09-19 17:02:43.833] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:02:43.838] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:02:44:2346559 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:44:2346559 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:02:44:2346558 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:02:44:2346558 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 68%]
distributed/test_c10d_xccl.py::CommTest::test_reduce_scatter_tensor_coalesced [2025-09-19 17:03:02.356] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:03:02.358] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:03:03:2346708 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:03:2346708 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:03:03:2346709 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:03:2346709 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 72%]
distributed/test_c10d_xccl.py::CommTest::test_single_p2p [2025-09-19 17:03:20.949] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:03:20.963] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:03:21:2346860 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:21:2346860 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:03:21:2346859 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:21:2346859 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED          [ 77%]
distributed/test_c10d_xccl.py::CommTest::test_tensor_dtype_complex [2025-09-19 17:03:24.376] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:03:24.392] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:03:25:2347010 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:25:2347010 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:03:25:2347009 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:25:2347009 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [ 81%]
distributed/test_c10d_xccl.py::CommTest::test_unwaited [2025-09-19 17:03:27.778] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:03:27.795] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:03:28:2347159 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:28:2347159 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:03:28:2347160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:28:2347160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED            [ 86%]
distributed/test_c10d_xccl.py::CommTest::test_wait_tensor [2025-09-19 17:03:49.625] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:03:49.643] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:03:50:2347310 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:50:2347310 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:03:50:2347311 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:03:50:2347311 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED         [ 90%]
distributed/test_c10d_xccl.py::CommTest::test_xccl_barrier [2025-09-19 17:04:04.472] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:04:04.486] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:04:05:2347462 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:05:2347462 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:04:05:2347461 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:05:2347461 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:04:17:2347461:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:04:17:2347462:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:04:17:2347462:[1] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
2025:09:19-17:04:17:2347461:[0] |CCL_WARN| Warning: CCL_OFI_ENABLE_HOSTNAME_SHARING is enabled; this feature is deprecated and might contain security vulnerabilities.
PASSED        [ 95%]
distributed/test_c10d_xccl.py::CommTest::test_xccl_barrier_device_ids [2025-09-19 17:04:19.979] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:04:19.991] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:04:20:2347621 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:20:2347620 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:20:2347621 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:04:20:2347620 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [100%]

- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_distributed_test_c10d_xccl.py.xml -
======================== 22 passed in 192.72s (0:03:12) ========================
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 17:04:36.347] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 32 items
Running 32 items in this shard

../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_coalesced [2025-09-19 17:04:47.592] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:04:47.594] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:04:57:2348231 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:57:2348231 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:04:58:2348232 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:04:58:2348232 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.7260s] [  3%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_single [2025-09-19 17:05:02.210] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:05:02.226] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:05:12:2349155 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:12:2349155 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:05:13:2349156 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:13:2349156 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.9263s] [  6%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced [2025-09-19 17:05:17.182] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:05:17.186] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:05:26:2350081 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:26:2350081 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:05:28:2350080 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:28:2350080 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [25.9414s] [  9%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced_ [2025-09-19 17:05:43.105] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:05:43.122] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:05:52:2351006 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:52:2351006 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:05:54:2351005 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:05:54:2351005 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [26.0426s] [ 12%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_single [2025-09-19 17:06:09.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:06:09.254] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:06:19:2351931 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:06:19:2351931 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:06:20:2351932 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:06:20:2351932 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [26.2420s] [ 15%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_single_ [2025-09-19 17:06:35.485] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:06:35.494] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:06:45:2352857 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:06:45:2352857 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:06:46:2352856 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:06:46:2352856 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [25.8402s] [ 18%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_all_to_all_single [2025-09-19 17:07:01.239] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:07:01.250] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:07:10:2353782 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:10:2353782 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:07:12:2353783 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:12:2353783 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.3233s] [ 21%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_broadcast [2025-09-19 17:07:15.555] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:07:15.562] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:07:25:2354705 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:25:2354705 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:07:26:2354706 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:26:2354706 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.7240s] [ 25%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_fixed_striding SKIPPED [0.0003s]) [ 28%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_functional_collectives_inference_mode [2025-09-19 17:07:30.298] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:07:30.326] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:07:39:2355629 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:39:2355629 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:07:41:2355630 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:41:2355630 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [14.5245s] [ 31%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_inductor_dtypeview_memory_leak [2025-09-19 17:07:44.823] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:07:44.826] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:07:55:2356556 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:55:2356556 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:07:57:2356555 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:07:57:2356555 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [17.5285s] [ 34%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_reduce_scatter_tensor_coalesced [2025-09-19 17:08:02.394] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:08:02.410] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:08:12:2357758 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:08:12:2357758 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:08:13:2357757 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:08:13:2357757 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [29.8477s] [ 37%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_reduce_scatter_tensor_single [2025-09-19 17:08:32.202] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:08:32.204] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:08:41:2358682 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:08:41:2358682 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:08:43:2358683 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:08:43:2358683 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [29.5482s] [ 40%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_threading [2025-09-19 17:09:01.746] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:09:01.747] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:09:11:2359608 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:09:11:2359608 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:09:12:2359607 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:09:12:2359607 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [32.4537s] [ 43%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_unwaited [2025-09-19 17:09:34.258] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:09:34.266] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:09:43:2360880 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:09:43:2360880 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:09:45:2360879 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:09:45:2360879 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [25.9420s] [ 46%]
../../../../test/distributed/test_c10d_functional_native.py::TestWithNCCL::test_wait_tensor [2025-09-19 17:10:00.140] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:10:00.150] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:10:09:2361804 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:10:09:2361804 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:10:11:2361805 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:10:11:2361805 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
PASSED [25.8405s] [ 50%]
../../../../test/distributed/test_c10d_functional_native.py::PyWorkTest::test_collectives [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
PASSED [0.3511s] [ 53%]
../../../../test/distributed/test_c10d_functional_native.py::PyWorkTest::test_wait_tensor PASSED [0.0758s] [ 56%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTestCPU::test_inductor_all_reduce_cpu PASSED [18.0598s] [ 59%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_gather_into_tensor_coalesced FAILED [0.1839s] [ 62%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_gather_into_tensor_single PASSED [8.4812s] [ 65%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_reduce_coalesced PASSED [6.5453s] [ 68%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_reduce_non_contig_input PASSED [4.4438s] [ 71%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_reduce_single PASSED [12.3547s] [ 75%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_to_all_single PASSED [0.7483s] [ 78%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_broadcast PASSED [6.6395s] [ 81%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_inplace_op_on_view PASSED [2.8739s] [ 84%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_reduce_scatter_tensor_coalesced PASSED [3.6841s] [ 87%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_reduce_scatter_tensor_single PASSED [3.4722s] [ 90%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_reuse_buffer_after_inplace_collective PASSED [2.8982s] [ 93%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_ranks_and_tag PASSED [2.8704s] [ 96%]
../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_wait_tensor PASSED [6.4341s] [100%]

=================================== FAILURES ===================================
__________ CompileTest.test_inductor_all_gather_into_tensor_coalesced __________
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_c10d_functional_native.py", line 1031, in test_inductor_all_gather_into_tensor_coalesced
    .run(code)
RuntimeError: Expected to find "buf0 = torch.ops._c10d_functional.all_gather_into_tensor_coalesced.default([arg3_1, arg2_1, arg1_1, arg0_1]" but did not find it
Searched string:
# AOT ID: ['4_inference']
~~~~~~~~~~~~~~~~~~~~~~~~~
from ctypes import c_void_p, c_long, c_int
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import torch
~~~~~~~~~~~~
import math
~~~~~~~~~~~
import random
~~~~~~~~~~~~~ <--- HERE
import os
import tempfile
From CHECK: buf0 = torch.ops._c10d_functional.all_gather_into_tensor_coalesced.default([arg3_1, arg2_1, arg1_1, arg0_1]


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_c10d_functional_native.py CompileTest.test_inductor_all_gather_into_tensor_coalesced

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
----------------------------- Captured stderr call -----------------------------
[rank0]:W0919 17:10:42.814000 2347771 site-packages/torch/_inductor/utils.py:1244] on error, temporary cache dir kept at /tmp/tmp2yt7zowp
- generated xml file: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/op_ut_with_skip_test_distributed_test_c10d_functional_native.py.xml -
=========================== short test summary info ============================
FAILED [0.1839s] ../../../../test/distributed/test_c10d_functional_native.py::CompileTest::test_inductor_all_gather_into_tensor_coalesced - RuntimeError: Expected to find "buf0 = torch.ops._c10d_functional.all_gather_into_tensor_coalesced.default([arg3_1, arg2_1, arg1_1, arg0_1]" but did not find it
Searched string:
# AOT ID: ['4_inference']
~~~~~~~~~~~~~~~~~~~~~~~~~
from ctypes import c_void_p, c_long, c_int
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
import torch
~~~~~~~~~~~~
import math
~~~~~~~~~~~
import random
~~~~~~~~~~~~~ <--- HERE
import os
import tempfile
From CHECK: buf0 = torch.ops._c10d_functional.all_gather_into_tensor_coalesced.default([arg3_1, arg2_1, arg1_1, arg0_1]


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_c10d_functional_native.py CompileTest.test_inductor_all_gather_into_tensor_coalesced

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
============= 1 failed, 30 passed, 1 skipped in 429.79s (0:07:09) ==============
============================= test session starts ==============================
platform linux -- Python 3.10.18, pytest-8.4.2, pluggy-1.6.0 -- /tmp/xpu-tool/Python/3.10.18/x64/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/third_party/torch-xpu-ops/test/xpu/.hypothesis/examples')
rootdir: /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch
configfile: pytest.ini
plugins: cpp-2.3.0, xdist-3.8.0, timeout-2.4.0, xdoctest-1.1.0, rerunfailures-14.0, flakefinder-1.1.0, subtests-0.13.1, hypothesis-5.35.1
collecting ... [2025-09-19 17:11:48.185] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
collected 8 items
Running 8 items in this shard

../../../../test/distributed/pipelining/test_stage.py::StageTest::test_custom_dw_with_fb_schedule [2025-09-19 17:11:50.238] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:11:50.246] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:11:50.247] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[2025-09-19 17:11:50.253] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
2025:09:19-17:11:50:2363479 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:11:50:2363479 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:11:50:2363476 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:11:50:2363476 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:11:50:2363477 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:11:50:2363477 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:19-17:11:50:2363478 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:19-17:11:50:2363478 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
