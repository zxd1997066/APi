W0817 14:49:18.470000 3989320 site-packages/torch/distributed/run.py:803] 
W0817 14:49:18.470000 3989320 site-packages/torch/distributed/run.py:803] *****************************************
W0817 14:49:18.470000 3989320 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0817 14:49:18.470000 3989320 site-packages/torch/distributed/run.py:803] *****************************************
Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
low_cpu_ram: false
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/logs
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
low_cpu_ram: false
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/logs
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
low_cpu_ram: false
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/logs
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
low_cpu_ram: false
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/logs
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank 
3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/torchtune/llama3_1_8B/qlora_single_device/logs/log_1755442161.txt
FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...
2025:08:17-14:49:24:(3989396) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:08:17-14:49:24:(3989396) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:08:17-14:49:24:(3989397) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:08:17-14:49:24:(3989397) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:08:17-14:49:24:(3989394) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:08:17-14:49:24:(3989394) |CCL_WARN| value of CCL_RECV changed to be direct (default:)
2025:08:17-14:49:24:(3989394) |CCL_WARN| value of CCL_SEND changed to be direct (default:)
2025:08:17-14:49:24:(3989394) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:08:17-14:49:25:(3989395) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:08:17-14:49:25:(3989395) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_device.py:103: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/c10/core/AllocatorConfig.cpp:28.)
  return func(*args, **kwargs)
/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_device.py:103: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/c10/core/AllocatorConfig.cpp:28.)
  return func(*args, **kwargs)
/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_device.py:103: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/c10/core/AllocatorConfig.cpp:28.)
  return func(*args, **kwargs)
/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_device.py:103: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/c10/core/AllocatorConfig.cpp:28.)
  return func(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py:535: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Instantiating model and loading checkpoint took 20.77 secs
Memory stats after model init:
	XPU peak memory active: 2.38 GiB
	XPU peak memory alloc: 2.38 GiB
	XPU peak memory reserved: 2.83 GiB
Optimizer is initialized.
Loss is initialized.
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 212/51760 [00:00<00:24, 2108.31it/s]Packing dataset:   1%|          | 537/51760 [00:00<00:18, 2773.95it/s]Packing dataset:   2%|▏         | 849/51760 [00:00<00:17, 2926.93it/s]Packing dataset:   2%|▏         | 1181/51760 [00:00<00:16, 3077.11it/s]Packing dataset:   3%|▎         | 1511/51760 [00:00<00:15, 3155.15it/s]Packing dataset:   4%|▎         | 1827/51760 [00:00<00:16, 3048.43it/s]Packing dataset:   4%|▍         | 2150/51760 [00:00<00:15, 3105.51it/s]Packing dataset:   5%|▍         | 2495/51760 [00:00<00:15, 3213.18it/s]Packing dataset:   5%|▌         | 2834/51760 [00:00<00:14, 3266.02it/s]Packing dataset:   6%|▌         | 3162/51760 [00:01<00:14, 3263.89it/s]Packing dataset:   7%|▋         | 3489/51760 [00:01<00:15, 3196.22it/s]Packing dataset:   7%|▋         | 3820/51760 [00:01<00:14, 3229.66it/s]Packing dataset:   8%|▊         | 4144/51760 [00:01<00:14, 3189.04it/s]Packing dataset:   9%|▊         | 4478/51760 [00:01<00:14, 3230.44it/s]Packing dataset:   9%|▉         | 4802/51760 [00:01<00:14, 3188.69it/s]Packing dataset:  10%|▉         | 5125/51760 [00:01<00:14, 3200.21it/s]Packing dataset:  11%|█         | 5446/51760 [00:01<00:14, 3188.61it/s]Packing dataset:  11%|█         | 5788/51760 [00:01<00:14, 3253.19it/s]Packing dataset:  12%|█▏        | 6114/51760 [00:01<00:14, 3238.37it/s]Packing dataset:  12%|█▏        | 6438/51760 [00:02<00:14, 3221.01it/s]Packing dataset:  13%|█▎        | 6761/51760 [00:02<00:14, 3181.36it/s]Packing dataset:  14%|█▎        | 7080/51760 [00:02<00:14, 3151.49it/s]Packing dataset:  14%|█▍        | 7396/51760 [00:02<00:14, 3138.41it/s]Packing dataset:  15%|█▍        | 7728/51760 [00:02<00:13, 3191.18it/s]Packing dataset:  16%|█▌        | 8048/51760 [00:02<00:13, 3156.67it/s]Packing dataset:  16%|█▌        | 8364/51760 [00:02<00:13, 3109.17it/s]Packing dataset:  17%|█▋        | 8693/51760 [00:02<00:13, 3157.39it/s]Packing dataset:  17%|█▋        | 9040/51760 [00:02<00:13, 3246.15it/s]Packing dataset:  18%|█▊        | 9365/51760 [00:02<00:13, 3192.11it/s]Packing dataset:  19%|█▊        | 9685/51760 [00:03<00:13, 3114.38it/s]Packing dataset:  19%|█▉        | 10001/51760 [00:03<00:13, 3126.45it/s]Packing dataset:  20%|█▉        | 10315/51760 [00:03<00:13, 3125.07it/s]Packing dataset:  21%|██        | 10640/51760 [00:03<00:13, 3159.50it/s]Packing dataset:  21%|██        | 10957/51760 [00:03<00:12, 3161.06it/s]Packing dataset:  22%|██▏       | 11274/51760 [00:03<00:13, 3105.99it/s]Packing dataset:  22%|██▏       | 11590/51760 [00:03<00:12, 3121.46it/s]Packing dataset:  23%|██▎       | 11903/51760 [00:03<00:12, 3123.40it/s]Packing dataset:  24%|██▎       | 12216/51760 [00:03<00:12, 3119.36it/s]Packing dataset:  24%|██▍       | 12529/51760 [00:03<00:12, 3121.15it/s]Packing dataset:  25%|██▍       | 12846/51760 [00:04<00:12, 3131.48it/s]Packing dataset:  25%|██▌       | 13161/51760 [00:04<00:12, 3136.75it/s]Packing dataset:  26%|██▌       | 13475/51760 [00:04<00:12, 3135.64it/s]Packing dataset:  27%|██▋       | 13789/51760 [00:04<00:12, 3119.77it/s]Packing dataset:  27%|██▋       | 14102/51760 [00:04<00:12, 3111.82it/s]Packing dataset:  28%|██▊       | 14428/51760 [00:04<00:11, 3151.22it/s]Packing dataset:  28%|██▊       | 14744/51760 [00:04<00:11, 3105.43it/s]Packing dataset:  29%|██▉       | 15058/51760 [00:04<00:11, 3112.03it/s]Packing dataset:  30%|██▉       | 15382/51760 [00:04<00:11, 3148.24it/s]Packing dataset:  30%|███       | 15697/51760 [00:04<00:11, 3128.98it/s]Packing dataset:  31%|███       | 16030/51760 [00:05<00:11, 3188.58it/s]Packing dataset:  32%|███▏      | 16349/51760 [00:05<00:11, 3134.06it/s]Packing dataset:  32%|███▏      | 16664/51760 [00:05<00:11, 3138.66it/s]Packing dataset:  33%|███▎      | 16979/51760 [00:05<00:11, 3122.04it/s]Packing dataset:  33%|███▎      | 17292/51760 [00:05<00:11, 3120.94it/s]Packing dataset:  34%|███▍      | 17605/51760 [00:05<00:11, 3090.22it/s]Packing dataset:  35%|███▍      | 17915/51760 [00:05<00:11, 3064.45it/s]Packing dataset:  35%|███▌      | 18236/51760 [00:05<00:10, 3104.67it/s]Packing dataset:  36%|███▌      | 18547/51760 [00:06<00:15, 2172.08it/s]Packing dataset:  36%|███▋      | 18866/51760 [00:06<00:13, 2404.03it/s]Packing dataset:  37%|███▋      | 19177/51760 [00:06<00:12, 2575.65it/s]Packing dataset:  38%|███▊      | 19469/51760 [00:06<00:12, 2664.43it/s]Packing dataset:  38%|███▊      | 19765/51760 [00:06<00:11, 2743.80it/s]Packing dataset:  39%|███▉      | 20079/51760 [00:06<00:11, 2851.63it/s]Packing dataset:  39%|███▉      | 20386/51760 [00:06<00:10, 2910.53it/s]Packing dataset:  40%|███▉      | 20686/51760 [00:06<00:10, 2929.09it/s]Packing dataset:  41%|████      | 21006/51760 [00:06<00:10, 3005.18it/s]Packing dataset:  41%|████      | 21314/51760 [00:06<00:10, 3025.69it/s]Packing dataset:  42%|████▏     | 21620/51760 [00:07<00:10, 3011.47it/s]Packing dataset:  42%|████▏     | 21926/51760 [00:07<00:09, 3022.57it/s]Packing dataset:  43%|████▎     | 22230/51760 [00:07<00:09, 2989.40it/s]Packing dataset:  44%|████▎     | 22550/51760 [00:07<00:09, 3050.35it/s]Packing dataset:  44%|████▍     | 22856/51760 [00:07<00:09, 3003.80it/s]Packing dataset:  45%|████▍     | 23164/51760 [00:07<00:09, 3024.81it/s]Packing dataset:  45%|████▌     | 23494/51760 [00:07<00:09, 3104.46it/s]Packing dataset:  46%|████▌     | 23805/51760 [00:07<00:09, 3092.84it/s]Packing dataset:  47%|████▋     | 24115/51760 [00:07<00:09, 3059.00it/s]Packing dataset:  47%|████▋     | 24455/51760 [00:07<00:08, 3158.11it/s]Packing dataset:  48%|████▊     | 24772/51760 [00:08<00:08, 3156.15it/s]Packing dataset:  48%|████▊     | 25088/51760 [00:08<00:08, 3057.55it/s]Packing dataset:  49%|████▉     | 25416/51760 [00:08<00:08, 3118.60it/s]Packing dataset:  50%|████▉     | 25729/51760 [00:08<00:08, 3093.33it/s]Packing dataset:  50%|█████     | 26039/51760 [00:08<00:08, 3066.90it/s]Packing dataset:  51%|█████     | 26347/51760 [00:08<00:08, 3064.23it/s]Packing dataset:  52%|█████▏    | 26665/51760 [00:08<00:08, 3096.99it/s]Packing dataset:  52%|█████▏    | 26994/51760 [00:08<00:07, 3149.67it/s]Packing dataset:  53%|█████▎    | 27315/51760 [00:08<00:07, 3164.26it/s]Packing dataset:  53%|█████▎    | 27632/51760 [00:09<00:07, 3094.89it/s]Packing dataset:  54%|█████▍    | 27942/51760 [00:09<00:07, 3079.79it/s]Packing dataset:  55%|█████▍    | 28251/51760 [00:09<00:07, 3027.15it/s]Packing dataset:  55%|█████▌    | 28555/51760 [00:09<00:07, 3012.41it/s]Packing dataset:  56%|█████▌    | 28857/51760 [00:09<00:07, 3014.49it/s]Packing dataset:  56%|█████▋    | 29161/51760 [00:09<00:07, 3019.43it/s]Packing dataset:  57%|█████▋    | 29469/51760 [00:09<00:07, 3034.26it/s]Packing dataset:  58%|█████▊    | 29773/51760 [00:09<00:07, 3002.58it/s]Packing dataset:  58%|█████▊    | 30074/51760 [00:09<00:07, 2980.10it/s]Packing dataset:  59%|█████▊    | 30386/51760 [00:09<00:07, 3016.57it/s]Packing dataset:  59%|█████▉    | 30688/51760 [00:10<00:07, 3007.53it/s]Packing dataset:  60%|█████▉    | 30989/51760 [00:10<00:06, 3000.12it/s]Packing dataset:  60%|██████    | 31295/51760 [00:10<00:06, 3014.56it/s]Packing dataset:  61%|██████    | 31623/51760 [00:10<00:06, 3092.48it/s]Packing dataset:  62%|██████▏   | 31944/51760 [00:10<00:06, 3126.40it/s]Packing dataset:  62%|██████▏   | 32257/51760 [00:10<00:06, 3079.82it/s]Packing dataset:  63%|██████▎   | 32569/51760 [00:10<00:06, 3090.98it/s]Packing dataset:  64%|██████▎   | 32898/51760 [00:10<00:05, 3148.57it/s]Packing dataset:  64%|██████▍   | 33214/51760 [00:10<00:05, 3129.14it/s]Packing dataset:  65%|██████▍   | 33537/51760 [00:10<00:05, 3158.96it/s]Packing dataset:  65%|██████▌   | 33854/51760 [00:11<00:05, 3119.64it/s]Packing dataset:  66%|██████▌   | 34167/51760 [00:11<00:05, 3118.35it/s]Packing dataset:  67%|██████▋   | 34485/51760 [00:11<00:05, 3136.51it/s]Packing dataset:  67%|██████▋   | 34804/51760 [00:11<00:05, 3147.14it/s]Packing dataset:  68%|██████▊   | 35119/51760 [00:11<00:05, 3118.36it/s]Packing dataset:  69%|██████▊   | 35457/51760 [00:11<00:05, 3193.62it/s]Packing dataset:  69%|██████▉   | 35777/51760 [00:11<00:05, 3094.97it/s]Packing dataset:  70%|██████▉   | 36088/51760 [00:11<00:05, 3080.59it/s]Packing dataset:  70%|███████   | 36401/51760 [00:11<00:04, 3094.66it/s]Packing dataset:  71%|███████   | 36725/51760 [00:11<00:04, 3135.13it/s]Packing dataset:  72%|███████▏  | 37039/51760 [00:12<00:04, 3104.65it/s]Packing dataset:  72%|███████▏  | 37354/51760 [00:12<00:04, 3112.49it/s]Packing dataset:  73%|███████▎  | 37679/51760 [00:12<00:04, 3153.21it/s]Packing dataset:  73%|███████▎  | 38001/51760 [00:12<00:04, 3171.13it/s]Packing dataset:  74%|███████▍  | 38319/51760 [00:12<00:04, 3156.47it/s]Packing dataset:  75%|███████▍  | 38635/51760 [00:12<00:04, 3106.47it/s]Packing dataset:  75%|███████▌  | 38946/51760 [00:12<00:04, 3104.03it/s]Packing dataset:  76%|███████▌  | 39273/51760 [00:12<00:03, 3151.59it/s]Packing dataset:  76%|███████▋  | 39589/51760 [00:12<00:03, 3132.49it/s]Packing dataset:  77%|███████▋  | 39903/51760 [00:12<00:03, 3130.17it/s]Packing dataset:  78%|███████▊  | 40217/51760 [00:13<00:03, 3096.08it/s]Packing dataset:  78%|███████▊  | 40549/51760 [00:13<00:03, 3160.27it/s]Packing dataset:  79%|███████▉  | 40868/51760 [00:13<00:03, 3168.53it/s]Packing dataset:  80%|███████▉  | 41185/51760 [00:13<00:03, 3157.15it/s]Packing dataset:  80%|████████  | 41501/51760 [00:13<00:03, 3146.11it/s]Packing dataset:  81%|████████  | 41816/51760 [00:13<00:03, 3136.96it/s]Packing dataset:  81%|████████▏ | 42130/51760 [00:13<00:03, 3074.56it/s]Packing dataset:  82%|████████▏ | 42458/51760 [00:13<00:02, 3131.09it/s]Packing dataset:  83%|████████▎ | 42772/51760 [00:13<00:02, 3063.21it/s]Packing dataset:  83%|████████▎ | 43079/51760 [00:13<00:02, 3018.66it/s]Packing dataset:  84%|████████▍ | 43385/51760 [00:14<00:02, 3030.60it/s]Packing dataset:  84%|████████▍ | 43697/51760 [00:14<00:02, 3054.51it/s]Packing dataset:  85%|████████▌ | 44004/51760 [00:14<00:02, 3057.05it/s]Packing dataset:  86%|████████▌ | 44321/51760 [00:14<00:02, 3089.85it/s]Packing dataset:  86%|████████▌ | 44631/51760 [00:14<00:02, 3086.67it/s]Packing dataset:  87%|████████▋ | 44945/51760 [00:14<00:02, 3101.28it/s]Packing dataset:  87%|████████▋ | 45268/51760 [00:14<00:02, 3136.17it/s]Packing dataset:  88%|████████▊ | 45597/51760 [00:14<00:01, 3181.51it/s]Packing dataset:  89%|████████▊ | 45916/51760 [00:14<00:01, 3072.06it/s]Packing dataset:  89%|████████▉ | 46228/51760 [00:15<00:01, 3083.32it/s]Packing dataset:  90%|████████▉ | 46537/51760 [00:15<00:01, 3078.25it/s]Packing dataset:  91%|█████████ | 46859/51760 [00:15<00:01, 3120.07it/s]Packing dataset:  91%|█████████ | 47172/51760 [00:15<00:01, 3088.52it/s]Packing dataset:  92%|█████████▏| 47482/51760 [00:15<00:01, 3088.59it/s]Packing dataset:  92%|█████████▏| 47792/51760 [00:15<00:01, 3078.24it/s]Packing dataset:  93%|█████████▎| 48100/51760 [00:15<00:01, 3050.50it/s]Packing dataset:  94%|█████████▎| 48420/51760 [00:15<00:01, 3092.94it/s]Packing dataset:  94%|█████████▍| 48730/51760 [00:15<00:00, 3091.68it/s]Packing dataset:  95%|█████████▍| 49040/51760 [00:15<00:00, 3068.87it/s]Packing dataset:  95%|█████████▌| 49353/51760 [00:16<00:00, 3085.34it/s]Packing dataset:  96%|█████████▌| 49669/51760 [00:16<00:00, 3103.98it/s]Packing dataset:  97%|█████████▋| 49980/51760 [00:16<00:00, 3093.18it/s]Packing dataset:  97%|█████████▋| 50290/51760 [00:16<00:00, 3079.98it/s]Packing dataset:  98%|█████████▊| 50603/51760 [00:16<00:00, 3089.96it/s]Packing dataset:  98%|█████████▊| 50916/51760 [00:16<00:00, 3096.46it/s]Packing dataset:  99%|█████████▉| 51233/51760 [00:16<00:00, 3116.68it/s]Packing dataset: 100%|█████████▉| 51549/51760 [00:16<00:00, 3129.47it/s]Packing dataset: 100%|██████████| 51760/51760 [00:16<00:00, 3081.50it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py:724: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, self.world_size / num_tokens)
 10%|█         | 1/10 [00:40<06:05, 40.62s/it]1|1|Loss: 10.230015754699707:  10%|█         | 1/10 [00:40<06:05, 40.62s/it]iteration:  1 tokens:  23761 time:  40.62450539693236 tokens_per_second:  584.89
1|1|Loss: 10.230015754699707:  20%|██        | 2/10 [01:05<04:09, 31.23s/it]1|2|Loss: 9.492205619812012:  20%|██        | 2/10 [01:05<04:09, 31.23s/it] iteration:  2 tokens:  24311 time:  24.620852791238576 tokens_per_second:  987.42
1|2|Loss: 9.492205619812012:  30%|███       | 3/10 [01:29<03:16, 28.08s/it]1|3|Loss: 9.254798889160156:  30%|███       | 3/10 [01:29<03:16, 28.08s/it]iteration:  3 tokens:  25616 time:  24.314327163156122 tokens_per_second:  1053.54
1|3|Loss: 9.254798889160156:  40%|████      | 4/10 [01:54<02:40, 26.68s/it]1|4|Loss: 10.697731971740723:  40%|████      | 4/10 [01:54<02:40, 26.68s/it]iteration:  4 tokens:  25431 time:  24.496766342781484 tokens_per_second:  1038.14
1|4|Loss: 10.697731971740723:  50%|█████     | 5/10 [02:18<02:09, 25.86s/it]1|5|Loss: 10.075939178466797:  50%|█████     | 5/10 [02:18<02:09, 25.86s/it]iteration:  5 tokens:  25001 time:  24.37847122689709 tokens_per_second:  1025.54
1|5|Loss: 10.075939178466797:  60%|██████    | 6/10 [02:43<01:41, 25.41s/it]1|6|Loss: 9.526391983032227:  60%|██████    | 6/10 [02:43<01:41, 25.41s/it] iteration:  6 tokens:  24727 time:  24.50602508895099 tokens_per_second:  1009.02
1|6|Loss: 9.526391983032227:  70%|███████   | 7/10 [03:07<01:15, 25.07s/it]1|7|Loss: 9.004551887512207:  70%|███████   | 7/10 [03:07<01:15, 25.07s/it]iteration:  7 tokens:  24700 time:  24.33884424297139 tokens_per_second:  1014.84
1|7|Loss: 9.004551887512207:  80%|████████  | 8/10 [03:31<00:49, 24.83s/it]1|8|Loss: 10.521263122558594:  80%|████████  | 8/10 [03:31<00:49, 24.83s/it]iteration:  8 tokens:  24112 time:  24.30027224915102 tokens_per_second:  992.25
1|8|Loss: 10.521263122558594:  90%|█████████ | 9/10 [03:56<00:24, 24.72s/it]1|9|Loss: 10.458183288574219:  90%|█████████ | 9/10 [03:56<00:24, 24.72s/it]iteration:  9 tokens:  24015 time:  24.446984488051385 tokens_per_second:  982.33
1|9|Loss: 10.458183288574219: 100%|██████████| 10/10 [04:20<00:00, 24.58s/it]1|10|Loss: 10.23769760131836: 100%|██████████| 10/10 [04:20<00:00, 24.58s/it]iteration:  10 tokens:  24249 time:  24.255285827908665 tokens_per_second:  999.74
avg tokens_per_second:  1014.43
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 912, in <module>
[rank1]:     sys.exit(recipe_main())
[rank1]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank1]:     sys.exit(recipe_main(conf))
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 907, in recipe_main
[rank1]:     recipe.train()
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 808, in train
[rank1]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank1]: ZeroDivisionError: division by zero
1|10|Loss: 10.23769760131836: 100%|██████████| 10/10 [04:20<00:00, 26.05s/it]
[rank0]:[W817 14:54:22.971045499 ProcessGroup.hpp:941] Warning: No backend of type 0 found for Process Group with name undefined. Assuming no hooks are registered. (function hasHooks)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 912, in <module>
[rank3]:     sys.exit(recipe_main())
[rank3]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank3]:     sys.exit(recipe_main(conf))
[rank3]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 907, in recipe_main
[rank3]:     recipe.train()
[rank3]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 808, in train
[rank3]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank3]: ZeroDivisionError: division by zero
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 912, in <module>
[rank2]:     sys.exit(recipe_main())
[rank2]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank2]:     sys.exit(recipe_main(conf))
[rank2]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 907, in recipe_main
[rank2]:     recipe.train()
[rank2]:   File "/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py", line 808, in train
[rank2]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank2]: ZeroDivisionError: division by zero
W0817 14:54:23.989000 3989320 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3989394 closing signal SIGTERM
W0817 14:54:23.991000 3989320 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3989395 closing signal SIGTERM
W0817 14:54:23.991000 3989320 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3989396 closing signal SIGTERM
E0817 14:54:24.206000 3989320 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 3 (pid: 3989397) of binary: /home/jenkins/.conda/envs/xpu_op_/bin/python3.10
Running with torchrun...
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/bin/tune", line 7, in <module>
    sys.exit(main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 212, in _run_cmd
    self._run_distributed(args, is_builtin=is_builtin)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 101, in _run_distributed
    run(args)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 157, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 294, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-17_14:54:23
  host      : dut7358
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3989397)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W817 14:54:24.943099027 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
