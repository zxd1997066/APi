W0916 01:58:54.026000 1542178 site-packages/torch/distributed/run.py:814] 
W0916 01:58:54.026000 1542178 site-packages/torch/distributed/run.py:814] *****************************************
W0916 01:58:54.026000 1542178 site-packages/torch/distributed/run.py:814] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0916 01:58:54.026000 1542178 site-packages/torch/distributed/run.py:814] *****************************************
Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: 123
shuffle: true
tensor_parallel_dim: 2
tensor_parallel_plan:
  _component_: torchtune.models.llama3.base_llama_tp_plan
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: 123
shuffle: true
tensor_parallel_dim: 2
tensor_parallel_plan:
  _component_: torchtune.models.llama3.base_llama_tp_plan
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/torchtune/llama3_1_8B/full/logs/log_1757987937.txt
Distributed training is enabled. Instantiating model and loading checkpoint on Rank 0 ...
using TP
using TP
2025:09:16-01:58:58:1542252 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:16-01:58:58:1542252 |CCL_WARN| value of CCL_RECV changed to be direct (default:)
2025:09:16-01:58:58:1542252 |CCL_WARN| value of CCL_SEND changed to be direct (default:)
2025:09:16-01:58:58:1542252 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:16-01:58:58:1542253 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi
2025:09:16-01:58:58:1542253 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Instantiating model and loading checkpoint took 5.63 secs
Memory stats after model init:
	XPU peak memory active: 8.47 GiB
	XPU peak memory alloc: 8.47 GiB
	XPU peak memory reserved: 8.62 GiB
In-backward optimizers are set up.
Loss is initialized.
Packing dataset:   0%|          | 0/49172 [00:00<?, ?it/s]Packing dataset:   1%|          | 330/49172 [00:00<00:14, 3294.42it/s]Packing dataset:   1%|▏         | 660/49172 [00:00<00:15, 3190.13it/s]Packing dataset:   2%|▏         | 980/49172 [00:00<00:15, 3126.10it/s]Packing dataset:   3%|▎         | 1325/49172 [00:00<00:14, 3250.60it/s]Packing dataset:   3%|▎         | 1651/49172 [00:00<00:14, 3177.58it/s]Packing dataset:   4%|▍         | 1970/49172 [00:00<00:14, 3147.67it/s]Packing dataset:   5%|▍         | 2304/49172 [00:00<00:14, 3208.51it/s]Packing dataset:   5%|▌         | 2641/49172 [00:00<00:14, 3258.12it/s]Packing dataset:   6%|▌         | 2985/49172 [00:00<00:13, 3313.26it/s]Packing dataset:   7%|▋         | 3317/49172 [00:01<00:14, 3215.99it/s]Packing dataset:   7%|▋         | 3640/49172 [00:01<00:14, 3204.13it/s]Packing dataset:   8%|▊         | 3968/49172 [00:01<00:14, 3223.62it/s]Packing dataset:   9%|▊         | 4291/49172 [00:01<00:13, 3215.89it/s]Packing dataset:   9%|▉         | 4613/49172 [00:01<00:13, 3195.39it/s]Packing dataset:  10%|█         | 4933/49172 [00:01<00:13, 3194.54it/s]Packing dataset:  11%|█         | 5253/49172 [00:01<00:13, 3176.32it/s]Packing dataset:  11%|█▏        | 5596/49172 [00:01<00:13, 3251.50it/s]Packing dataset:  12%|█▏        | 5922/49172 [00:01<00:13, 3237.35it/s]Packing dataset:  13%|█▎        | 6246/49172 [00:01<00:13, 3237.37it/s]Packing dataset:  13%|█▎        | 6570/49172 [00:02<00:13, 3178.74it/s]Packing dataset:  14%|█▍        | 6890/49172 [00:02<00:13, 3185.00it/s]Packing dataset:  15%|█▍        | 7209/49172 [00:02<00:13, 3173.42it/s]Packing dataset:  15%|█▌        | 7527/49172 [00:02<00:13, 3154.16it/s]Packing dataset:  16%|█▌        | 7848/49172 [00:02<00:13, 3168.87it/s]Packing dataset:  17%|█▋        | 8168/49172 [00:02<00:12, 3177.86it/s]Packing dataset:  17%|█▋        | 8486/49172 [00:02<00:12, 3151.33it/s]Packing dataset:  18%|█▊        | 8825/49172 [00:02<00:12, 3219.64it/s]Packing dataset:  19%|█▊        | 9148/49172 [00:02<00:12, 3191.29it/s]Packing dataset:  19%|█▉        | 9471/49172 [00:02<00:12, 3197.78it/s]Packing dataset:  20%|█▉        | 9791/49172 [00:03<00:12, 3090.45it/s]Packing dataset:  21%|██        | 10119/49172 [00:03<00:12, 3143.64it/s]Packing dataset:  21%|██        | 10438/49172 [00:03<00:12, 3154.84it/s]Packing dataset:  22%|██▏       | 10774/49172 [00:03<00:11, 3211.49it/s]Packing dataset:  23%|██▎       | 11096/49172 [00:03<00:11, 3173.76it/s]Packing dataset:  23%|██▎       | 11414/49172 [00:03<00:11, 3152.76it/s]Packing dataset:  24%|██▍       | 11730/49172 [00:03<00:11, 3140.55it/s]Packing dataset:  24%|██▍       | 12045/49172 [00:03<00:11, 3131.73it/s]Packing dataset:  25%|██▌       | 12359/49172 [00:03<00:11, 3129.72it/s]Packing dataset:  26%|██▌       | 12677/49172 [00:03<00:11, 3143.68it/s]Packing dataset:  26%|██▋       | 12992/49172 [00:04<00:11, 3096.45it/s]Packing dataset:  27%|██▋       | 13312/49172 [00:04<00:11, 3124.28it/s]Packing dataset:  28%|██▊       | 13625/49172 [00:04<00:11, 3120.34it/s]Packing dataset:  28%|██▊       | 13941/49172 [00:04<00:11, 3130.62it/s]Packing dataset:  29%|██▉       | 14255/49172 [00:04<00:11, 3122.88it/s]Packing dataset:  30%|██▉       | 14582/49172 [00:04<00:10, 3162.87it/s]Packing dataset:  30%|███       | 14899/49172 [00:04<00:10, 3131.98it/s]Packing dataset:  31%|███       | 15225/49172 [00:04<00:10, 3167.49it/s]Packing dataset:  32%|███▏      | 15552/49172 [00:04<00:10, 3197.53it/s]Packing dataset:  32%|███▏      | 15872/49172 [00:04<00:10, 3195.17it/s]Packing dataset:  33%|███▎      | 16192/49172 [00:05<00:10, 3136.87it/s]Packing dataset:  34%|███▎      | 16506/49172 [00:05<00:10, 3133.67it/s]Packing dataset:  34%|███▍      | 16820/49172 [00:05<00:10, 3108.37it/s]Packing dataset:  35%|███▍      | 17131/49172 [00:05<00:10, 3106.01it/s]Packing dataset:  35%|███▌      | 17442/49172 [00:05<00:10, 3084.86it/s]Packing dataset:  36%|███▌      | 17754/49172 [00:05<00:10, 3095.10it/s]Packing dataset:  37%|███▋      | 18065/49172 [00:05<00:10, 3097.48it/s]Packing dataset:  37%|███▋      | 18375/49172 [00:05<00:10, 3064.92it/s]Packing dataset:  38%|███▊      | 18694/49172 [00:05<00:09, 3101.97it/s]Packing dataset:  39%|███▊      | 19005/49172 [00:06<00:09, 3096.75it/s]Packing dataset:  39%|███▉      | 19315/49172 [00:06<00:09, 3038.69it/s]Packing dataset:  40%|███▉      | 19620/49172 [00:06<00:09, 3023.81it/s]Packing dataset:  41%|████      | 19925/49172 [00:06<00:09, 3029.45it/s]Packing dataset:  41%|████      | 20237/49172 [00:06<00:09, 3054.77it/s]Packing dataset:  42%|████▏     | 20543/49172 [00:06<00:09, 3010.61it/s]Packing dataset:  42%|████▏     | 20864/49172 [00:06<00:09, 3066.41it/s]Packing dataset:  43%|████▎     | 21204/49172 [00:06<00:08, 3164.48it/s]Packing dataset:  44%|████▍     | 21521/49172 [00:06<00:08, 3118.45it/s]Packing dataset:  44%|████▍     | 21842/49172 [00:06<00:08, 3142.92it/s]Packing dataset:  45%|████▌     | 22157/49172 [00:07<00:08, 3064.69it/s]Packing dataset:  46%|████▌     | 22482/49172 [00:07<00:08, 3114.69it/s]Packing dataset:  46%|████▋     | 22794/49172 [00:07<00:08, 3033.74it/s]Packing dataset:  47%|████▋     | 23100/49172 [00:07<00:08, 3040.10it/s]Packing dataset:  48%|████▊     | 23422/49172 [00:07<00:08, 3088.66it/s]Packing dataset:  48%|████▊     | 23740/49172 [00:07<00:08, 3114.17it/s]Packing dataset:  49%|████▉     | 24054/49172 [00:07<00:08, 3120.29it/s]Packing dataset:  50%|████▉     | 24379/49172 [00:07<00:07, 3156.49it/s]Packing dataset:  50%|█████     | 24709/49172 [00:07<00:07, 3195.43it/s]Packing dataset:  51%|█████     | 25029/49172 [00:07<00:07, 3143.50it/s]Packing dataset:  52%|█████▏    | 25344/49172 [00:08<00:07, 3139.70it/s]Packing dataset:  52%|█████▏    | 25659/49172 [00:08<00:07, 3116.18it/s]Packing dataset:  53%|█████▎    | 25971/49172 [00:08<00:07, 3113.44it/s]Packing dataset:  53%|█████▎    | 26283/49172 [00:08<00:07, 3092.76it/s]Packing dataset:  54%|█████▍    | 26596/49172 [00:08<00:07, 3103.64it/s]Packing dataset:  55%|█████▍    | 26912/49172 [00:08<00:07, 3118.25it/s]Packing dataset:  55%|█████▌    | 27238/49172 [00:08<00:06, 3156.23it/s]Packing dataset:  56%|█████▌    | 27554/49172 [00:08<00:06, 3137.20it/s]Packing dataset:  57%|█████▋    | 27868/49172 [00:08<00:06, 3098.34it/s]Packing dataset:  57%|█████▋    | 28178/49172 [00:08<00:06, 3044.00it/s]Packing dataset:  58%|█████▊    | 28483/49172 [00:09<00:06, 3022.71it/s]Packing dataset:  59%|█████▊    | 28798/49172 [00:09<00:06, 3054.68it/s]Packing dataset:  59%|█████▉    | 29104/49172 [00:09<00:06, 3044.27it/s]Packing dataset:  60%|█████▉    | 29412/49172 [00:09<00:06, 3050.24it/s]Packing dataset:  60%|██████    | 29718/49172 [00:09<00:06, 3044.83it/s]Packing dataset:  61%|██████    | 30023/49172 [00:09<00:06, 3032.56it/s]Packing dataset:  62%|██████▏   | 30337/49172 [00:09<00:06, 3062.96it/s]Packing dataset:  62%|██████▏   | 30644/49172 [00:09<00:06, 3037.69it/s]Packing dataset:  63%|██████▎   | 30950/49172 [00:09<00:05, 3039.40it/s]Packing dataset:  64%|██████▎   | 31254/49172 [00:09<00:05, 3000.36it/s]Packing dataset:  64%|██████▍   | 31582/49172 [00:10<00:05, 3078.80it/s]Packing dataset:  65%|██████▍   | 31904/49172 [00:10<00:05, 3120.19it/s]Packing dataset:  66%|██████▌   | 32217/49172 [00:10<00:05, 3075.47it/s]Packing dataset:  66%|██████▌   | 32535/49172 [00:10<00:05, 3105.41it/s]Packing dataset:  67%|██████▋   | 32848/49172 [00:10<00:05, 3112.62it/s]Packing dataset:  67%|██████▋   | 33160/49172 [00:10<00:05, 3111.20it/s]Packing dataset:  68%|██████▊   | 33489/49172 [00:10<00:04, 3161.68it/s]Packing dataset:  69%|██████▉   | 33806/49172 [00:10<00:04, 3117.54it/s]Packing dataset:  69%|██████▉   | 34126/49172 [00:10<00:04, 3138.66it/s]Packing dataset:  70%|███████   | 34441/49172 [00:11<00:04, 3093.37it/s]Packing dataset:  71%|███████   | 34760/49172 [00:11<00:04, 3117.40it/s]Packing dataset:  71%|███████▏  | 35072/49172 [00:11<00:04, 3110.32it/s]Packing dataset:  72%|███████▏  | 35398/49172 [00:11<00:04, 3152.84it/s]Packing dataset:  73%|███████▎  | 35714/49172 [00:11<00:04, 3097.51it/s]Packing dataset:  73%|███████▎  | 36025/49172 [00:11<00:04, 3046.99it/s]Packing dataset:  74%|███████▍  | 36331/49172 [00:11<00:04, 3044.73it/s]Packing dataset:  75%|███████▍  | 36650/49172 [00:11<00:04, 3085.85it/s]Packing dataset:  75%|███████▌  | 36959/49172 [00:11<00:03, 3083.31it/s]Packing dataset:  76%|███████▌  | 37286/49172 [00:11<00:03, 3136.82it/s]Packing dataset:  76%|███████▋  | 37601/49172 [00:12<00:03, 3139.93it/s]Packing dataset:  77%|███████▋  | 37916/49172 [00:12<00:03, 3057.20it/s]Packing dataset:  78%|███████▊  | 38223/49172 [00:12<00:03, 3050.19it/s]Packing dataset:  78%|███████▊  | 38529/49172 [00:12<00:03, 3047.49it/s]Packing dataset:  79%|███████▉  | 38834/49172 [00:12<00:03, 3016.45it/s]Packing dataset:  80%|███████▉  | 39150/49172 [00:12<00:03, 3055.57it/s]Packing dataset:  80%|████████  | 39457/49172 [00:12<00:03, 3059.82it/s]Packing dataset:  81%|████████  | 39769/49172 [00:12<00:03, 3075.51it/s]Packing dataset:  82%|████████▏ | 40081/49172 [00:12<00:02, 3088.58it/s]Packing dataset:  82%|████████▏ | 40394/49172 [00:12<00:02, 3100.88it/s]Packing dataset:  83%|████████▎ | 40707/49172 [00:13<00:02, 3109.54it/s]Packing dataset:  83%|████████▎ | 41025/49172 [00:13<00:02, 3126.27it/s]Packing dataset:  84%|████████▍ | 41338/49172 [00:13<00:02, 3118.34it/s]Packing dataset:  85%|████████▍ | 41650/49172 [00:13<00:02, 3084.97it/s]Packing dataset:  85%|████████▌ | 41959/49172 [00:13<00:02, 3056.92it/s]Packing dataset:  86%|████████▌ | 42265/49172 [00:13<00:02, 3046.56it/s]Packing dataset:  87%|████████▋ | 42570/49172 [00:13<00:02, 3024.85it/s]Packing dataset:  87%|████████▋ | 42873/49172 [00:13<00:02, 3023.70it/s]Packing dataset:  88%|████████▊ | 43176/49172 [00:13<00:02, 2977.56it/s]Packing dataset:  88%|████████▊ | 43482/49172 [00:13<00:01, 2999.84it/s]Packing dataset:  89%|████████▉ | 43791/49172 [00:14<00:01, 3026.51it/s]Packing dataset:  90%|████████▉ | 44096/49172 [00:14<00:01, 3033.44it/s]Packing dataset:  90%|█████████ | 44411/49172 [00:14<00:01, 3066.99it/s]Packing dataset:  91%|█████████ | 44718/49172 [00:14<00:01, 3064.41it/s]Packing dataset:  92%|█████████▏| 45025/49172 [00:14<00:01, 3060.98it/s]Packing dataset:  92%|█████████▏| 45352/49172 [00:14<00:01, 3122.82it/s]Packing dataset:  93%|█████████▎| 45665/49172 [00:14<00:01, 3103.84it/s]Packing dataset:  94%|█████████▎| 45976/49172 [00:14<00:01, 3050.33it/s]Packing dataset:  94%|█████████▍| 46292/49172 [00:14<00:00, 3081.02it/s]Packing dataset:  95%|█████████▍| 46601/49172 [00:14<00:00, 3053.47it/s]Packing dataset:  95%|█████████▌| 46920/49172 [00:15<00:00, 3090.26it/s]Packing dataset:  96%|█████████▌| 47230/49172 [00:15<00:00, 3064.12it/s]Packing dataset:  97%|█████████▋| 47547/49172 [00:15<00:00, 3092.74it/s]Packing dataset:  97%|█████████▋| 47857/49172 [00:15<00:00, 3068.80it/s]Packing dataset:  98%|█████████▊| 48164/49172 [00:15<00:00, 3051.25it/s]Packing dataset:  99%|█████████▊| 48473/49172 [00:15<00:00, 3061.51it/s]Packing dataset:  99%|█████████▉| 48796/49172 [00:15<00:00, 3107.28it/s]Packing dataset: 100%|█████████▉| 49107/49172 [00:15<00:00, 3083.95it/s]Packing dataset: 100%|██████████| 49172/49172 [00:15<00:00, 3112.36it/s]
No learning rate scheduler configured. Using constant learning rate.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:50<07:36, 50.68s/it]1|1|Loss: 1.8316072225570679:  10%|█         | 1/10 [00:50<07:36, 50.68s/it]iteration:  1 tokens:  1506 time:  50.68168375303503 tokens_per_second:  29.71
1|1|Loss: 1.8316072225570679:  20%|██        | 2/10 [00:52<02:53, 21.71s/it]1|2|Loss: 1.422127366065979:  20%|██        | 2/10 [00:52<02:53, 21.71s/it] iteration:  2 tokens:  1488 time:  1.4158869569655508 tokens_per_second:  1050.93
1|2|Loss: 1.422127366065979:  30%|███       | 3/10 [00:53<01:27, 12.45s/it]1|3|Loss: 1.1250905990600586:  30%|███       | 3/10 [00:53<01:27, 12.45s/it]iteration:  3 tokens:  1596 time:  1.4211385360104032 tokens_per_second:  1123.04
1|3|Loss: 1.1250905990600586:  40%|████      | 4/10 [00:54<00:48,  8.09s/it]1|4|Loss: 0.9198748469352722:  40%|████      | 4/10 [00:54<00:48,  8.09s/it]iteration:  4 tokens:  1526 time:  1.3961519079748541 tokens_per_second:  1093.0
1|4|Loss: 0.9198748469352722:  50%|█████     | 5/10 [00:56<00:28,  5.67s/it]1|5|Loss: 0.9819098711013794:  50%|█████     | 5/10 [00:56<00:28,  5.67s/it]iteration:  5 tokens:  1524 time:  1.374021364026703 tokens_per_second:  1109.15
1|5|Loss: 0.9819098711013794:  60%|██████    | 6/10 [00:57<00:16,  4.22s/it]1|6|Loss: 1.2770682573318481:  60%|██████    | 6/10 [00:57<00:16,  4.22s/it]iteration:  6 tokens:  1808 time:  1.4020435819984414 tokens_per_second:  1289.55
1|6|Loss: 1.2770682573318481:  70%|███████   | 7/10 [00:59<00:09,  3.30s/it]1|7|Loss: 1.0911906957626343:  70%|███████   | 7/10 [00:59<00:09,  3.30s/it]iteration:  7 tokens:  1666 time:  1.4204881340265274 tokens_per_second:  1172.84
1|7|Loss: 1.0911906957626343:  80%|████████  | 8/10 [01:00<00:05,  2.82s/it]1|8|Loss: 0.9436113834381104:  80%|████████  | 8/10 [01:00<00:05,  2.82s/it]iteration:  8 tokens:  1424 time:  1.7692344209644943 tokens_per_second:  804.87
1|8|Loss: 0.9436113834381104:  90%|█████████ | 9/10 [01:02<00:02,  2.50s/it]1|9|Loss: 0.943732738494873:  90%|█████████ | 9/10 [01:02<00:02,  2.50s/it] iteration:  9 tokens:  1386 time:  1.79338484298205 tokens_per_second:  772.84
1|9|Loss: 0.943732738494873: 100%|██████████| 10/10 [01:04<00:00,  2.16s/it]1|10|Loss: 0.9176751971244812: 100%|██████████| 10/10 [01:04<00:00,  2.16s/it]iteration:  10 tokens:  1818 time:  1.4140280520077795 tokens_per_second:  1285.69
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1124, in <module>
[rank1]:     sys.exit(recipe_main())
[rank1]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank1]:     sys.exit(recipe_main(conf))
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1119, in recipe_main
[rank1]:     recipe.train()
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1098, in train
[rank1]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank1]: ZeroDivisionError: division by zero
avg tokens_per_second:  1063.18
1|10|Loss: 0.9176751971244812: 100%|██████████| 10/10 [01:04<00:00,  6.41s/it]
[rank0]:[W916 02:00:38.328542796 ProcessGroup.hpp:941] Warning: No backend of type 0 found for Process Group with name undefined. Assuming no hooks are registered. (function hasHooks)
[rank1]:[W916 02:00:39.859271563 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0916 02:00:40.080000 1542178 site-packages/torch/distributed/elastic/multiprocessing/api.py:932] Sending process 1542252 closing signal SIGTERM
E0916 02:00:40.245000 1542178 site-packages/torch/distributed/elastic/multiprocessing/api.py:906] failed (exitcode: 1) local_rank: 1 (pid: 1542253) of binary: /home/jenkins/.conda/envs/xpu_op_/bin/python3.10
Running with torchrun...
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/bin/tune", line 7, in <module>
    sys.exit(main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 212, in _run_cmd
    self._run_distributed(args, is_builtin=is_builtin)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 101, in _run_distributed
    run(args)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/run.py", line 939, in run
    elastic_launch(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 158, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 299, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-16_02:00:40
  host      : dut7358
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1542253)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W916 02:00:40.962378632 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
