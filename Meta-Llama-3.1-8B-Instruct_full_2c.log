W0901 15:32:58.115000 3755082 site-packages/torch/distributed/run.py:803] 
W0901 15:32:58.115000 3755082 site-packages/torch/distributed/run.py:803] *****************************************
W0901 15:32:58.115000 3755082 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0901 15:32:58.115000 3755082 site-packages/torch/distributed/run.py:803] *****************************************
Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Running FullFinetuneRecipeDistributed with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1[Gloo] Rank 
0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Writing logs to /tmp/torchtune/llama3_1_8B/full/logs/log_1756740781.txt
Distributed training is enabled. Instantiating model and loading checkpoint on Rank 0 ...
2025:09:01-15:33:02:3755161 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi2025:09:01-15:33:02:3755160 |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi

2025:09:01-15:33:02:3755160 |CCL_WARN| value of CCL_RECV changed to be direct (default:)
2025:09:01-15:33:02:3755160 |CCL_WARN| value of CCL_SEND changed to be direct (default:)
2025:09:01-15:33:02:3755160 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
2025:09:01-15:33:02:3755161 |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
Instantiating model and loading checkpoint took 5.59 secs
Memory stats after model init:
	XPU peak memory active: 8.47 GiB
	XPU peak memory alloc: 8.47 GiB
	XPU peak memory reserved: 8.62 GiB
In-backward optimizers are set up.
Loss is initialized.
Packing dataset:   0%|          | 0/49172 [00:00<?, ?it/s]Packing dataset:   1%|          | 328/49172 [00:00<00:14, 3270.51it/s]Packing dataset:   1%|▏         | 656/49172 [00:00<00:14, 3240.06it/s]Packing dataset:   2%|▏         | 981/49172 [00:00<00:15, 3133.60it/s]Packing dataset:   3%|▎         | 1323/49172 [00:00<00:14, 3239.17it/s]Packing dataset:   3%|▎         | 1648/49172 [00:00<00:15, 3158.29it/s]Packing dataset:   4%|▍         | 1965/49172 [00:00<00:15, 3118.17it/s]Packing dataset:   5%|▍         | 2296/49172 [00:00<00:14, 3177.57it/s]Packing dataset:   5%|▌         | 2630/49172 [00:00<00:14, 3227.73it/s]Packing dataset:   6%|▌         | 2966/49172 [00:00<00:14, 3268.32it/s]Packing dataset:   7%|▋         | 3294/49172 [00:01<00:14, 3223.25it/s]Packing dataset:   7%|▋         | 3617/49172 [00:01<00:14, 3192.11it/s]Packing dataset:   8%|▊         | 3949/49172 [00:01<00:14, 3228.41it/s]Packing dataset:   9%|▊         | 4273/49172 [00:01<00:14, 3200.68it/s]Packing dataset:   9%|▉         | 4594/49172 [00:01<00:14, 3173.37it/s]Packing dataset:  10%|▉         | 4912/49172 [00:01<00:13, 3172.43it/s]Packing dataset:  11%|█         | 5230/49172 [00:01<00:13, 3158.71it/s]Packing dataset:  11%|█▏        | 5567/49172 [00:01<00:13, 3218.71it/s]Packing dataset:  12%|█▏        | 5891/49172 [00:01<00:13, 3224.39it/s]Packing dataset:  13%|█▎        | 6217/49172 [00:01<00:13, 3234.17it/s]Packing dataset:  13%|█▎        | 6541/49172 [00:02<00:13, 3137.63it/s]Packing dataset:  14%|█▍        | 6856/49172 [00:02<00:13, 3137.35it/s]Packing dataset:  15%|█▍        | 7171/49172 [00:02<00:13, 3103.36it/s]Packing dataset:  15%|█▌        | 7483/49172 [00:02<00:13, 3107.89it/s]Packing dataset:  16%|█▌        | 7812/49172 [00:02<00:13, 3159.04it/s]Packing dataset:  17%|█▋        | 8129/49172 [00:02<00:13, 3127.71it/s]Packing dataset:  17%|█▋        | 8442/49172 [00:02<00:13, 3106.24it/s]Packing dataset:  18%|█▊        | 8767/49172 [00:02<00:12, 3148.63it/s]Packing dataset:  19%|█▊        | 9097/49172 [00:02<00:12, 3189.37it/s]Packing dataset:  19%|█▉        | 9417/49172 [00:02<00:12, 3171.31it/s]Packing dataset:  20%|█▉        | 9735/49172 [00:03<00:12, 3062.02it/s]Packing dataset:  20%|██        | 10046/49172 [00:03<00:12, 3074.04it/s]Packing dataset:  21%|██        | 10361/49172 [00:03<00:12, 3095.74it/s]Packing dataset:  22%|██▏       | 10693/49172 [00:03<00:12, 3160.39it/s]Packing dataset:  22%|██▏       | 11010/49172 [00:03<00:12, 3145.02it/s]Packing dataset:  23%|██▎       | 11325/49172 [00:03<00:12, 3125.60it/s]Packing dataset:  24%|██▎       | 11640/49172 [00:03<00:11, 3130.18it/s]Packing dataset:  24%|██▍       | 11954/49172 [00:03<00:11, 3120.15it/s]Packing dataset:  25%|██▍       | 12267/49172 [00:03<00:11, 3094.74it/s]Packing dataset:  26%|██▌       | 12577/49172 [00:03<00:11, 3087.80it/s]Packing dataset:  26%|██▌       | 12886/49172 [00:04<00:11, 3088.26it/s]Packing dataset:  27%|██▋       | 13207/49172 [00:04<00:11, 3122.24it/s]Packing dataset:  27%|██▋       | 13520/49172 [00:04<00:11, 3087.51it/s]Packing dataset:  28%|██▊       | 13829/49172 [00:04<00:11, 3078.95it/s]Packing dataset:  29%|██▉       | 14137/49172 [00:04<00:11, 3059.59it/s]Packing dataset:  29%|██▉       | 14466/49172 [00:04<00:11, 3127.60it/s]Packing dataset:  30%|███       | 14779/49172 [00:04<00:11, 3086.37it/s]Packing dataset:  31%|███       | 15092/49172 [00:04<00:11, 3098.06it/s]Packing dataset:  31%|███▏      | 15414/49172 [00:04<00:10, 3132.75it/s]Packing dataset:  32%|███▏      | 15737/49172 [00:04<00:10, 3158.58it/s]Packing dataset:  33%|███▎      | 16067/49172 [00:05<00:10, 3200.32it/s]Packing dataset:  33%|███▎      | 16388/49172 [00:05<00:10, 3120.68it/s]Packing dataset:  34%|███▍      | 16705/49172 [00:05<00:10, 3130.85it/s]Packing dataset:  35%|███▍      | 17019/49172 [00:05<00:10, 3086.32it/s]Packing dataset:  35%|███▌      | 17328/49172 [00:05<00:10, 3068.60it/s]Packing dataset:  36%|███▌      | 17636/49172 [00:05<00:10, 3067.25it/s]Packing dataset:  36%|███▋      | 17943/49172 [00:05<00:10, 3036.72it/s]Packing dataset:  37%|███▋      | 18261/49172 [00:05<00:10, 3075.83it/s]Packing dataset:  38%|███▊      | 18569/49172 [00:05<00:10, 3037.97it/s]Packing dataset:  38%|███▊      | 18891/49172 [00:06<00:09, 3091.54it/s]Packing dataset:  39%|███▉      | 19208/49172 [00:06<00:09, 3113.32it/s]Packing dataset:  40%|███▉      | 19520/49172 [00:06<00:09, 3050.55it/s]Packing dataset:  40%|████      | 19826/49172 [00:06<00:09, 3050.50it/s]Packing dataset:  41%|████      | 20132/49172 [00:06<00:09, 3042.40it/s]Packing dataset:  42%|████▏     | 20441/49172 [00:06<00:09, 3053.23it/s]Packing dataset:  42%|████▏     | 20747/49172 [00:06<00:09, 3047.70it/s]Packing dataset:  43%|████▎     | 21066/49172 [00:06<00:09, 3089.51it/s]Packing dataset:  43%|████▎     | 21379/49172 [00:06<00:08, 3100.82it/s]Packing dataset:  44%|████▍     | 21690/49172 [00:06<00:08, 3074.29it/s]Packing dataset:  45%|████▍     | 21998/49172 [00:07<00:08, 3064.11it/s]Packing dataset:  45%|████▌     | 22305/49172 [00:07<00:08, 3037.02it/s]Packing dataset:  46%|████▌     | 22615/49172 [00:07<00:08, 3054.40it/s]Packing dataset:  47%|████▋     | 22921/49172 [00:07<00:08, 3017.00it/s]Packing dataset:  47%|████▋     | 23223/49172 [00:07<00:08, 3017.36it/s]Packing dataset:  48%|████▊     | 23550/49172 [00:07<00:08, 3091.57it/s]Packing dataset:  49%|████▊     | 23860/49172 [00:07<00:08, 3081.57it/s]Packing dataset:  49%|████▉     | 24169/49172 [00:07<00:08, 3082.91it/s]Packing dataset:  50%|████▉     | 24484/49172 [00:07<00:07, 3102.51it/s]Packing dataset:  50%|█████     | 24795/49172 [00:07<00:07, 3090.84it/s]Packing dataset:  51%|█████     | 25105/49172 [00:08<00:07, 3039.32it/s]Packing dataset:  52%|█████▏    | 25423/49172 [00:08<00:07, 3080.48it/s]Packing dataset:  52%|█████▏    | 25732/49172 [00:08<00:10, 2141.47it/s]Packing dataset:  53%|█████▎    | 26026/49172 [00:08<00:09, 2321.04it/s]Packing dataset:  54%|█████▎    | 26322/49172 [00:08<00:09, 2475.94it/s]Packing dataset:  54%|█████▍    | 26630/49172 [00:08<00:08, 2632.12it/s]Packing dataset:  55%|█████▍    | 26943/49172 [00:08<00:08, 2765.81it/s]Packing dataset:  55%|█████▌    | 27261/49172 [00:08<00:07, 2880.23it/s]Packing dataset:  56%|█████▌    | 27564/49172 [00:09<00:07, 2921.69it/s]Packing dataset:  57%|█████▋    | 27865/49172 [00:09<00:07, 2912.11it/s]Packing dataset:  57%|█████▋    | 28163/49172 [00:09<00:07, 2897.69it/s]Packing dataset:  58%|█████▊    | 28458/49172 [00:09<00:07, 2901.62it/s]Packing dataset:  58%|█████▊    | 28764/49172 [00:09<00:06, 2947.87it/s]Packing dataset:  59%|█████▉    | 29062/49172 [00:09<00:06, 2943.55it/s]Packing dataset:  60%|█████▉    | 29370/49172 [00:09<00:06, 2982.88it/s]Packing dataset:  60%|██████    | 29670/49172 [00:09<00:06, 2987.49it/s]Packing dataset:  61%|██████    | 29970/49172 [00:09<00:06, 2983.81it/s]Packing dataset:  62%|██████▏   | 30281/49172 [00:09<00:06, 3021.00it/s]Packing dataset:  62%|██████▏   | 30584/49172 [00:10<00:06, 2987.91it/s]Packing dataset:  63%|██████▎   | 30884/49172 [00:10<00:06, 2989.84it/s]Packing dataset:  63%|██████▎   | 31185/49172 [00:10<00:06, 2992.93it/s]Packing dataset:  64%|██████▍   | 31500/49172 [00:10<00:05, 3039.37it/s]Packing dataset:  65%|██████▍   | 31832/49172 [00:10<00:05, 3120.49it/s]Packing dataset:  65%|██████▌   | 32145/49172 [00:10<00:05, 3057.02it/s]Packing dataset:  66%|██████▌   | 32461/49172 [00:10<00:05, 3083.99it/s]Packing dataset:  67%|██████▋   | 32770/49172 [00:10<00:05, 3059.71it/s]Packing dataset:  67%|██████▋   | 33090/49172 [00:10<00:05, 3097.51it/s]Packing dataset:  68%|██████▊   | 33412/49172 [00:10<00:05, 3132.68it/s]Packing dataset:  69%|██████▊   | 33726/49172 [00:11<00:05, 3082.13it/s]Packing dataset:  69%|██████▉   | 34041/49172 [00:11<00:04, 3098.00it/s]Packing dataset:  70%|██████▉   | 34352/49172 [00:11<00:04, 3077.79it/s]Packing dataset:  70%|███████   | 34661/49172 [00:11<00:04, 3080.81it/s]Packing dataset:  71%|███████   | 34970/49172 [00:11<00:04, 3065.22it/s]Packing dataset:  72%|███████▏  | 35289/49172 [00:11<00:04, 3100.92it/s]Packing dataset:  72%|███████▏  | 35600/49172 [00:11<00:04, 3090.48it/s]Packing dataset:  73%|███████▎  | 35910/49172 [00:11<00:04, 3028.30it/s]Packing dataset:  74%|███████▎  | 36214/49172 [00:11<00:04, 3017.60it/s]Packing dataset:  74%|███████▍  | 36531/49172 [00:11<00:04, 3062.36it/s]Packing dataset:  75%|███████▍  | 36845/49172 [00:12<00:03, 3084.27it/s]Packing dataset:  76%|███████▌  | 37158/49172 [00:12<00:03, 3093.71it/s]Packing dataset:  76%|███████▌  | 37468/49172 [00:12<00:03, 3083.29it/s]Packing dataset:  77%|███████▋  | 37777/49172 [00:12<00:03, 3065.04it/s]Packing dataset:  77%|███████▋  | 38092/49172 [00:12<00:03, 3087.35it/s]Packing dataset:  78%|███████▊  | 38401/49172 [00:12<00:03, 3086.54it/s]Packing dataset:  79%|███████▊  | 38710/49172 [00:12<00:03, 3063.11it/s]Packing dataset:  79%|███████▉  | 39017/49172 [00:12<00:03, 3022.14it/s]Packing dataset:  80%|████████  | 39342/49172 [00:12<00:03, 3087.81it/s]Packing dataset:  81%|████████  | 39651/49172 [00:12<00:03, 3047.96it/s]Packing dataset:  81%|████████▏ | 39960/49172 [00:13<00:03, 3057.82it/s]Packing dataset:  82%|████████▏ | 40267/49172 [00:13<00:02, 3060.79it/s]Packing dataset:  83%|████████▎ | 40585/49172 [00:13<00:02, 3093.64it/s]Packing dataset:  83%|████████▎ | 40899/49172 [00:13<00:02, 3102.92it/s]Packing dataset:  84%|████████▍ | 41210/49172 [00:13<00:02, 3081.31it/s]Packing dataset:  84%|████████▍ | 41519/49172 [00:13<00:02, 3060.56it/s]Packing dataset:  85%|████████▌ | 41827/49172 [00:13<00:02, 3064.90it/s]Packing dataset:  86%|████████▌ | 42134/49172 [00:13<00:02, 3008.87it/s]Packing dataset:  86%|████████▋ | 42460/49172 [00:13<00:02, 3080.77it/s]Packing dataset:  87%|████████▋ | 42769/49172 [00:13<00:02, 3008.10it/s]Packing dataset:  88%|████████▊ | 43071/49172 [00:14<00:02, 2976.37it/s]Packing dataset:  88%|████████▊ | 43369/49172 [00:14<00:01, 2976.80it/s]Packing dataset:  89%|████████▉ | 43674/49172 [00:14<00:01, 2994.91it/s]Packing dataset:  89%|████████▉ | 43978/49172 [00:14<00:01, 3005.79it/s]Packing dataset:  90%|█████████ | 44282/49172 [00:14<00:01, 3014.53it/s]Packing dataset:  91%|█████████ | 44590/49172 [00:14<00:01, 3031.35it/s]Packing dataset:  91%|█████████▏| 44898/49172 [00:14<00:01, 3043.14it/s]Packing dataset:  92%|█████████▏| 45211/49172 [00:14<00:01, 3067.83it/s]Packing dataset:  93%|█████████▎| 45529/49172 [00:14<00:01, 3098.93it/s]Packing dataset:  93%|█████████▎| 45839/49172 [00:15<00:01, 2999.85it/s]Packing dataset:  94%|█████████▍| 46140/49172 [00:15<00:01, 2994.81it/s]Packing dataset:  94%|█████████▍| 46455/49172 [00:15<00:00, 3039.06it/s]Packing dataset:  95%|█████████▌| 46763/49172 [00:15<00:00, 3049.66it/s]Packing dataset:  96%|█████████▌| 47069/49172 [00:15<00:00, 3041.29it/s]Packing dataset:  96%|█████████▋| 47375/49172 [00:15<00:00, 3044.97it/s]Packing dataset:  97%|█████████▋| 47680/49172 [00:15<00:00, 3007.73it/s]Packing dataset:  98%|█████████▊| 47986/49172 [00:15<00:00, 3022.35it/s]Packing dataset:  98%|█████████▊| 48290/49172 [00:15<00:00, 3026.05it/s]Packing dataset:  99%|█████████▉| 48601/49172 [00:15<00:00, 3047.57it/s]Packing dataset:  99%|█████████▉| 48911/49172 [00:16<00:00, 3061.64it/s]Packing dataset: 100%|██████████| 49172/49172 [00:16<00:00, 3053.89it/s]
No learning rate scheduler configured. Using constant learning rate.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:46<07:01, 46.79s/it]1|1|Loss: 2.14093017578125:  10%|█         | 1/10 [00:46<07:01, 46.79s/it]iteration:  1 tokens:  1497 time:  46.790074409917 tokens_per_second:  31.99
1|1|Loss: 2.14093017578125:  20%|██        | 2/10 [00:48<02:44, 20.51s/it]1|2|Loss: 1.1706840991973877:  20%|██        | 2/10 [00:48<02:44, 20.51s/it]iteration:  2 tokens:  1561 time:  2.106310522183776 tokens_per_second:  741.11
1|2|Loss: 1.1706840991973877:  30%|███       | 3/10 [00:51<01:24, 12.11s/it]1|3|Loss: 1.1597411632537842:  30%|███       | 3/10 [00:51<01:24, 12.11s/it]iteration:  3 tokens:  1666 time:  2.111278371885419 tokens_per_second:  789.1
1|3|Loss: 1.1597411632537842:  40%|████      | 4/10 [00:53<00:48,  8.16s/it]1|4|Loss: 1.0445669889450073:  40%|████      | 4/10 [00:53<00:48,  8.16s/it]iteration:  4 tokens:  1545 time:  2.0939630516804755 tokens_per_second:  737.84
1|4|Loss: 1.0445669889450073:  50%|█████     | 5/10 [00:55<00:29,  5.99s/it]1|5|Loss: 0.9381914734840393:  50%|█████     | 5/10 [00:55<00:29,  5.99s/it]iteration:  5 tokens:  1602 time:  2.1443292698822916 tokens_per_second:  747.09
1|5|Loss: 0.9381914734840393:  60%|██████    | 6/10 [00:57<00:19,  4.76s/it]1|6|Loss: 0.8924615383148193:  60%|██████    | 6/10 [00:57<00:19,  4.76s/it]iteration:  6 tokens:  1746 time:  2.363301453180611 tokens_per_second:  738.8
1|6|Loss: 0.8924615383148193:  70%|███████   | 7/10 [00:59<00:11,  3.88s/it]1|7|Loss: 0.9696875810623169:  70%|███████   | 7/10 [00:59<00:11,  3.88s/it]iteration:  7 tokens:  1155 time:  2.073767955880612 tokens_per_second:  556.96
1|7|Loss: 0.9696875810623169:  80%|████████  | 8/10 [01:01<00:06,  3.31s/it]1|8|Loss: 0.9095448851585388:  80%|████████  | 8/10 [01:01<00:06,  3.31s/it]iteration:  8 tokens:  1742 time:  2.096762827131897 tokens_per_second:  830.8
1|8|Loss: 0.9095448851585388:  90%|█████████ | 9/10 [01:03<00:02,  2.93s/it]1|9|Loss: 0.8778583407402039:  90%|█████████ | 9/10 [01:03<00:02,  2.93s/it]iteration:  9 tokens:  1595 time:  2.095772746950388 tokens_per_second:  761.06
1|9|Loss: 0.8778583407402039: 100%|██████████| 10/10 [01:06<00:00,  2.68s/it]1|10|Loss: 0.705141544342041: 100%|██████████| 10/10 [01:06<00:00,  2.68s/it]iteration:  10 tokens:  1578 time:  2.102231317665428 tokens_per_second:  750.63
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1124, in <module>
[rank1]:     sys.exit(recipe_main())
[rank1]:   File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
[rank1]:     sys.exit(recipe_main(conf))
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1119, in recipe_main
[rank1]:     recipe.train()
[rank1]:   File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py", line 1098, in train
[rank1]:     print("avg tokens_per_second: ", round(total_tokens / total_time, 2))
[rank1]: ZeroDivisionError: division by zero
avg tokens_per_second:  739.34
1|10|Loss: 0.705141544342041: 100%|██████████| 10/10 [01:06<00:00,  6.60s/it]
[rank0]:[W901 15:34:43.183608464 ProcessGroup.hpp:941] Warning: No backend of type 0 found for Process Group with name undefined. Assuming no hooks are registered. (function hasHooks)
[rank1]:[W901 15:34:45.809891315 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
W0901 15:34:46.020000 3755082 site-packages/torch/distributed/elastic/multiprocessing/api.py:908] Sending process 3755160 closing signal SIGTERM
E0901 15:34:46.184000 3755082 site-packages/torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 3755161) of binary: /home/jenkins/.conda/envs/xpu_op_/bin/python3.10
Running with torchrun...
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/bin/tune", line 7, in <module>
    sys.exit(main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 212, in _run_cmd
    self._run_distributed(args, is_builtin=is_builtin)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 101, in _run_distributed
    run(args)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/jenkins/xiangdong/torchtune/recipes/full_finetune_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-01_15:34:46
  host      : dut7358
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3755161)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[W901 15:34:46.784074146 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
