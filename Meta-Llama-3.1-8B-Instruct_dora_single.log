Running LoRAFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_8B/dora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_8B/dora/logs
model:
  _component_: torchtune.models.llama3.lora_llama3_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_rank: 8
  use_dora: true
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_8B/dora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_8B/dora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 256
  path: /tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:436: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.45 GiB
	XPU peak memory alloc: 15.45 GiB
	XPU peak memory reserved: 15.56 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_8B/dora/logs/log_1760955261.txt
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 210/51760 [00:00<00:24, 2097.86it/s]Packing dataset:   1%|          | 528/51760 [00:00<00:18, 2725.52it/s]Packing dataset:   2%|▏         | 835/51760 [00:00<00:17, 2878.65it/s]Packing dataset:   2%|▏         | 1158/51760 [00:00<00:16, 3016.43it/s]Packing dataset:   3%|▎         | 1474/51760 [00:00<00:16, 3065.39it/s]Packing dataset:   3%|▎         | 1781/51760 [00:00<00:16, 3006.05it/s]Packing dataset:   4%|▍         | 2087/51760 [00:00<00:16, 3020.99it/s]Packing dataset:   5%|▍         | 2428/51760 [00:00<00:15, 3141.02it/s]Packing dataset:   5%|▌         | 2763/51760 [00:00<00:15, 3204.84it/s]Packing dataset:   6%|▌         | 3084/51760 [00:01<00:15, 3178.40it/s]Packing dataset:   7%|▋         | 3403/51760 [00:01<00:15, 3159.89it/s]Packing dataset:   7%|▋         | 3720/51760 [00:01<00:15, 3157.42it/s]Packing dataset:   8%|▊         | 4036/51760 [00:01<00:15, 3142.12it/s]Packing dataset:   8%|▊         | 4356/51760 [00:01<00:15, 3159.12it/s]Packing dataset:   9%|▉         | 4672/51760 [00:01<00:15, 3106.37it/s]Packing dataset:  10%|▉         | 4993/51760 [00:01<00:14, 3135.76it/s]Packing dataset:  10%|█         | 5307/51760 [00:01<00:15, 3096.56it/s]Packing dataset:  11%|█         | 5640/51760 [00:01<00:14, 3164.08it/s]Packing dataset:  12%|█▏        | 5957/51760 [00:01<00:14, 3144.06it/s]Packing dataset:  12%|█▏        | 6280/51760 [00:02<00:14, 3168.95it/s]Packing dataset:  13%|█▎        | 6598/51760 [00:02<00:14, 3113.16it/s]Packing dataset:  13%|█▎        | 6910/51760 [00:02<00:14, 3105.86it/s]Packing dataset:  14%|█▍        | 7221/51760 [00:02<00:14, 3090.68it/s]Packing dataset:  15%|█▍        | 7531/51760 [00:02<00:14, 3083.27it/s]Packing dataset:  15%|█▌        | 7847/51760 [00:02<00:14, 3103.07it/s]Packing dataset:  16%|█▌        | 8161/51760 [00:02<00:14, 3112.96it/s]Packing dataset:  16%|█▋        | 8473/51760 [00:02<00:14, 3073.63it/s]Packing dataset:  17%|█▋        | 8794/51760 [00:02<00:13, 3112.24it/s]Packing dataset:  18%|█▊        | 9113/51760 [00:02<00:13, 3131.70it/s]Packing dataset:  18%|█▊        | 9427/51760 [00:03<00:13, 3131.68it/s]Packing dataset:  19%|█▉        | 9741/51760 [00:03<00:13, 3041.55it/s]Packing dataset:  19%|█▉        | 10048/51760 [00:03<00:13, 3049.67it/s]Packing dataset:  20%|██        | 10359/51760 [00:03<00:13, 3066.29it/s]Packing dataset:  21%|██        | 10682/51760 [00:03<00:13, 3114.45it/s]Packing dataset:  21%|██        | 10994/51760 [00:03<00:13, 3113.68it/s]Packing dataset:  22%|██▏       | 11306/51760 [00:03<00:13, 3076.18it/s]Packing dataset:  22%|██▏       | 11617/51760 [00:03<00:13, 3085.84it/s]Packing dataset:  23%|██▎       | 11926/51760 [00:03<00:12, 3077.29it/s]Packing dataset:  24%|██▎       | 12234/51760 [00:03<00:12, 3062.64it/s]Packing dataset:  24%|██▍       | 12541/51760 [00:04<00:12, 3055.68it/s]Packing dataset:  25%|██▍       | 12849/51760 [00:04<00:12, 3062.59it/s]Packing dataset:  25%|██▌       | 13160/51760 [00:04<00:12, 3076.30it/s]Packing dataset:  26%|██▌       | 13468/51760 [00:04<00:12, 3070.95it/s]Packing dataset:  27%|██▋       | 13776/51760 [00:04<00:12, 3054.55it/s]Packing dataset:  27%|██▋       | 14082/51760 [00:04<00:12, 3020.57it/s]Packing dataset:  28%|██▊       | 14412/51760 [00:04<00:12, 3103.12it/s]Packing dataset:  28%|██▊       | 14723/51760 [00:04<00:12, 3073.34it/s]Packing dataset:  29%|██▉       | 15031/51760 [00:04<00:11, 3066.71it/s]Packing dataset:  30%|██▉       | 15339/51760 [00:04<00:11, 3067.69it/s]Packing dataset:  30%|███       | 15667/51760 [00:05<00:11, 3130.13it/s]Packing dataset:  31%|███       | 15986/51760 [00:05<00:11, 3145.68it/s]Packing dataset:  31%|███▏      | 16301/51760 [00:05<00:11, 3116.95it/s]Packing dataset:  32%|███▏      | 16613/51760 [00:05<00:11, 3070.45it/s]Packing dataset:  33%|███▎      | 16924/51760 [00:05<00:11, 3078.42it/s]Packing dataset:  33%|███▎      | 17232/51760 [00:05<00:11, 3069.00it/s]Packing dataset:  34%|███▍      | 17539/51760 [00:05<00:11, 3024.23it/s]Packing dataset:  34%|███▍      | 17844/51760 [00:05<00:11, 3028.96it/s]Packing dataset:  35%|███▌      | 18148/51760 [00:05<00:11, 3029.58it/s]Packing dataset:  36%|███▌      | 18452/51760 [00:05<00:11, 3013.88it/s]Packing dataset:  36%|███▋      | 18776/51760 [00:06<00:10, 3078.73it/s]Packing dataset:  37%|███▋      | 19084/51760 [00:06<00:10, 3048.94it/s]Packing dataset:  37%|███▋      | 19396/51760 [00:06<00:10, 3068.01it/s]Packing dataset:  38%|███▊      | 19703/51760 [00:06<00:10, 3034.80it/s]Packing dataset:  39%|███▊      | 20007/51760 [00:06<00:10, 3026.27it/s]Packing dataset:  39%|███▉      | 20319/51760 [00:06<00:10, 3051.65it/s]Packing dataset:  40%|███▉      | 20625/51760 [00:06<00:10, 3003.50it/s]Packing dataset:  40%|████      | 20926/51760 [00:06<00:13, 2231.99it/s]Packing dataset:  41%|████      | 21241/51760 [00:07<00:12, 2450.00it/s]Packing dataset:  42%|████▏     | 21541/51760 [00:07<00:11, 2589.46it/s]Packing dataset:  42%|████▏     | 21853/51760 [00:07<00:10, 2728.98it/s]Packing dataset:  43%|████▎     | 22144/51760 [00:07<00:10, 2776.28it/s]Packing dataset:  43%|████▎     | 22455/51760 [00:07<00:10, 2868.17it/s]Packing dataset:  44%|████▍     | 22751/51760 [00:07<00:10, 2870.55it/s]Packing dataset:  45%|████▍     | 23053/51760 [00:07<00:09, 2912.25it/s]Packing dataset:  45%|████▌     | 23366/51760 [00:07<00:09, 2973.96it/s]Packing dataset:  46%|████▌     | 23680/51760 [00:07<00:09, 3021.01it/s]Packing dataset:  46%|████▋     | 23985/51760 [00:07<00:09, 3029.12it/s]Packing dataset:  47%|████▋     | 24290/51760 [00:08<00:09, 3034.15it/s]Packing dataset:  48%|████▊     | 24612/51760 [00:08<00:08, 3088.59it/s]Packing dataset:  48%|████▊     | 24922/51760 [00:08<00:08, 3059.85it/s]Packing dataset:  49%|████▊     | 25229/51760 [00:08<00:08, 3036.21it/s]Packing dataset:  49%|████▉     | 25550/51760 [00:08<00:08, 3084.49it/s]Packing dataset:  50%|████▉     | 25859/51760 [00:08<00:08, 3022.84it/s]Packing dataset:  51%|█████     | 26173/51760 [00:08<00:08, 3053.78it/s]Packing dataset:  51%|█████     | 26479/51760 [00:08<00:08, 3053.89it/s]Packing dataset:  52%|█████▏    | 26785/51760 [00:08<00:08, 3025.08it/s]Packing dataset:  52%|█████▏    | 27112/51760 [00:08<00:07, 3097.00it/s]Packing dataset:  53%|█████▎    | 27422/51760 [00:09<00:07, 3087.26it/s]Packing dataset:  54%|█████▎    | 27731/51760 [00:09<00:07, 3062.36it/s]Packing dataset:  54%|█████▍    | 28038/51760 [00:09<00:07, 3016.91it/s]Packing dataset:  55%|█████▍    | 28342/51760 [00:09<00:07, 3023.13it/s]Packing dataset:  55%|█████▌    | 28645/51760 [00:09<00:07, 2971.64it/s]Packing dataset:  56%|█████▌    | 28946/51760 [00:09<00:07, 2982.27it/s]Packing dataset:  57%|█████▋    | 29246/51760 [00:09<00:07, 2984.83it/s]Packing dataset:  57%|█████▋    | 29552/51760 [00:09<00:07, 3002.84it/s]Packing dataset:  58%|█████▊    | 29853/51760 [00:09<00:07, 2995.31it/s]Packing dataset:  58%|█████▊    | 30160/51760 [00:09<00:07, 3016.35it/s]Packing dataset:  59%|█████▉    | 30462/51760 [00:10<00:07, 2984.98it/s]Packing dataset:  59%|█████▉    | 30766/51760 [00:10<00:06, 3000.32it/s]Packing dataset:  60%|██████    | 31067/51760 [00:10<00:06, 2982.21it/s]Packing dataset:  61%|██████    | 31370/51760 [00:10<00:06, 2995.42it/s]Packing dataset:  61%|██████▏   | 31705/51760 [00:10<00:06, 3100.24it/s]Packing dataset:  62%|██████▏   | 32016/51760 [00:10<00:06, 3072.59it/s]Packing dataset:  62%|██████▏   | 32324/51760 [00:10<00:06, 3054.75it/s]Packing dataset:  63%|██████▎   | 32630/51760 [00:10<00:06, 3045.58it/s]Packing dataset:  64%|██████▎   | 32939/51760 [00:10<00:06, 3056.70it/s]Packing dataset:  64%|██████▍   | 33251/51760 [00:10<00:06, 3075.09it/s]Packing dataset:  65%|██████▍   | 33561/51760 [00:11<00:05, 3079.69it/s]Packing dataset:  65%|██████▌   | 33870/51760 [00:11<00:05, 3060.64it/s]Packing dataset:  66%|██████▌   | 34178/51760 [00:11<00:05, 3065.85it/s]Packing dataset:  67%|██████▋   | 34493/51760 [00:11<00:05, 3090.02it/s]Packing dataset:  67%|██████▋   | 34803/51760 [00:11<00:05, 3085.05it/s]Packing dataset:  68%|██████▊   | 35112/51760 [00:11<00:05, 3043.95it/s]Packing dataset:  68%|██████▊   | 35439/51760 [00:11<00:05, 3109.28it/s]Packing dataset:  69%|██████▉   | 35751/51760 [00:11<00:05, 3041.20it/s]Packing dataset:  70%|██████▉   | 36056/51760 [00:11<00:05, 2993.21it/s]Packing dataset:  70%|███████   | 36364/51760 [00:12<00:05, 3018.50it/s]Packing dataset:  71%|███████   | 36684/51760 [00:12<00:04, 3071.65it/s]Packing dataset:  71%|███████▏  | 36992/51760 [00:12<00:04, 3040.07it/s]Packing dataset:  72%|███████▏  | 37308/51760 [00:12<00:04, 3074.28it/s]Packing dataset:  73%|███████▎  | 37620/51760 [00:12<00:04, 3084.88it/s]Packing dataset:  73%|███████▎  | 37929/51760 [00:12<00:04, 3068.48it/s]Packing dataset:  74%|███████▍  | 38236/51760 [00:12<00:04, 3063.36it/s]Packing dataset:  74%|███████▍  | 38543/51760 [00:12<00:04, 3041.75it/s]Packing dataset:  75%|███████▌  | 38848/51760 [00:12<00:04, 3020.95it/s]Packing dataset:  76%|███████▌  | 39158/51760 [00:12<00:04, 3043.45it/s]Packing dataset:  76%|███████▌  | 39463/51760 [00:13<00:04, 3039.04it/s]Packing dataset:  77%|███████▋  | 39774/51760 [00:13<00:03, 3056.57it/s]Packing dataset:  77%|███████▋  | 40088/51760 [00:13<00:03, 3080.12it/s]Packing dataset:  78%|███████▊  | 40399/51760 [00:13<00:03, 3086.88it/s]Packing dataset:  79%|███████▊  | 40708/51760 [00:13<00:03, 3072.20it/s]Packing dataset:  79%|███████▉  | 41024/51760 [00:13<00:03, 3094.93it/s]Packing dataset:  80%|███████▉  | 41334/51760 [00:13<00:03, 3075.93it/s]Packing dataset:  80%|████████  | 41642/51760 [00:13<00:03, 3056.22it/s]Packing dataset:  81%|████████  | 41948/51760 [00:13<00:03, 3027.68it/s]Packing dataset:  82%|████████▏ | 42251/51760 [00:13<00:03, 3021.55it/s]Packing dataset:  82%|████████▏ | 42554/51760 [00:14<00:03, 3009.86it/s]Packing dataset:  83%|████████▎ | 42856/51760 [00:14<00:02, 2996.52it/s]Packing dataset:  83%|████████▎ | 43156/51760 [00:14<00:02, 2979.11it/s]Packing dataset:  84%|████████▍ | 43459/51760 [00:14<00:02, 2993.83it/s]Packing dataset:  85%|████████▍ | 43771/51760 [00:14<00:02, 3030.38it/s]Packing dataset:  85%|████████▌ | 44075/51760 [00:14<00:02, 3015.19it/s]Packing dataset:  86%|████████▌ | 44389/51760 [00:14<00:02, 3051.72it/s]Packing dataset:  86%|████████▋ | 44695/51760 [00:14<00:02, 3032.23it/s]Packing dataset:  87%|████████▋ | 45002/51760 [00:14<00:02, 3042.00it/s]Packing dataset:  88%|████████▊ | 45319/51760 [00:14<00:02, 3079.12it/s]Packing dataset:  88%|████████▊ | 45629/51760 [00:15<00:01, 3083.00it/s]Packing dataset:  89%|████████▉ | 45938/51760 [00:15<00:01, 3026.95it/s]Packing dataset:  89%|████████▉ | 46246/51760 [00:15<00:01, 3041.73it/s]Packing dataset:  90%|████████▉ | 46551/51760 [00:15<00:01, 3032.39it/s]Packing dataset:  91%|█████████ | 46868/51760 [00:15<00:01, 3071.99it/s]Packing dataset:  91%|█████████ | 47176/51760 [00:15<00:01, 3037.35it/s]Packing dataset:  92%|█████████▏| 47480/51760 [00:15<00:01, 3032.08it/s]Packing dataset:  92%|█████████▏| 47784/51760 [00:15<00:01, 3021.57it/s]Packing dataset:  93%|█████████▎| 48087/51760 [00:15<00:01, 3011.08it/s]Packing dataset:  94%|█████████▎| 48399/51760 [00:15<00:01, 3039.64it/s]Packing dataset:  94%|█████████▍| 48711/51760 [00:16<00:00, 3060.66it/s]Packing dataset:  95%|█████████▍| 49018/51760 [00:16<00:00, 3033.15it/s]Packing dataset:  95%|█████████▌| 49328/51760 [00:16<00:00, 3052.11it/s]Packing dataset:  96%|█████████▌| 49639/51760 [00:16<00:00, 3065.62it/s]Packing dataset:  96%|█████████▋| 49946/51760 [00:16<00:00, 3057.68it/s]Packing dataset:  97%|█████████▋| 50252/51760 [00:16<00:00, 3033.14it/s]Packing dataset:  98%|█████████▊| 50567/51760 [00:16<00:00, 3067.47it/s]Packing dataset:  98%|█████████▊| 50875/51760 [00:16<00:00, 3068.10it/s]Packing dataset:  99%|█████████▉| 51185/51760 [00:16<00:00, 3076.79it/s]Packing dataset:  99%|█████████▉| 51495/51760 [00:16<00:00, 3083.33it/s]Packing dataset: 100%|██████████| 51760/51760 [00:17<00:00, 3036.69it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:626: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:02<00:18,  2.02s/it]1|1|Loss: 2.29964542388916:  10%|█         | 1/10 [00:02<00:18,  2.02s/it]1|1|Loss: 2.29964542388916:  20%|██        | 2/10 [00:02<00:09,  1.23s/it]1|2|Loss: 1.9578802585601807:  20%|██        | 2/10 [00:02<00:09,  1.23s/it]1|2|Loss: 1.9578802585601807:  30%|███       | 3/10 [00:03<00:06,  1.05it/s]1|3|Loss: 3.4749646186828613:  30%|███       | 3/10 [00:03<00:06,  1.05it/s]1|3|Loss: 3.4749646186828613:  40%|████      | 4/10 [00:03<00:05,  1.20it/s]1|4|Loss: 1.8163986206054688:  40%|████      | 4/10 [00:03<00:05,  1.20it/s]1|4|Loss: 1.8163986206054688:  50%|█████     | 5/10 [00:04<00:03,  1.31it/s]1|5|Loss: 2.1782877445220947:  50%|█████     | 5/10 [00:04<00:03,  1.31it/s]1|5|Loss: 2.1782877445220947:  60%|██████    | 6/10 [00:05<00:02,  1.39it/s]1|6|Loss: 2.918851852416992:  60%|██████    | 6/10 [00:05<00:02,  1.39it/s] 1|6|Loss: 2.918851852416992:  70%|███████   | 7/10 [00:05<00:02,  1.44it/s]1|7|Loss: 3.0269110202789307:  70%|███████   | 7/10 [00:05<00:02,  1.44it/s]1|7|Loss: 3.0269110202789307:  80%|████████  | 8/10 [00:06<00:01,  1.49it/s]1|8|Loss: 2.198643445968628:  80%|████████  | 8/10 [00:06<00:01,  1.49it/s] 1|8|Loss: 2.198643445968628:  90%|█████████ | 9/10 [00:07<00:00,  1.51it/s]1|9|Loss: 2.4325225353240967:  90%|█████████ | 9/10 [00:07<00:00,  1.51it/s]1|9|Loss: 2.4325225353240967: 100%|██████████| 10/10 [00:07<00:00,  1.52it/s]1|10|Loss: 2.358106851577759: 100%|██████████| 10/10 [00:07<00:00,  1.52it/s]Starting checkpoint save...
Checkpoint saved in 0.00 seconds.
1|10|Loss: 2.358106851577759: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]
iteration:  1 tokens:  330 time:  2.0206443939823657 tokens_per_second_on_single_device:  163.31
iteration:  2 tokens:  425 time:  0.6618915351573378 tokens_per_second_on_single_device:  642.1
iteration:  3 tokens:  262 time:  0.6187522239051759 tokens_per_second_on_single_device:  423.43
iteration:  4 tokens:  508 time:  0.6541584900114685 tokens_per_second_on_single_device:  776.57
iteration:  5 tokens:  450 time:  0.6467632949352264 tokens_per_second_on_single_device:  695.77
iteration:  6 tokens:  293 time:  0.6249077192042023 tokens_per_second_on_single_device:  468.87
iteration:  7 tokens:  426 time:  0.6448248990345746 tokens_per_second_on_single_device:  660.64
iteration:  8 tokens:  307 time:  0.6248974138870835 tokens_per_second_on_single_device:  491.28
iteration:  9 tokens:  382 time:  0.63623739592731 tokens_per_second_on_single_device:  600.4
iteration:  10 tokens:  454 time:  0.6522133739199489 tokens_per_second_on_single_device:  696.09
avg tokens_per_second_on_single_device:  603.99
