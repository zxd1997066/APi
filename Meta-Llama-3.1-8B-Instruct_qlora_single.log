Running LoRAFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
low_cpu_ram: false
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/logs
model:
  _component_: torchtune.models.llama3_1.qlora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/qlora_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:436: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 6.18 GiB
	XPU peak memory alloc: 6.18 GiB
	XPU peak memory reserved: 6.60 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/qlora_single_device/logs/log_1754234313.txt
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 161/51760 [00:00<00:32, 1600.34it/s]Packing dataset:   1%|          | 352/51760 [00:00<00:28, 1779.95it/s]Packing dataset:   1%|          | 542/51760 [00:00<00:27, 1831.85it/s]Packing dataset:   1%|▏         | 726/51760 [00:00<00:28, 1811.00it/s]Packing dataset:   2%|▏         | 908/51760 [00:00<00:28, 1773.89it/s]Packing dataset:   2%|▏         | 1104/51760 [00:00<00:27, 1831.56it/s]Packing dataset:   3%|▎         | 1298/51760 [00:00<00:27, 1863.97it/s]Packing dataset:   3%|▎         | 1485/51760 [00:00<00:27, 1847.50it/s]Packing dataset:   3%|▎         | 1670/51760 [00:00<00:28, 1783.92it/s]Packing dataset:   4%|▎         | 1849/51760 [00:01<00:28, 1764.43it/s]Packing dataset:   4%|▍         | 2029/51760 [00:01<00:28, 1772.83it/s]Packing dataset:   4%|▍         | 2232/51760 [00:01<00:26, 1847.33it/s]Packing dataset:   5%|▍         | 2428/51760 [00:01<00:26, 1878.76it/s]Packing dataset:   5%|▌         | 2617/51760 [00:01<00:26, 1875.13it/s]Packing dataset:   5%|▌         | 2825/51760 [00:01<00:25, 1934.35it/s]Packing dataset:   6%|▌         | 3019/51760 [00:01<00:25, 1903.27it/s]Packing dataset:   6%|▌         | 3211/51760 [00:01<00:25, 1906.81it/s]Packing dataset:   7%|▋         | 3402/51760 [00:01<00:25, 1867.95it/s]Packing dataset:   7%|▋         | 3590/51760 [00:01<00:25, 1858.15it/s]Packing dataset:   7%|▋         | 3782/51760 [00:02<00:25, 1870.18it/s]Packing dataset:   8%|▊         | 3970/51760 [00:02<00:25, 1860.85it/s]Packing dataset:   8%|▊         | 4157/51760 [00:02<00:26, 1830.63it/s]Packing dataset:   8%|▊         | 4359/51760 [00:02<00:25, 1885.01it/s]Packing dataset:   9%|▉         | 4548/51760 [00:02<00:25, 1826.03it/s]Packing dataset:   9%|▉         | 4732/51760 [00:02<00:25, 1821.96it/s]Packing dataset:   9%|▉         | 4916/51760 [00:02<00:25, 1823.11it/s]Packing dataset:  10%|▉         | 5106/51760 [00:02<00:25, 1845.36it/s]Packing dataset:  10%|█         | 5291/51760 [00:02<00:25, 1816.50it/s]Packing dataset:  11%|█         | 5487/51760 [00:02<00:24, 1854.23it/s]Packing dataset:  11%|█         | 5677/51760 [00:03<00:24, 1866.78it/s]Packing dataset:  11%|█▏        | 5866/51760 [00:03<00:24, 1873.36it/s]Packing dataset:  12%|█▏        | 6054/51760 [00:03<00:24, 1865.94it/s]Packing dataset:  12%|█▏        | 6241/51760 [00:03<00:24, 1843.37it/s]Packing dataset:  12%|█▏        | 6426/51760 [00:03<00:24, 1834.71it/s]Packing dataset:  13%|█▎        | 6610/51760 [00:03<00:24, 1807.75it/s]Packing dataset:  13%|█▎        | 6791/51760 [00:03<00:25, 1797.54it/s]Packing dataset:  13%|█▎        | 6974/51760 [00:03<00:24, 1802.07it/s]Packing dataset:  14%|█▍        | 7156/51760 [00:03<00:24, 1803.41it/s]Packing dataset:  14%|█▍        | 7344/51760 [00:03<00:24, 1824.82it/s]Packing dataset:  15%|█▍        | 7527/51760 [00:04<00:24, 1806.93it/s]Packing dataset:  15%|█▍        | 7726/51760 [00:04<00:23, 1854.65it/s]Packing dataset:  15%|█▌        | 7912/51760 [00:04<00:23, 1839.76it/s]Packing dataset:  16%|█▌        | 8097/51760 [00:04<00:24, 1776.90it/s]Packing dataset:  16%|█▌        | 8276/51760 [00:04<00:24, 1773.34it/s]Packing dataset:  16%|█▋        | 8456/51760 [00:04<00:24, 1779.10it/s]Packing dataset:  17%|█▋        | 8645/51760 [00:04<00:23, 1811.51it/s]Packing dataset:  17%|█▋        | 8844/51760 [00:04<00:23, 1861.03it/s]Packing dataset:  18%|█▊        | 9117/51760 [00:04<00:20, 2118.08it/s]Packing dataset:  18%|█▊        | 9423/51760 [00:05<00:17, 2396.86it/s]Packing dataset:  19%|█▊        | 9693/51760 [00:05<00:16, 2483.90it/s]Packing dataset:  19%|█▉        | 9992/51760 [00:05<00:15, 2634.26it/s]Packing dataset:  20%|█▉        | 10300/51760 [00:05<00:14, 2766.63it/s]Packing dataset:  20%|██        | 10606/51760 [00:05<00:14, 2853.95it/s]Packing dataset:  21%|██        | 10911/51760 [00:05<00:14, 2912.00it/s]Packing dataset:  22%|██▏       | 11204/51760 [00:05<00:13, 2916.73it/s]Packing dataset:  22%|██▏       | 11509/51760 [00:05<00:13, 2956.26it/s]Packing dataset:  23%|██▎       | 11813/51760 [00:05<00:13, 2978.56it/s]Packing dataset:  23%|██▎       | 12111/51760 [00:05<00:13, 2963.90it/s]Packing dataset:  24%|██▍       | 12411/51760 [00:06<00:13, 2971.73it/s]Packing dataset:  25%|██▍       | 12715/51760 [00:06<00:13, 2989.93it/s]Packing dataset:  25%|██▌       | 13015/51760 [00:06<00:13, 2945.41it/s]Packing dataset:  26%|██▌       | 13319/51760 [00:06<00:12, 2971.04it/s]Packing dataset:  26%|██▋       | 13617/51760 [00:06<00:12, 2963.81it/s]Packing dataset:  27%|██▋       | 13921/51760 [00:06<00:12, 2986.19it/s]Packing dataset:  27%|██▋       | 14220/51760 [00:06<00:12, 2974.37it/s]Packing dataset:  28%|██▊       | 14534/51760 [00:06<00:12, 3021.51it/s]Packing dataset:  29%|██▊       | 14837/51760 [00:06<00:12, 2996.22it/s]Packing dataset:  29%|██▉       | 15142/51760 [00:06<00:12, 3010.55it/s]Packing dataset:  30%|██▉       | 15445/51760 [00:07<00:12, 3012.85it/s]Packing dataset:  30%|███       | 15748/51760 [00:07<00:11, 3017.81it/s]Packing dataset:  31%|███       | 16071/51760 [00:07<00:11, 3074.82it/s]Packing dataset:  32%|███▏      | 16379/51760 [00:07<00:11, 3005.69it/s]Packing dataset:  32%|███▏      | 16680/51760 [00:07<00:11, 3001.13it/s]Packing dataset:  33%|███▎      | 16981/51760 [00:07<00:11, 2974.69it/s]Packing dataset:  33%|███▎      | 17280/51760 [00:07<00:11, 2975.61it/s]Packing dataset:  34%|███▍      | 17578/51760 [00:07<00:11, 2942.90it/s]Packing dataset:  35%|███▍      | 17873/51760 [00:07<00:11, 2926.15it/s]Packing dataset:  35%|███▌      | 18167/51760 [00:07<00:11, 2930.00it/s]Packing dataset:  36%|███▌      | 18461/51760 [00:08<00:11, 2906.85it/s]Packing dataset:  36%|███▋      | 18778/51760 [00:08<00:11, 2983.99it/s]Packing dataset:  37%|███▋      | 19077/51760 [00:08<00:11, 2946.47it/s]Packing dataset:  37%|███▋      | 19384/51760 [00:08<00:10, 2979.49it/s]Packing dataset:  38%|███▊      | 19683/51760 [00:08<00:10, 2924.29it/s]Packing dataset:  39%|███▊      | 19984/51760 [00:08<00:10, 2948.85it/s]Packing dataset:  39%|███▉      | 20281/51760 [00:08<00:10, 2954.46it/s]Packing dataset:  40%|███▉      | 20577/51760 [00:08<00:10, 2893.00it/s]Packing dataset:  40%|████      | 20889/51760 [00:08<00:10, 2957.57it/s]Packing dataset:  41%|████      | 21212/51760 [00:08<00:10, 3033.75it/s]Packing dataset:  42%|████▏     | 21516/51760 [00:09<00:10, 2987.27it/s]Packing dataset:  42%|████▏     | 21821/51760 [00:09<00:09, 3004.19it/s]Packing dataset:  43%|████▎     | 22122/51760 [00:09<00:10, 2944.68it/s]Packing dataset:  43%|████▎     | 22428/51760 [00:09<00:09, 2977.45it/s]Packing dataset:  44%|████▍     | 22727/51760 [00:09<00:09, 2924.97it/s]Packing dataset:  44%|████▍     | 23020/51760 [00:09<00:09, 2924.96it/s]Packing dataset:  45%|████▌     | 23320/51760 [00:09<00:09, 2946.97it/s]Packing dataset:  46%|████▌     | 23638/51760 [00:09<00:09, 3013.67it/s]Packing dataset:  46%|████▋     | 23940/51760 [00:09<00:09, 2979.66it/s]Packing dataset:  47%|████▋     | 24243/51760 [00:10<00:09, 2993.20it/s]Packing dataset:  47%|████▋     | 24562/51760 [00:10<00:08, 3047.17it/s]Packing dataset:  48%|████▊     | 24867/51760 [00:10<00:08, 2999.55it/s]Packing dataset:  49%|████▊     | 25168/51760 [00:10<00:09, 2953.10it/s]Packing dataset:  49%|████▉     | 25487/51760 [00:10<00:08, 3018.35it/s]Packing dataset:  50%|████▉     | 25790/51760 [00:10<00:08, 2914.33it/s]Packing dataset:  50%|█████     | 26094/51760 [00:10<00:08, 2950.40it/s]Packing dataset:  51%|█████     | 26390/51760 [00:10<00:08, 2943.94it/s]Packing dataset:  52%|█████▏    | 26685/51760 [00:10<00:08, 2922.85it/s]Packing dataset:  52%|█████▏    | 27001/51760 [00:10<00:08, 2991.90it/s]Packing dataset:  53%|█████▎    | 27313/51760 [00:11<00:08, 3028.89it/s]Packing dataset:  53%|█████▎    | 27617/51760 [00:11<00:08, 2974.77it/s]Packing dataset:  54%|█████▍    | 27915/51760 [00:11<00:08, 2952.20it/s]Packing dataset:  55%|█████▍    | 28211/51760 [00:11<00:08, 2881.69it/s]Packing dataset:  55%|█████▌    | 28500/51760 [00:11<00:08, 2852.05it/s]Packing dataset:  56%|█████▌    | 28800/51760 [00:11<00:07, 2894.68it/s]Packing dataset:  56%|█████▌    | 29090/51760 [00:11<00:07, 2877.52it/s]Packing dataset:  57%|█████▋    | 29378/51760 [00:11<00:10, 2084.82it/s]Packing dataset:  57%|█████▋    | 29666/51760 [00:11<00:09, 2269.18it/s]Packing dataset:  58%|█████▊    | 29950/51760 [00:12<00:09, 2411.35it/s]Packing dataset:  58%|█████▊    | 30246/51760 [00:12<00:08, 2555.33it/s]Packing dataset:  59%|█████▉    | 30521/51760 [00:12<00:08, 2607.48it/s]Packing dataset:  60%|█████▉    | 30819/51760 [00:12<00:07, 2709.65it/s]Packing dataset:  60%|██████    | 31104/51760 [00:12<00:07, 2747.62it/s]Packing dataset:  61%|██████    | 31388/51760 [00:12<00:07, 2772.60it/s]Packing dataset:  61%|██████▏   | 31720/51760 [00:12<00:06, 2931.70it/s]Packing dataset:  62%|██████▏   | 32017/51760 [00:12<00:06, 2909.22it/s]Packing dataset:  62%|██████▏   | 32311/51760 [00:12<00:06, 2904.73it/s]Packing dataset:  63%|██████▎   | 32607/51760 [00:12<00:06, 2919.42it/s]Packing dataset:  64%|██████▎   | 32914/51760 [00:13<00:06, 2957.85it/s]Packing dataset:  64%|██████▍   | 33211/51760 [00:13<00:06, 2958.07it/s]Packing dataset:  65%|██████▍   | 33519/51760 [00:13<00:06, 2993.55it/s]Packing dataset:  65%|██████▌   | 33819/51760 [00:13<00:06, 2971.90it/s]Packing dataset:  66%|██████▌   | 34120/51760 [00:13<00:05, 2979.79it/s]Packing dataset:  66%|██████▋   | 34419/51760 [00:13<00:05, 2938.77it/s]Packing dataset:  67%|██████▋   | 34727/51760 [00:13<00:05, 2979.61it/s]Packing dataset:  68%|██████▊   | 35026/51760 [00:13<00:05, 2972.72it/s]Packing dataset:  68%|██████▊   | 35326/51760 [00:13<00:05, 2978.26it/s]Packing dataset:  69%|██████▉   | 35624/51760 [00:14<00:05, 2948.73it/s]Packing dataset:  69%|██████▉   | 35920/51760 [00:14<00:05, 2904.54it/s]Packing dataset:  70%|██████▉   | 36211/51760 [00:14<00:05, 2892.33it/s]Packing dataset:  71%|███████   | 36517/51760 [00:14<00:05, 2937.72it/s]Packing dataset:  71%|███████   | 36823/51760 [00:14<00:05, 2972.37it/s]Packing dataset:  72%|███████▏  | 37121/51760 [00:14<00:04, 2962.35it/s]Packing dataset:  72%|███████▏  | 37418/51760 [00:14<00:04, 2942.70it/s]Packing dataset:  73%|███████▎  | 37723/51760 [00:14<00:04, 2969.43it/s]Packing dataset:  73%|███████▎  | 38021/51760 [00:14<00:04, 2966.86it/s]Packing dataset:  74%|███████▍  | 38321/51760 [00:14<00:04, 2976.05it/s]Packing dataset:  75%|███████▍  | 38619/51760 [00:15<00:04, 2924.98it/s]Packing dataset:  75%|███████▌  | 38917/51760 [00:15<00:04, 2936.75it/s]Packing dataset:  76%|███████▌  | 39223/51760 [00:15<00:04, 2971.03it/s]Packing dataset:  76%|███████▋  | 39521/51760 [00:15<00:04, 2946.88it/s]Packing dataset:  77%|███████▋  | 39823/51760 [00:15<00:04, 2968.40it/s]Packing dataset:  78%|███████▊  | 40120/51760 [00:15<00:03, 2927.77it/s]Packing dataset:  78%|███████▊  | 40424/51760 [00:15<00:03, 2958.74it/s]Packing dataset:  79%|███████▊  | 40726/51760 [00:15<00:03, 2975.62it/s]Packing dataset:  79%|███████▉  | 41026/51760 [00:15<00:03, 2980.45it/s]Packing dataset:  80%|███████▉  | 41326/51760 [00:15<00:03, 2981.29it/s]Packing dataset:  80%|████████  | 41625/51760 [00:16<00:03, 2937.78it/s]Packing dataset:  81%|████████  | 41919/51760 [00:16<00:03, 2912.87it/s]Packing dataset:  82%|████████▏ | 42211/51760 [00:16<00:03, 2893.78it/s]Packing dataset:  82%|████████▏ | 42501/51760 [00:16<00:03, 2893.98it/s]Packing dataset:  83%|████████▎ | 42791/51760 [00:16<00:03, 2846.16it/s]Packing dataset:  83%|████████▎ | 43076/51760 [00:16<00:03, 2822.93it/s]Packing dataset:  84%|████████▍ | 43359/51760 [00:16<00:02, 2824.86it/s]Packing dataset:  84%|████████▍ | 43656/51760 [00:16<00:02, 2866.03it/s]Packing dataset:  85%|████████▍ | 43943/51760 [00:16<00:02, 2857.95it/s]Packing dataset:  85%|████████▌ | 44232/51760 [00:16<00:02, 2865.51it/s]Packing dataset:  86%|████████▌ | 44530/51760 [00:17<00:02, 2898.23it/s]Packing dataset:  87%|████████▋ | 44823/51760 [00:17<00:02, 2906.69it/s]Packing dataset:  87%|████████▋ | 45128/51760 [00:17<00:02, 2947.66it/s]Packing dataset:  88%|████████▊ | 45433/51760 [00:17<00:02, 2975.92it/s]Packing dataset:  88%|████████▊ | 45731/51760 [00:17<00:02, 2928.42it/s]Packing dataset:  89%|████████▉ | 46025/51760 [00:17<00:01, 2885.10it/s]Packing dataset:  90%|████████▉ | 46330/51760 [00:17<00:01, 2931.59it/s]Packing dataset:  90%|█████████ | 46624/51760 [00:17<00:01, 2915.43it/s]Packing dataset:  91%|█████████ | 46928/51760 [00:17<00:01, 2949.83it/s]Packing dataset:  91%|█████████ | 47224/51760 [00:17<00:01, 2914.69it/s]Packing dataset:  92%|█████████▏| 47530/51760 [00:18<00:01, 2956.78it/s]Packing dataset:  92%|█████████▏| 47826/51760 [00:18<00:01, 2927.54it/s]Packing dataset:  93%|█████████▎| 48119/51760 [00:18<00:01, 2876.98it/s]Packing dataset:  94%|█████████▎| 48421/51760 [00:18<00:01, 2918.27it/s]Packing dataset:  94%|█████████▍| 48726/51760 [00:18<00:01, 2956.87it/s]Packing dataset:  95%|█████████▍| 49022/51760 [00:18<00:00, 2936.90it/s]Packing dataset:  95%|█████████▌| 49321/51760 [00:18<00:00, 2951.00it/s]Packing dataset:  96%|█████████▌| 49617/51760 [00:18<00:00, 2945.38it/s]Packing dataset:  96%|█████████▋| 49912/51760 [00:18<00:00, 2934.66it/s]Packing dataset:  97%|█████████▋| 50208/51760 [00:18<00:00, 2941.92it/s]Packing dataset:  98%|█████████▊| 50503/51760 [00:19<00:00, 2935.57it/s]Packing dataset:  98%|█████████▊| 50811/51760 [00:19<00:00, 2978.46it/s]Packing dataset:  99%|█████████▊| 51112/51760 [00:19<00:00, 2985.79it/s]Packing dataset:  99%|█████████▉| 51411/51760 [00:19<00:00, 2965.94it/s]Packing dataset: 100%|█████████▉| 51715/51760 [00:19<00:00, 2983.72it/s]Packing dataset: 100%|██████████| 51760/51760 [00:19<00:00, 2653.42it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:626: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:23<03:30, 23.44s/it]1|1|Loss: 1.7549535036087036:  10%|█         | 1/10 [00:23<03:30, 23.44s/it]1|1|Loss: 1.7549535036087036:  20%|██        | 2/10 [00:45<03:00, 22.59s/it]1|2|Loss: 1.691725730895996:  20%|██        | 2/10 [00:45<03:00, 22.59s/it] 1|2|Loss: 1.691725730895996:  30%|███       | 3/10 [01:07<02:36, 22.37s/it]1|3|Loss: 1.8272643089294434:  30%|███       | 3/10 [01:07<02:36, 22.37s/it]1|3|Loss: 1.8272643089294434:  40%|████      | 4/10 [01:29<02:13, 22.21s/it]1|4|Loss: 1.883032202720642:  40%|████      | 4/10 [01:29<02:13, 22.21s/it] 1|4|Loss: 1.883032202720642:  50%|█████     | 5/10 [01:51<01:50, 22.11s/it]1|5|Loss: 1.6286770105361938:  50%|█████     | 5/10 [01:51<01:50, 22.11s/it]1|5|Loss: 1.6286770105361938:  60%|██████    | 6/10 [02:13<01:28, 22.08s/it]1|6|Loss: 1.630145788192749:  60%|██████    | 6/10 [02:13<01:28, 22.08s/it] 1|6|Loss: 1.630145788192749:  70%|███████   | 7/10 [02:35<01:06, 22.06s/it]1|7|Loss: 1.702297329902649:  70%|███████   | 7/10 [02:35<01:06, 22.06s/it]1|7|Loss: 1.702297329902649:  80%|████████  | 8/10 [02:57<00:44, 22.06s/it]1|8|Loss: 1.7116625308990479:  80%|████████  | 8/10 [02:57<00:44, 22.06s/it]1|8|Loss: 1.7116625308990479:  90%|█████████ | 9/10 [03:19<00:22, 22.07s/it]1|9|Loss: 1.7064359188079834:  90%|█████████ | 9/10 [03:19<00:22, 22.07s/it]1|9|Loss: 1.7064359188079834: 100%|██████████| 10/10 [03:41<00:00, 22.08s/it]1|10|Loss: 1.7847598791122437: 100%|██████████| 10/10 [03:41<00:00, 22.08s/it]Starting checkpoint save...
Checkpoint saved in 0.00 seconds.
1|10|Loss: 1.7847598791122437: 100%|██████████| 10/10 [03:41<00:00, 22.18s/it]
iteration:  1 tokens:  6046 time:  23.44345563510433 tokens_per_second_on_single_device:  257.9
iteration:  2 tokens:  5669 time:  21.977764137089252 tokens_per_second_on_single_device:  257.94
iteration:  3 tokens:  6522 time:  22.082204923965037 tokens_per_second_on_single_device:  295.35
iteration:  4 tokens:  5524 time:  21.93247123598121 tokens_per_second_on_single_device:  251.86
iteration:  5 tokens:  5683 time:  21.921855431981385 tokens_per_second_on_single_device:  259.24
iteration:  6 tokens:  5929 time:  21.992581337923184 tokens_per_second_on_single_device:  269.59
iteration:  7 tokens:  6404 time:  22.016340864123777 tokens_per_second_on_single_device:  290.87
iteration:  8 tokens:  6295 time:  22.048612616956234 tokens_per_second_on_single_device:  285.51
iteration:  9 tokens:  6725 time:  22.075459541985765 tokens_per_second_on_single_device:  304.64
iteration:  10 tokens:  6521 time:  22.060070862062275 tokens_per_second_on_single_device:  295.6
avg tokens_per_second_on_single_device:  281.63
