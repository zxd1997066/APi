Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1760345924.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   1%|          | 420/52002 [00:00<00:12, 4198.71it/s]Packing dataset:   2%|▏         | 886/52002 [00:00<00:11, 4467.68it/s]Packing dataset:   3%|▎         | 1370/52002 [00:00<00:10, 4633.06it/s]Packing dataset:   4%|▎         | 1836/52002 [00:00<00:10, 4637.84it/s]Packing dataset:   4%|▍         | 2319/52002 [00:00<00:10, 4703.96it/s]Packing dataset:   5%|▌         | 2800/52002 [00:00<00:10, 4737.08it/s]Packing dataset:   6%|▋         | 3274/52002 [00:00<00:10, 4732.40it/s]Packing dataset:   7%|▋         | 3748/52002 [00:00<00:10, 4683.27it/s]Packing dataset:   8%|▊         | 4217/52002 [00:00<00:10, 4645.64it/s]Packing dataset:   9%|▉         | 4692/52002 [00:01<00:10, 4675.72it/s]Packing dataset:  10%|▉         | 5166/52002 [00:01<00:09, 4693.79it/s]Packing dataset:  11%|█         | 5649/52002 [00:01<00:09, 4734.61it/s]Packing dataset:  12%|█▏        | 6123/52002 [00:01<00:09, 4668.51it/s]Packing dataset:  13%|█▎        | 6591/52002 [00:01<00:09, 4634.65it/s]Packing dataset:  14%|█▎        | 7055/52002 [00:01<00:09, 4567.66it/s]Packing dataset:  14%|█▍        | 7513/52002 [00:01<00:09, 4544.11it/s]Packing dataset:  15%|█▌        | 7971/52002 [00:01<00:09, 4553.33it/s]Packing dataset:  16%|█▌        | 8427/52002 [00:01<00:09, 4545.22it/s]Packing dataset:  17%|█▋        | 8918/52002 [00:01<00:09, 4653.22it/s]Packing dataset:  18%|█▊        | 9385/52002 [00:02<00:09, 4655.05it/s]Packing dataset:  19%|█▉        | 9851/52002 [00:02<00:09, 4609.99it/s]Packing dataset:  20%|█▉        | 10316/52002 [00:02<00:09, 4621.52it/s]Packing dataset:  21%|██        | 10785/52002 [00:02<00:08, 4639.56it/s]Packing dataset:  22%|██▏       | 11250/52002 [00:02<00:08, 4593.59it/s]Packing dataset:  23%|██▎       | 11715/52002 [00:02<00:08, 4609.54it/s]Packing dataset:  23%|██▎       | 12177/52002 [00:02<00:08, 4584.11it/s]Packing dataset:  24%|██▍       | 12642/52002 [00:02<00:08, 4602.54it/s]Packing dataset:  25%|██▌       | 13103/52002 [00:02<00:08, 4601.44it/s]Packing dataset:  26%|██▌       | 13576/52002 [00:02<00:08, 4638.10it/s]Packing dataset:  27%|██▋       | 14040/52002 [00:03<00:08, 4577.11it/s]Packing dataset:  28%|██▊       | 14504/52002 [00:03<00:08, 4594.85it/s]Packing dataset:  29%|██▉       | 14964/52002 [00:03<00:08, 4585.08it/s]Packing dataset:  30%|██▉       | 15423/52002 [00:03<00:07, 4573.25it/s]Packing dataset:  31%|███       | 15881/52002 [00:03<00:07, 4558.75it/s]Packing dataset:  31%|███▏      | 16349/52002 [00:03<00:07, 4593.11it/s]Packing dataset:  32%|███▏      | 16823/52002 [00:03<00:07, 4635.75it/s]Packing dataset:  33%|███▎      | 17287/52002 [00:03<00:07, 4636.40it/s]Packing dataset:  34%|███▍      | 17759/52002 [00:03<00:07, 4659.98it/s]Packing dataset:  35%|███▌      | 18226/52002 [00:03<00:07, 4611.42it/s]Packing dataset:  36%|███▌      | 18688/52002 [00:04<00:07, 4533.63it/s]Packing dataset:  37%|███▋      | 19153/52002 [00:04<00:07, 4566.43it/s]Packing dataset:  38%|███▊      | 19611/52002 [00:04<00:07, 4569.58it/s]Packing dataset:  39%|███▊      | 20077/52002 [00:04<00:06, 4596.16it/s]Packing dataset:  39%|███▉      | 20537/52002 [00:04<00:06, 4583.74it/s]Packing dataset:  40%|████      | 21024/52002 [00:04<00:06, 4667.12it/s]Packing dataset:  41%|████▏     | 21505/52002 [00:04<00:06, 4708.42it/s]Packing dataset:  42%|████▏     | 21976/52002 [00:04<00:06, 4672.87it/s]Packing dataset:  43%|████▎     | 22444/52002 [00:04<00:06, 4660.45it/s]Packing dataset:  44%|████▍     | 22911/52002 [00:04<00:06, 4593.98it/s]Packing dataset:  45%|████▍     | 23385/52002 [00:05<00:06, 4634.92it/s]Packing dataset:  46%|████▌     | 23858/52002 [00:05<00:06, 4662.07it/s]Packing dataset:  47%|████▋     | 24325/52002 [00:05<00:05, 4643.17it/s]Packing dataset:  48%|████▊     | 24790/52002 [00:05<00:05, 4633.12it/s]Packing dataset:  49%|████▊     | 25256/52002 [00:05<00:05, 4638.40it/s]Packing dataset:  49%|████▉     | 25720/52002 [00:05<00:05, 4622.67it/s]Packing dataset:  50%|█████     | 26183/52002 [00:05<00:05, 4609.03it/s]Packing dataset:  51%|█████     | 26644/52002 [00:05<00:05, 4604.99it/s]Packing dataset:  52%|█████▏    | 27105/52002 [00:05<00:05, 4569.39it/s]Packing dataset:  53%|█████▎    | 27573/52002 [00:05<00:05, 4602.13it/s]Packing dataset:  54%|█████▍    | 28034/52002 [00:06<00:05, 4547.98it/s]Packing dataset:  55%|█████▍    | 28489/52002 [00:06<00:05, 4519.36it/s]Packing dataset:  56%|█████▌    | 28944/52002 [00:06<00:05, 4526.88it/s]Packing dataset:  57%|█████▋    | 29410/52002 [00:06<00:04, 4565.66it/s]Packing dataset:  57%|█████▋    | 29867/52002 [00:06<00:04, 4544.16it/s]Packing dataset:  58%|█████▊    | 30330/52002 [00:06<00:04, 4569.23it/s]Packing dataset:  59%|█████▉    | 30788/52002 [00:06<00:04, 4516.79it/s]Packing dataset:  60%|██████    | 31242/52002 [00:06<00:04, 4523.26it/s]Packing dataset:  61%|██████    | 31708/52002 [00:06<00:04, 4563.71it/s]Packing dataset:  62%|██████▏   | 32197/52002 [00:06<00:04, 4659.14it/s]Packing dataset:  63%|██████▎   | 32664/52002 [00:07<00:04, 4597.48it/s]Packing dataset:  64%|██████▎   | 33125/52002 [00:07<00:04, 4599.45it/s]Packing dataset:  65%|██████▍   | 33592/52002 [00:07<00:03, 4618.24it/s]Packing dataset:  65%|██████▌   | 34054/52002 [00:07<00:03, 4589.91it/s]Packing dataset:  66%|██████▋   | 34514/52002 [00:07<00:03, 4588.04it/s]Packing dataset:  67%|██████▋   | 34973/52002 [00:07<00:03, 4546.86it/s]Packing dataset:  68%|██████▊   | 35436/52002 [00:07<00:03, 4571.20it/s]Packing dataset:  69%|██████▉   | 35896/52002 [00:07<00:03, 4576.82it/s]Packing dataset:  70%|██████▉   | 36354/52002 [00:07<00:03, 4494.87it/s]Packing dataset:  71%|███████   | 36814/52002 [00:07<00:03, 4524.60it/s]Packing dataset:  72%|███████▏  | 37301/52002 [00:08<00:03, 4624.82it/s]Packing dataset:  73%|███████▎  | 37764/52002 [00:08<00:03, 4588.80it/s]Packing dataset:  74%|███████▎  | 38235/52002 [00:08<00:02, 4622.32it/s]Packing dataset:  74%|███████▍  | 38699/52002 [00:08<00:02, 4625.83it/s]Packing dataset:  75%|███████▌  | 39162/52002 [00:08<00:02, 4578.17it/s]Packing dataset:  76%|███████▌  | 39624/52002 [00:08<00:02, 4589.93it/s]Packing dataset:  77%|███████▋  | 40084/52002 [00:08<00:02, 4590.08it/s]Packing dataset:  78%|███████▊  | 40544/52002 [00:08<00:02, 4584.81it/s]Packing dataset:  79%|███████▉  | 41012/52002 [00:08<00:02, 4611.69it/s]Packing dataset:  80%|███████▉  | 41474/52002 [00:09<00:02, 4600.26it/s]Packing dataset:  81%|████████  | 41935/52002 [00:09<00:02, 4562.37it/s]Packing dataset:  82%|████████▏ | 42392/52002 [00:09<00:02, 4544.82it/s]Packing dataset:  82%|████████▏ | 42847/52002 [00:09<00:02, 4522.04it/s]Packing dataset:  83%|████████▎ | 43300/52002 [00:09<00:01, 4521.42it/s]Packing dataset:  84%|████████▍ | 43753/52002 [00:09<00:01, 4513.35it/s]Packing dataset:  85%|████████▌ | 44209/52002 [00:09<00:01, 4525.22it/s]Packing dataset:  86%|████████▌ | 44676/52002 [00:09<00:01, 4567.38it/s]Packing dataset:  87%|████████▋ | 45134/52002 [00:09<00:01, 4570.78it/s]Packing dataset:  88%|████████▊ | 45604/52002 [00:09<00:01, 4607.88it/s]Packing dataset:  89%|████████▊ | 46065/52002 [00:10<00:01, 4579.96it/s]Packing dataset:  89%|████████▉ | 46524/52002 [00:10<00:01, 4577.62it/s]Packing dataset:  90%|█████████ | 46982/52002 [00:10<00:01, 4556.72it/s]Packing dataset:  91%|█████████ | 47438/52002 [00:10<00:01, 4551.46it/s]Packing dataset:  92%|█████████▏| 47894/52002 [00:10<00:00, 4507.49it/s]Packing dataset:  93%|█████████▎| 48356/52002 [00:10<00:00, 4540.66it/s]Packing dataset:  94%|█████████▍| 48811/52002 [00:10<00:00, 4525.92it/s]Packing dataset:  95%|█████████▍| 49268/52002 [00:10<00:00, 4538.33it/s]Packing dataset:  96%|█████████▌| 49722/52002 [00:10<00:00, 4517.09it/s]Packing dataset:  96%|█████████▋| 50177/52002 [00:10<00:00, 4525.63it/s]Packing dataset:  97%|█████████▋| 50630/52002 [00:11<00:00, 4507.44it/s]Packing dataset:  98%|█████████▊| 51090/52002 [00:11<00:00, 4534.52it/s]Packing dataset:  99%|█████████▉| 51546/52002 [00:11<00:00, 4541.58it/s]Packing dataset: 100%|█████████▉| 52001/52002 [00:11<00:00, 4541.13it/s]Packing dataset: 100%|██████████| 52002/52002 [00:11<00:00, 4590.18it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [01:13<10:59, 73.28s/it]1|1|Loss: 1.6737:  10%|█         | 1/10 [01:13<10:59, 73.28s/it]1|1|Loss: 1.6737:  20%|██        | 2/10 [01:33<05:35, 41.97s/it]1|2|Loss: 1.3810:  20%|██        | 2/10 [01:33<05:35, 41.97s/it]1|2|Loss: 1.3810:  30%|███       | 3/10 [01:53<03:43, 31.98s/it]1|3|Loss: 1.2921:  30%|███       | 3/10 [01:53<03:43, 31.98s/it]1|3|Loss: 1.2921:  40%|████      | 4/10 [02:13<02:43, 27.24s/it]1|4|Loss: 1.1811:  40%|████      | 4/10 [02:13<02:43, 27.24s/it]1|4|Loss: 1.1811:  50%|█████     | 5/10 [02:33<02:03, 24.64s/it]1|5|Loss: 0.9936:  50%|█████     | 5/10 [02:33<02:03, 24.64s/it]1|5|Loss: 0.9936:  60%|██████    | 6/10 [02:53<01:32, 23.10s/it]1|6|Loss: 1.0010:  60%|██████    | 6/10 [02:53<01:32, 23.10s/it]1|6|Loss: 1.0010:  70%|███████   | 7/10 [03:13<01:06, 22.06s/it]1|7|Loss: 0.9840:  70%|███████   | 7/10 [03:13<01:06, 22.06s/it]1|7|Loss: 0.9840:  80%|████████  | 8/10 [03:33<00:42, 21.41s/it]1|8|Loss: 1.1850:  80%|████████  | 8/10 [03:33<00:42, 21.41s/it]1|8|Loss: 1.1850:  90%|█████████ | 9/10 [03:53<00:20, 20.97s/it]1|9|Loss: 0.9444:  90%|█████████ | 9/10 [03:53<00:20, 20.97s/it]1|9|Loss: 0.9444: 100%|██████████| 10/10 [04:13<00:00, 20.66s/it]1|10|Loss: 0.7476: 100%|██████████| 10/10 [04:13<00:00, 20.66s/it]1|10|Loss: 0.7476: 100%|██████████| 10/10 [04:13<00:00, 25.35s/it]
iteration:  1 tokens:  900 time:  73.27941997302696 tokens_per_second_on_single_device:  12.28
iteration:  2 tokens:  845 time:  20.03618132113479 tokens_per_second_on_single_device:  42.17
iteration:  3 tokens:  916 time:  20.086118309991434 tokens_per_second_on_single_device:  45.6
iteration:  4 tokens:  867 time:  19.975025688065216 tokens_per_second_on_single_device:  43.4
iteration:  5 tokens:  831 time:  20.042098701000214 tokens_per_second_on_single_device:  41.46
iteration:  6 tokens:  859 time:  20.09312969702296 tokens_per_second_on_single_device:  42.75
iteration:  7 tokens:  917 time:  19.917016892926767 tokens_per_second_on_single_device:  46.04
iteration:  8 tokens:  835 time:  20.01735433191061 tokens_per_second_on_single_device:  41.71
iteration:  9 tokens:  848 time:  20.01925670192577 tokens_per_second_on_single_device:  42.36
iteration:  10 tokens:  892 time:  19.96568553103134 tokens_per_second_on_single_device:  44.68
avg tokens_per_second_on_single_device:  34.37
[W1013 09:03:15.842271583 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
