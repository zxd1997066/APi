Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1754234213.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   0%|          | 238/52002 [00:00<00:21, 2375.21it/s]Packing dataset:   1%|          | 497/52002 [00:00<00:20, 2499.08it/s]Packing dataset:   1%|▏         | 769/52002 [00:00<00:19, 2599.34it/s]Packing dataset:   2%|▏         | 1037/52002 [00:00<00:19, 2630.01it/s]Packing dataset:   3%|▎         | 1324/52002 [00:00<00:18, 2714.09it/s]Packing dataset:   3%|▎         | 1600/52002 [00:00<00:18, 2727.39it/s]Packing dataset:   4%|▎         | 1873/52002 [00:00<00:18, 2664.06it/s]Packing dataset:   4%|▍         | 2150/52002 [00:00<00:18, 2694.42it/s]Packing dataset:   5%|▍         | 2431/52002 [00:00<00:18, 2728.10it/s]Packing dataset:   5%|▌         | 2711/52002 [00:01<00:17, 2749.00it/s]Packing dataset:   6%|▌         | 2987/52002 [00:01<00:18, 2714.17it/s]Packing dataset:   6%|▋         | 3262/52002 [00:01<00:17, 2724.42it/s]Packing dataset:   7%|▋         | 3535/52002 [00:01<00:17, 2702.17it/s]Packing dataset:   7%|▋         | 3806/52002 [00:01<00:18, 2674.64it/s]Packing dataset:   8%|▊         | 4074/52002 [00:01<00:18, 2645.09it/s]Packing dataset:   8%|▊         | 4356/52002 [00:01<00:17, 2696.40it/s]Packing dataset:   9%|▉         | 4626/52002 [00:01<00:17, 2695.07it/s]Packing dataset:   9%|▉         | 4900/52002 [00:01<00:17, 2706.93it/s]Packing dataset:  10%|▉         | 5171/52002 [00:01<00:17, 2689.45it/s]Packing dataset:  10%|█         | 5441/52002 [00:02<00:17, 2633.75it/s]Packing dataset:  11%|█         | 5726/52002 [00:02<00:17, 2695.47it/s]Packing dataset:  12%|█▏        | 5996/52002 [00:02<00:17, 2652.35it/s]Packing dataset:  12%|█▏        | 6262/52002 [00:02<00:17, 2654.46it/s]Packing dataset:  13%|█▎        | 6530/52002 [00:02<00:17, 2659.50it/s]Packing dataset:  13%|█▎        | 6797/52002 [00:02<00:17, 2593.99it/s]Packing dataset:  14%|█▎        | 7057/52002 [00:02<00:17, 2571.42it/s]Packing dataset:  14%|█▍        | 7322/52002 [00:02<00:17, 2591.81it/s]Packing dataset:  15%|█▍        | 7582/52002 [00:02<00:17, 2581.12it/s]Packing dataset:  15%|█▌        | 7849/52002 [00:02<00:16, 2603.95it/s]Packing dataset:  16%|█▌        | 8114/52002 [00:03<00:16, 2616.34it/s]Packing dataset:  16%|█▌        | 8376/52002 [00:03<00:16, 2575.08it/s]Packing dataset:  17%|█▋        | 8653/52002 [00:03<00:16, 2632.06it/s]Packing dataset:  17%|█▋        | 8941/52002 [00:03<00:15, 2703.30it/s]Packing dataset:  18%|█▊        | 9212/52002 [00:03<00:16, 2670.38it/s]Packing dataset:  18%|█▊        | 9500/52002 [00:03<00:15, 2729.95it/s]Packing dataset:  19%|█▉        | 9774/52002 [00:03<00:16, 2634.48it/s]Packing dataset:  19%|█▉        | 10039/52002 [00:03<00:16, 2618.60it/s]Packing dataset:  20%|█▉        | 10310/52002 [00:03<00:15, 2643.18it/s]Packing dataset:  20%|██        | 10585/52002 [00:03<00:15, 2672.98it/s]Packing dataset:  21%|██        | 10853/52002 [00:04<00:15, 2661.50it/s]Packing dataset:  21%|██▏       | 11120/52002 [00:04<00:15, 2622.70it/s]Packing dataset:  22%|██▏       | 11383/52002 [00:04<00:15, 2596.22it/s]Packing dataset:  22%|██▏       | 11648/52002 [00:04<00:15, 2608.53it/s]Packing dataset:  23%|██▎       | 11910/52002 [00:04<00:15, 2600.76it/s]Packing dataset:  23%|██▎       | 12178/52002 [00:04<00:15, 2618.65it/s]Packing dataset:  24%|██▍       | 12561/52002 [00:04<00:13, 2976.80it/s]Packing dataset:  25%|██▍       | 12984/52002 [00:04<00:11, 3347.24it/s]Packing dataset:  26%|██▌       | 13433/52002 [00:04<00:10, 3686.92it/s]Packing dataset:  27%|██▋       | 13873/52002 [00:04<00:09, 3899.19it/s]Packing dataset:  27%|██▋       | 14297/52002 [00:05<00:09, 4000.55it/s]Packing dataset:  28%|██▊       | 14736/52002 [00:05<00:09, 4113.84it/s]Packing dataset:  29%|██▉       | 15167/52002 [00:05<00:08, 4172.26it/s]Packing dataset:  30%|███       | 15603/52002 [00:05<00:08, 4223.75it/s]Packing dataset:  31%|███       | 16038/52002 [00:05<00:08, 4261.54it/s]Packing dataset:  32%|███▏      | 16468/52002 [00:05<00:08, 4271.01it/s]Packing dataset:  33%|███▎      | 16919/52002 [00:05<00:08, 4340.29it/s]Packing dataset:  33%|███▎      | 17367/52002 [00:05<00:07, 4378.97it/s]Packing dataset:  34%|███▍      | 17811/52002 [00:05<00:07, 4395.73it/s]Packing dataset:  35%|███▌      | 18251/52002 [00:06<00:07, 4336.45it/s]Packing dataset:  36%|███▌      | 18685/52002 [00:06<00:07, 4284.66it/s]Packing dataset:  37%|███▋      | 19127/52002 [00:06<00:07, 4323.73it/s]Packing dataset:  38%|███▊      | 19560/52002 [00:06<00:07, 4320.81it/s]Packing dataset:  38%|███▊      | 19996/52002 [00:06<00:07, 4331.55it/s]Packing dataset:  39%|███▉      | 20430/52002 [00:06<00:07, 4312.45it/s]Packing dataset:  40%|████      | 20886/52002 [00:06<00:07, 4384.39it/s]Packing dataset:  41%|████      | 21344/52002 [00:06<00:06, 4440.50it/s]Packing dataset:  42%|████▏     | 21789/52002 [00:06<00:06, 4422.86it/s]Packing dataset:  43%|████▎     | 22232/52002 [00:06<00:06, 4372.74it/s]Packing dataset:  44%|████▎     | 22670/52002 [00:07<00:06, 4368.75it/s]Packing dataset:  44%|████▍     | 23107/52002 [00:07<00:06, 4358.18it/s]Packing dataset:  45%|████▌     | 23558/52002 [00:07<00:06, 4401.34it/s]Packing dataset:  46%|████▌     | 24004/52002 [00:07<00:06, 4415.35it/s]Packing dataset:  47%|████▋     | 24446/52002 [00:07<00:06, 4384.16it/s]Packing dataset:  48%|████▊     | 24885/52002 [00:07<00:06, 4372.19it/s]Packing dataset:  49%|████▊     | 25323/52002 [00:07<00:06, 4370.38it/s]Packing dataset:  50%|████▉     | 25763/52002 [00:07<00:05, 4377.93it/s]Packing dataset:  50%|█████     | 26201/52002 [00:07<00:05, 4344.74it/s]Packing dataset:  51%|█████     | 26646/52002 [00:07<00:05, 4375.58it/s]Packing dataset:  52%|█████▏    | 27084/52002 [00:08<00:05, 4333.98it/s]Packing dataset:  53%|█████▎    | 27540/52002 [00:08<00:05, 4398.54it/s]Packing dataset:  54%|█████▍    | 27981/52002 [00:08<00:05, 4302.14it/s]Packing dataset:  55%|█████▍    | 28412/52002 [00:08<00:05, 4291.31it/s]Packing dataset:  55%|█████▌    | 28847/52002 [00:08<00:05, 4307.32it/s]Packing dataset:  56%|█████▋    | 29293/52002 [00:08<00:05, 4349.47it/s]Packing dataset:  57%|█████▋    | 29733/52002 [00:08<00:05, 4360.63it/s]Packing dataset:  58%|█████▊    | 30170/52002 [00:08<00:05, 4315.15it/s]Packing dataset:  59%|█████▉    | 30602/52002 [00:08<00:04, 4289.34it/s]Packing dataset:  60%|█████▉    | 31042/52002 [00:08<00:04, 4321.67it/s]Packing dataset:  61%|██████    | 31479/52002 [00:09<00:04, 4334.36it/s]Packing dataset:  61%|██████▏   | 31928/52002 [00:09<00:04, 4380.44it/s]Packing dataset:  62%|██████▏   | 32367/52002 [00:09<00:04, 4382.75it/s]Packing dataset:  63%|██████▎   | 32807/52002 [00:09<00:04, 4387.46it/s]Packing dataset:  64%|██████▍   | 33258/52002 [00:09<00:04, 4420.17it/s]Packing dataset:  65%|██████▍   | 33701/52002 [00:09<00:04, 4384.40it/s]Packing dataset:  66%|██████▌   | 34140/52002 [00:09<00:04, 4375.07it/s]Packing dataset:  66%|██████▋   | 34578/52002 [00:09<00:04, 4344.17it/s]Packing dataset:  67%|██████▋   | 35014/52002 [00:09<00:03, 4348.35it/s]Packing dataset:  68%|██████▊   | 35459/52002 [00:09<00:03, 4376.70it/s]Packing dataset:  69%|██████▉   | 35897/52002 [00:10<00:03, 4327.12it/s]Packing dataset:  70%|██████▉   | 36330/52002 [00:10<00:03, 4264.15it/s]Packing dataset:  71%|███████   | 36771/52002 [00:10<00:03, 4306.16it/s]Packing dataset:  72%|███████▏  | 37235/52002 [00:10<00:03, 4402.24it/s]Packing dataset:  72%|███████▏  | 37676/52002 [00:10<00:03, 4348.46it/s]Packing dataset:  73%|███████▎  | 38123/52002 [00:10<00:03, 4383.99it/s]Packing dataset:  74%|███████▍  | 38568/52002 [00:10<00:03, 4402.95it/s]Packing dataset:  75%|███████▌  | 39009/52002 [00:10<00:02, 4352.09it/s]Packing dataset:  76%|███████▌  | 39460/52002 [00:10<00:02, 4394.50it/s]Packing dataset:  77%|███████▋  | 39900/52002 [00:10<00:02, 4328.88it/s]Packing dataset:  78%|███████▊  | 40340/52002 [00:11<00:02, 4349.05it/s]Packing dataset:  78%|███████▊  | 40782/52002 [00:11<00:02, 4368.34it/s]Packing dataset:  79%|███████▉  | 41235/52002 [00:11<00:02, 4412.66it/s]Packing dataset:  80%|████████  | 41677/52002 [00:11<00:02, 4345.08it/s]Packing dataset:  81%|████████  | 42112/52002 [00:11<00:02, 4319.51it/s]Packing dataset:  82%|████████▏ | 42547/52002 [00:11<00:02, 4325.55it/s]Packing dataset:  83%|████████▎ | 42980/52002 [00:11<00:02, 4297.24it/s]Packing dataset:  83%|████████▎ | 43410/52002 [00:11<00:02, 4235.74it/s]Packing dataset:  84%|████████▍ | 43858/52002 [00:11<00:01, 4305.78it/s]Packing dataset:  85%|████████▌ | 44294/52002 [00:11<00:01, 4320.30it/s]Packing dataset:  86%|████████▌ | 44739/52002 [00:12<00:01, 4357.60it/s]Packing dataset:  87%|████████▋ | 45182/52002 [00:12<00:01, 4378.94it/s]Packing dataset:  88%|████████▊ | 45621/52002 [00:12<00:01, 4369.18it/s]Packing dataset:  89%|████████▊ | 46059/52002 [00:12<00:01, 4351.43it/s]Packing dataset:  89%|████████▉ | 46495/52002 [00:12<00:01, 4351.47it/s]Packing dataset:  90%|█████████ | 46931/52002 [00:12<00:01, 4341.29it/s]Packing dataset:  91%|█████████ | 47366/52002 [00:12<00:01, 4319.13it/s]Packing dataset:  92%|█████████▏| 47799/52002 [00:12<00:00, 4320.53it/s]Packing dataset:  93%|█████████▎| 48232/52002 [00:12<00:00, 4308.73it/s]Packing dataset:  94%|█████████▎| 48665/52002 [00:12<00:00, 4311.92it/s]Packing dataset:  94%|█████████▍| 49097/52002 [00:13<00:00, 4278.12it/s]Packing dataset:  95%|█████████▌| 49531/52002 [00:13<00:00, 4296.31it/s]Packing dataset:  96%|█████████▌| 49961/52002 [00:13<00:00, 4296.93it/s]Packing dataset:  97%|█████████▋| 50391/52002 [00:13<00:00, 4292.78it/s]Packing dataset:  98%|█████████▊| 50821/52002 [00:13<00:00, 4293.64it/s]Packing dataset:  99%|█████████▊| 51259/52002 [00:13<00:00, 4318.31it/s]Packing dataset:  99%|█████████▉| 51695/52002 [00:13<00:00, 4327.78it/s]Packing dataset: 100%|██████████| 52002/52002 [00:13<00:00, 3776.08it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/bin/tune", line 7, in <module>
    sys.exit(main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 52, in main
    parser.run(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/tune.py", line 46, in run
    args.func(args)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 214, in _run_cmd
    self._run_single_device(args, is_builtin=is_builtin)
  File "/home/jenkins/xiangdong/torchtune/torchtune/_cli/run.py", line 108, in _run_single_device
    runpy.run_path(str(args.recipe), run_name="__main__")
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/runpy.py", line 289, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/runpy.py", line 96, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_single_device.py", line 685, in <module>
    sys.exit(recipe_main())
  File "/home/jenkins/xiangdong/torchtune/torchtune/config/_parse.py", line 99, in wrapper
    sys.exit(recipe_main(conf))
  File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_single_device.py", line 680, in recipe_main
    recipe.train()
  File "/home/jenkins/xiangdong/torchtune/recipes/full_finetune_single_device.py", line 591, in train
    current_loss.backward()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/jenkins/xiangdong/torchtune/torchtune/modules/optim.py", line 52, in <lambda>
    lambda param=p: self._step_and_clear(param)
  File "/home/jenkins/xiangdong/torchtune/torchtune/modules/optim.py", line 59, in _step_and_clear
    self._optimizers[idx].step()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/optim/optimizer.py", line 516, in wrapper
    out = func(*args, **kwargs)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torchao/optim/adam.py", line 142, in step
    torch.compile(single_param_adam, fullgraph=True, dynamic=False)(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 817, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 979, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 963, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1643, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1506, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2318, in compile_to_module
    return self._compile_to_module()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2328, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2396, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3458, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/tmp/torchinductor_jenkins/ji/cjibipk4r3ehubvzcqmrjksjzbrfpysd7syddtbw7vcdgnneslcg.py", line 273, in <module>
    triton_red_fused__to_copy_abs_add_amax_clamp_copy_copy__div_ge_index_lerp_mul_pow_rsub_scalar_tensor_sqrt_sub_view_where_0 = async_compile.triton('triton_red_fused__to_copy_abs_add_amax_clamp_copy_copy__div_ge_index_lerp_mul_pow_rsub_scalar_tensor_sqrt_sub_view_where_0', '''
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 486, in triton
    kernel.precompile(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 440, in precompile
    self._make_launchers()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 603, in _make_launchers
    launchers.append(result.make_launcher())
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 1638, in make_launcher
    binary._init_handles()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/compiler/compiler.py", line 411, in _init_handles
    self.run = driver.active.launcher_cls(self.src, self.metadata)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/backends/intel/driver.py", line 711, in __init__
    self.mod = compile_module_from_src(src, "__triton_launcher")
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/backends/intel/driver.py", line 281, in compile_module_from_src
    return TritonLauncher(cache_path)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/backends/intel/driver.py", line 227, in __init__
    self.shared_library = ctypes.PyDLL(cache_path)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/ctypes/__init__.py", line 374, in __init__
    self._handle = _dlopen(self._name, mode)
torch._inductor.exc.InductorError: OSError: /tmp/torchinductor_jenkins/triton/0/V4OAKOLVJ2OHMUE5Z4WRPED4DDDEMRQ623WX4YJONLPDZWSJHKAQ/__triton_launcher.so: undefined symbol: _ZN4sycl3_V15queue22submit_with_event_implERKNS0_6detail19type_erased_cgfo_tyERKNS2_14SubmissionInfoERKNS2_13code_locationEb

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Exception ignored in: <function TritonLauncher.__del__ at 0x7fba721f1b40>
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/backends/intel/driver.py", line 241, in __del__
    handle = self.shared_library._handle
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/triton/backends/intel/driver.py", line 236, in __getattribute__
    return super().__getattribute__(name)
AttributeError: 'TritonLauncher' object has no attribute 'shared_library'
  0%|          | 0/10 [00:09<?, ?it/s]
