Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1757988044.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   1%|          | 407/52002 [00:00<00:12, 4049.98it/s]Packing dataset:   2%|▏         | 872/52002 [00:00<00:11, 4399.58it/s]Packing dataset:   3%|▎         | 1352/52002 [00:00<00:11, 4581.48it/s]Packing dataset:   3%|▎         | 1811/52002 [00:00<00:10, 4575.30it/s]Packing dataset:   4%|▍         | 2292/52002 [00:00<00:10, 4656.18it/s]Packing dataset:   5%|▌         | 2773/52002 [00:00<00:10, 4705.37it/s]Packing dataset:   6%|▌         | 3244/52002 [00:00<00:10, 4692.77it/s]Packing dataset:   7%|▋         | 3714/52002 [00:00<00:10, 4622.71it/s]Packing dataset:   8%|▊         | 4177/52002 [00:00<00:10, 4614.94it/s]Packing dataset:   9%|▉         | 4640/52002 [00:01<00:10, 4618.04it/s]Packing dataset:  10%|▉         | 5105/52002 [00:01<00:10, 4626.62it/s]Packing dataset:  11%|█         | 5579/52002 [00:01<00:09, 4658.84it/s]Packing dataset:  12%|█▏        | 6045/52002 [00:01<00:09, 4618.66it/s]Packing dataset:  13%|█▎        | 6507/52002 [00:01<00:09, 4596.77it/s]Packing dataset:  13%|█▎        | 6967/52002 [00:01<00:09, 4551.63it/s]Packing dataset:  14%|█▍        | 7423/52002 [00:01<00:09, 4505.96it/s]Packing dataset:  15%|█▌        | 7876/52002 [00:01<00:09, 4511.50it/s]Packing dataset:  16%|█▌        | 8334/52002 [00:01<00:09, 4529.87it/s]Packing dataset:  17%|█▋        | 8807/52002 [00:01<00:09, 4589.28it/s]Packing dataset:  18%|█▊        | 9276/52002 [00:02<00:09, 4615.91it/s]Packing dataset:  19%|█▊        | 9738/52002 [00:02<00:09, 4612.01it/s]Packing dataset:  20%|█▉        | 10200/52002 [00:02<00:09, 4576.85it/s]Packing dataset:  20%|██        | 10658/52002 [00:02<00:09, 4565.61it/s]Packing dataset:  21%|██▏       | 11121/52002 [00:02<00:08, 4580.87it/s]Packing dataset:  22%|██▏       | 11580/52002 [00:02<00:08, 4532.71it/s]Packing dataset:  23%|██▎       | 12034/52002 [00:02<00:08, 4531.00it/s]Packing dataset:  24%|██▍       | 12496/52002 [00:02<00:08, 4556.10it/s]Packing dataset:  25%|██▍       | 12952/52002 [00:02<00:08, 4541.56it/s]Packing dataset:  26%|██▌       | 13410/52002 [00:02<00:08, 4551.94it/s]Packing dataset:  27%|██▋       | 13866/52002 [00:03<00:08, 4553.43it/s]Packing dataset:  28%|██▊       | 14322/52002 [00:03<00:08, 4511.30it/s]Packing dataset:  28%|██▊       | 14779/52002 [00:03<00:08, 4527.70it/s]Packing dataset:  29%|██▉       | 15242/52002 [00:03<00:08, 4557.71it/s]Packing dataset:  30%|███       | 15698/52002 [00:03<00:08, 4488.95it/s]Packing dataset:  31%|███       | 16162/52002 [00:03<00:07, 4533.60it/s]Packing dataset:  32%|███▏      | 16620/52002 [00:03<00:07, 4546.58it/s]Packing dataset:  33%|███▎      | 17077/52002 [00:03<00:07, 4552.40it/s]Packing dataset:  34%|███▎      | 17542/52002 [00:03<00:07, 4580.53it/s]Packing dataset:  35%|███▍      | 18001/52002 [00:03<00:07, 4544.57it/s]Packing dataset:  35%|███▌      | 18456/52002 [00:04<00:07, 4485.76it/s]Packing dataset:  36%|███▋      | 18905/52002 [00:04<00:07, 4467.02it/s]Packing dataset:  37%|███▋      | 19363/52002 [00:04<00:07, 4498.01it/s]Packing dataset:  38%|███▊      | 19813/52002 [00:04<00:07, 4484.71it/s]Packing dataset:  39%|███▉      | 20265/52002 [00:04<00:07, 4494.44it/s]Packing dataset:  40%|███▉      | 20728/52002 [00:04<00:06, 4533.02it/s]Packing dataset:  41%|████      | 21201/52002 [00:04<00:06, 4590.68it/s]Packing dataset:  42%|████▏     | 21661/52002 [00:04<00:06, 4563.82it/s]Packing dataset:  43%|████▎     | 22118/52002 [00:04<00:06, 4527.53it/s]Packing dataset:  43%|████▎     | 22573/52002 [00:04<00:06, 4532.59it/s]Packing dataset:  44%|████▍     | 23027/52002 [00:05<00:06, 4499.34it/s]Packing dataset:  45%|████▌     | 23499/52002 [00:05<00:06, 4564.17it/s]Packing dataset:  46%|████▌     | 23956/52002 [00:05<00:06, 4564.15it/s]Packing dataset:  47%|████▋     | 24413/52002 [00:05<00:06, 4545.70it/s]Packing dataset:  48%|████▊     | 24868/52002 [00:05<00:05, 4540.37it/s]Packing dataset:  49%|████▊     | 25323/52002 [00:05<00:05, 4529.25it/s]Packing dataset:  50%|████▉     | 25780/52002 [00:05<00:05, 4541.06it/s]Packing dataset:  50%|█████     | 26235/52002 [00:05<00:05, 4534.73it/s]Packing dataset:  51%|█████▏    | 26689/52002 [00:05<00:05, 4535.20it/s]Packing dataset:  52%|█████▏    | 27143/52002 [00:05<00:05, 4503.14it/s]Packing dataset:  53%|█████▎    | 27604/52002 [00:06<00:05, 4534.54it/s]Packing dataset:  54%|█████▍    | 28058/52002 [00:06<00:05, 4494.22it/s]Packing dataset:  55%|█████▍    | 28508/52002 [00:06<00:05, 4426.68it/s]Packing dataset:  56%|█████▌    | 28951/52002 [00:06<00:05, 4419.75it/s]Packing dataset:  57%|█████▋    | 29407/52002 [00:06<00:05, 4446.47it/s]Packing dataset:  57%|█████▋    | 29854/52002 [00:06<00:04, 4450.11it/s]Packing dataset:  58%|█████▊    | 30316/52002 [00:06<00:04, 4500.48it/s]Packing dataset:  59%|█████▉    | 30767/52002 [00:06<00:04, 4457.56it/s]Packing dataset:  60%|██████    | 31213/52002 [00:06<00:04, 4447.27it/s]Packing dataset:  61%|██████    | 31670/52002 [00:06<00:04, 4483.34it/s]Packing dataset:  62%|██████▏   | 32145/52002 [00:07<00:04, 4561.68it/s]Packing dataset:  63%|██████▎   | 32607/52002 [00:07<00:04, 4577.75it/s]Packing dataset:  64%|██████▎   | 33069/52002 [00:07<00:04, 4588.53it/s]Packing dataset:  64%|██████▍   | 33528/52002 [00:07<00:04, 4569.25it/s]Packing dataset:  65%|██████▌   | 33985/52002 [00:07<00:03, 4551.50it/s]Packing dataset:  66%|██████▌   | 34441/52002 [00:07<00:03, 4530.76it/s]Packing dataset:  67%|██████▋   | 34895/52002 [00:07<00:03, 4532.47it/s]Packing dataset:  68%|██████▊   | 35352/52002 [00:07<00:03, 4543.09it/s]Packing dataset:  69%|██████▉   | 35808/52002 [00:07<00:03, 4546.33it/s]Packing dataset:  70%|██████▉   | 36263/52002 [00:07<00:03, 4469.99it/s]Packing dataset:  71%|███████   | 36718/52002 [00:08<00:03, 4490.98it/s]Packing dataset:  72%|███████▏  | 37194/52002 [00:08<00:03, 4566.77it/s]Packing dataset:  72%|███████▏  | 37651/52002 [00:08<00:03, 4482.01it/s]Packing dataset:  73%|███████▎  | 38108/52002 [00:08<00:03, 4506.63it/s]Packing dataset:  74%|███████▍  | 38565/52002 [00:08<00:02, 4522.25it/s]Packing dataset:  75%|███████▌  | 39018/52002 [00:08<00:02, 4465.83it/s]Packing dataset:  76%|███████▌  | 39482/52002 [00:08<00:02, 4516.56it/s]Packing dataset:  77%|███████▋  | 39934/52002 [00:08<00:02, 4494.92it/s]Packing dataset:  78%|███████▊  | 40388/52002 [00:08<00:02, 4505.58it/s]Packing dataset:  79%|███████▊  | 40850/52002 [00:09<00:02, 4538.28it/s]Packing dataset:  79%|███████▉  | 41314/52002 [00:09<00:02, 4567.02it/s]Packing dataset:  80%|████████  | 41771/52002 [00:09<00:02, 4501.38it/s]Packing dataset:  81%|████████  | 42222/52002 [00:09<00:02, 4462.22it/s]Packing dataset:  82%|████████▏ | 42684/52002 [00:09<00:02, 4508.48it/s]Packing dataset:  83%|████████▎ | 43136/52002 [00:09<00:01, 4450.48it/s]Packing dataset:  84%|████████▍ | 43582/52002 [00:09<00:01, 4386.80it/s]Packing dataset:  85%|████████▍ | 44041/52002 [00:09<00:01, 4446.37it/s]Packing dataset:  86%|████████▌ | 44495/52002 [00:09<00:01, 4473.88it/s]Packing dataset:  86%|████████▋ | 44966/52002 [00:09<00:01, 4543.34it/s]Packing dataset:  87%|████████▋ | 45421/52002 [00:10<00:01, 4538.88it/s]Packing dataset:  88%|████████▊ | 45890/52002 [00:10<00:01, 4582.73it/s]Packing dataset:  89%|████████▉ | 46349/52002 [00:10<00:01, 4536.09it/s]Packing dataset:  90%|█████████ | 46811/52002 [00:10<00:01, 4557.07it/s]Packing dataset:  91%|█████████ | 47271/52002 [00:10<00:01, 4568.14it/s]Packing dataset:  92%|█████████▏| 47728/52002 [00:10<00:00, 4535.18it/s]Packing dataset:  93%|█████████▎| 48182/52002 [00:10<00:00, 4520.01it/s]Packing dataset:  94%|█████████▎| 48639/52002 [00:10<00:00, 4532.19it/s]Packing dataset:  94%|█████████▍| 49093/52002 [00:10<00:00, 4518.96it/s]Packing dataset:  95%|█████████▌| 49545/52002 [00:10<00:00, 4487.95it/s]Packing dataset:  96%|█████████▌| 49994/52002 [00:11<00:00, 4455.13it/s]Packing dataset:  97%|█████████▋| 50443/52002 [00:11<00:00, 4463.57it/s]Packing dataset:  98%|█████████▊| 50894/52002 [00:11<00:00, 4475.46it/s]Packing dataset:  99%|█████████▊| 51348/52002 [00:11<00:00, 4494.45it/s]Packing dataset: 100%|█████████▉| 51798/52002 [00:11<00:00, 4485.71it/s]Packing dataset: 100%|██████████| 52002/52002 [00:11<00:00, 4528.00it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:27<04:04, 27.20s/it]1|1|Loss: 1.6729:  10%|█         | 1/10 [00:27<04:04, 27.20s/it]1|1|Loss: 1.6729:  20%|██        | 2/10 [00:28<01:35, 11.94s/it]1|2|Loss: 1.3793:  20%|██        | 2/10 [00:28<01:35, 11.94s/it]1|2|Loss: 1.3793:  30%|███       | 3/10 [00:29<00:50,  7.16s/it]1|3|Loss: 1.2953:  30%|███       | 3/10 [00:29<00:50,  7.16s/it]1|3|Loss: 1.2953:  40%|████      | 4/10 [00:31<00:28,  4.82s/it]1|4|Loss: 1.1877:  40%|████      | 4/10 [00:31<00:28,  4.82s/it]1|4|Loss: 1.1877:  50%|█████     | 5/10 [00:32<00:17,  3.53s/it]1|5|Loss: 0.9955:  50%|█████     | 5/10 [00:32<00:17,  3.53s/it]1|5|Loss: 0.9955:  60%|██████    | 6/10 [00:33<00:11,  2.75s/it]1|6|Loss: 1.0026:  60%|██████    | 6/10 [00:33<00:11,  2.75s/it]1|6|Loss: 1.0026:  70%|███████   | 7/10 [00:34<00:06,  2.26s/it]1|7|Loss: 0.9855:  70%|███████   | 7/10 [00:34<00:06,  2.26s/it]1|7|Loss: 0.9855:  80%|████████  | 8/10 [00:36<00:03,  1.94s/it]1|8|Loss: 1.1853:  80%|████████  | 8/10 [00:36<00:03,  1.94s/it]1|8|Loss: 1.1853:  90%|█████████ | 9/10 [00:37<00:01,  1.72s/it]1|9|Loss: 0.9451:  90%|█████████ | 9/10 [00:37<00:01,  1.72s/it]1|9|Loss: 0.9451: 100%|██████████| 10/10 [00:38<00:00,  1.57s/it]1|10|Loss: 0.7483: 100%|██████████| 10/10 [00:38<00:00,  1.57s/it]1|10|Loss: 0.7483: 100%|██████████| 10/10 [00:38<00:00,  3.86s/it]
iteration:  1 tokens:  900 time:  27.20496736804489 tokens_per_second_on_single_device:  33.08
iteration:  2 tokens:  845 time:  1.2376832729787566 tokens_per_second_on_single_device:  682.73
iteration:  3 tokens:  916 time:  1.4798731110058725 tokens_per_second_on_single_device:  618.97
iteration:  4 tokens:  867 time:  1.2381076070014387 tokens_per_second_on_single_device:  700.26
iteration:  5 tokens:  831 time:  1.2355997679987922 tokens_per_second_on_single_device:  672.55
iteration:  6 tokens:  859 time:  1.2407490239711478 tokens_per_second_on_single_device:  692.32
iteration:  7 tokens:  917 time:  1.253575300972443 tokens_per_second_on_single_device:  731.51
iteration:  8 tokens:  835 time:  1.2357451610150747 tokens_per_second_on_single_device:  675.71
iteration:  9 tokens:  848 time:  1.2333561389823444 tokens_per_second_on_single_device:  687.55
iteration:  10 tokens:  892 time:  1.2449642550200224 tokens_per_second_on_single_device:  716.49
avg tokens_per_second_on_single_device:  225.62
[W916 02:01:39.731900147 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
