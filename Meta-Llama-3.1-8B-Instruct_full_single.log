Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1756740889.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   1%|          | 423/52002 [00:00<00:12, 4226.03it/s]Packing dataset:   2%|▏         | 891/52002 [00:00<00:11, 4490.45it/s]Packing dataset:   3%|▎         | 1376/52002 [00:00<00:10, 4649.73it/s]Packing dataset:   4%|▎         | 1841/52002 [00:00<00:10, 4594.81it/s]Packing dataset:   4%|▍         | 2336/52002 [00:00<00:10, 4721.38it/s]Packing dataset:   5%|▌         | 2813/52002 [00:00<00:10, 4737.39it/s]Packing dataset:   6%|▋         | 3287/52002 [00:00<00:10, 4728.10it/s]Packing dataset:   7%|▋         | 3760/52002 [00:00<00:10, 4647.05it/s]Packing dataset:   8%|▊         | 4229/52002 [00:00<00:10, 4659.91it/s]Packing dataset:   9%|▉         | 4706/52002 [00:01<00:10, 4691.74it/s]Packing dataset:  10%|▉         | 5179/52002 [00:01<00:09, 4701.58it/s]Packing dataset:  11%|█         | 5663/52002 [00:01<00:09, 4741.43it/s]Packing dataset:  12%|█▏        | 6138/52002 [00:01<00:09, 4646.95it/s]Packing dataset:  13%|█▎        | 6604/52002 [00:01<00:09, 4650.53it/s]Packing dataset:  14%|█▎        | 7070/52002 [00:01<00:09, 4585.19it/s]Packing dataset:  14%|█▍        | 7529/52002 [00:01<00:09, 4563.11it/s]Packing dataset:  15%|█▌        | 7986/52002 [00:01<00:09, 4559.29it/s]Packing dataset:  16%|█▌        | 8445/52002 [00:01<00:09, 4563.65it/s]Packing dataset:  17%|█▋        | 8941/52002 [00:01<00:09, 4679.62it/s]Packing dataset:  18%|█▊        | 9411/52002 [00:02<00:09, 4684.20it/s]Packing dataset:  19%|█▉        | 9880/52002 [00:02<00:09, 4627.09it/s]Packing dataset:  20%|█▉        | 10343/52002 [00:02<00:09, 4627.66it/s]Packing dataset:  21%|██        | 10813/52002 [00:02<00:08, 4649.05it/s]Packing dataset:  22%|██▏       | 11279/52002 [00:02<00:08, 4601.75it/s]Packing dataset:  23%|██▎       | 11749/52002 [00:02<00:08, 4628.60it/s]Packing dataset:  23%|██▎       | 12213/52002 [00:02<00:08, 4585.06it/s]Packing dataset:  24%|██▍       | 12687/52002 [00:02<00:08, 4629.91it/s]Packing dataset:  25%|██▌       | 13153/52002 [00:02<00:08, 4636.14it/s]Packing dataset:  26%|██▌       | 13617/52002 [00:02<00:08, 4628.13it/s]Packing dataset:  27%|██▋       | 14080/52002 [00:03<00:08, 4577.75it/s]Packing dataset:  28%|██▊       | 14552/52002 [00:03<00:08, 4617.48it/s]Packing dataset:  29%|██▉       | 15017/52002 [00:03<00:07, 4623.42it/s]Packing dataset:  30%|██▉       | 15480/52002 [00:03<00:07, 4601.72it/s]Packing dataset:  31%|███       | 15941/52002 [00:03<00:07, 4554.66it/s]Packing dataset:  32%|███▏      | 16411/52002 [00:03<00:07, 4597.28it/s]Packing dataset:  32%|███▏      | 16885/52002 [00:03<00:07, 4638.44it/s]Packing dataset:  33%|███▎      | 17356/52002 [00:03<00:07, 4658.87it/s]Packing dataset:  34%|███▍      | 17823/52002 [00:03<00:07, 4658.84it/s]Packing dataset:  35%|███▌      | 18289/52002 [00:03<00:07, 4592.30it/s]Packing dataset:  36%|███▌      | 18749/52002 [00:04<00:07, 4558.27it/s]Packing dataset:  37%|███▋      | 19212/52002 [00:04<00:07, 4578.36it/s]Packing dataset:  38%|███▊      | 19670/52002 [00:04<00:07, 4568.51it/s]Packing dataset:  39%|███▊      | 20129/52002 [00:04<00:06, 4573.76it/s]Packing dataset:  40%|███▉      | 20594/52002 [00:04<00:06, 4594.71it/s]Packing dataset:  41%|████      | 21088/52002 [00:04<00:06, 4696.81it/s]Packing dataset:  41%|████▏     | 21558/52002 [00:04<00:06, 4694.38it/s]Packing dataset:  42%|████▏     | 22028/52002 [00:04<00:06, 4672.80it/s]Packing dataset:  43%|████▎     | 22496/52002 [00:04<00:06, 4636.44it/s]Packing dataset:  44%|████▍     | 22960/52002 [00:04<00:06, 4593.06it/s]Packing dataset:  45%|████▌     | 23439/52002 [00:05<00:06, 4650.50it/s]Packing dataset:  46%|████▌     | 23913/52002 [00:05<00:06, 4675.87it/s]Packing dataset:  47%|████▋     | 24381/52002 [00:05<00:05, 4634.51it/s]Packing dataset:  48%|████▊     | 24845/52002 [00:05<00:05, 4629.03it/s]Packing dataset:  49%|████▊     | 25309/52002 [00:05<00:05, 4631.25it/s]Packing dataset:  50%|████▉     | 25773/52002 [00:05<00:05, 4631.75it/s]Packing dataset:  50%|█████     | 26237/52002 [00:05<00:05, 4626.44it/s]Packing dataset:  51%|█████▏    | 26700/52002 [00:05<00:05, 4601.42it/s]Packing dataset:  52%|█████▏    | 27162/52002 [00:05<00:05, 4605.20it/s]Packing dataset:  53%|█████▎    | 27626/52002 [00:05<00:05, 4613.42it/s]Packing dataset:  54%|█████▍    | 28088/52002 [00:06<00:05, 4582.70it/s]Packing dataset:  55%|█████▍    | 28547/52002 [00:06<00:05, 4518.24it/s]Packing dataset:  56%|█████▌    | 29016/52002 [00:06<00:05, 4568.17it/s]Packing dataset:  57%|█████▋    | 29485/52002 [00:06<00:04, 4602.90it/s]Packing dataset:  58%|█████▊    | 29946/52002 [00:06<00:04, 4586.53it/s]Packing dataset:  58%|█████▊    | 30406/52002 [00:06<00:04, 4588.47it/s]Packing dataset:  59%|█████▉    | 30865/52002 [00:06<00:04, 4552.78it/s]Packing dataset:  60%|██████    | 31323/52002 [00:06<00:04, 4559.68it/s]Packing dataset:  61%|██████    | 31793/52002 [00:06<00:04, 4593.20it/s]Packing dataset:  62%|██████▏   | 32273/52002 [00:06<00:04, 4653.82it/s]Packing dataset:  63%|██████▎   | 32739/52002 [00:07<00:04, 4605.88it/s]Packing dataset:  64%|██████▍   | 33209/52002 [00:07<00:04, 4631.77it/s]Packing dataset:  65%|██████▍   | 33673/52002 [00:07<00:03, 4623.98it/s]Packing dataset:  66%|██████▌   | 34136/52002 [00:07<00:03, 4615.84it/s]Packing dataset:  67%|██████▋   | 34598/52002 [00:07<00:03, 4567.44it/s]Packing dataset:  67%|██████▋   | 35058/52002 [00:07<00:03, 4575.20it/s]Packing dataset:  68%|██████▊   | 35526/52002 [00:07<00:03, 4605.70it/s]Packing dataset:  69%|██████▉   | 35987/52002 [00:07<00:03, 4549.71it/s]Packing dataset:  70%|███████   | 36443/52002 [00:07<00:03, 4504.42it/s]Packing dataset:  71%|███████   | 36923/52002 [00:08<00:03, 4591.28it/s]Packing dataset:  72%|███████▏  | 37397/52002 [00:08<00:03, 4635.06it/s]Packing dataset:  73%|███████▎  | 37861/52002 [00:08<00:03, 4612.35it/s]Packing dataset:  74%|███████▎  | 38338/52002 [00:08<00:02, 4656.25it/s]Packing dataset:  75%|███████▍  | 38804/52002 [00:08<00:02, 4609.29it/s]Packing dataset:  76%|███████▌  | 39268/52002 [00:08<00:02, 4617.37it/s]Packing dataset:  76%|███████▋  | 39730/52002 [00:08<00:02, 4612.29it/s]Packing dataset:  77%|███████▋  | 40196/52002 [00:08<00:02, 4626.44it/s]Packing dataset:  78%|███████▊  | 40659/52002 [00:08<00:02, 4606.76it/s]Packing dataset:  79%|███████▉  | 41123/52002 [00:08<00:02, 4616.48it/s]Packing dataset:  80%|███████▉  | 41594/52002 [00:09<00:02, 4643.01it/s]Packing dataset:  81%|████████  | 42059/52002 [00:09<00:02, 4574.33it/s]Packing dataset:  82%|████████▏ | 42518/52002 [00:09<00:02, 4578.20it/s]Packing dataset:  83%|████████▎ | 42976/52002 [00:09<00:01, 4541.78it/s]Packing dataset:  84%|████████▎ | 43431/52002 [00:09<00:01, 4513.60it/s]Packing dataset:  84%|████████▍ | 43899/52002 [00:09<00:01, 4560.17it/s]Packing dataset:  85%|████████▌ | 44359/52002 [00:09<00:01, 4570.05it/s]Packing dataset:  86%|████████▌ | 44817/52002 [00:09<00:01, 4568.12it/s]Packing dataset:  87%|████████▋ | 45287/52002 [00:09<00:01, 4605.99it/s]Packing dataset:  88%|████████▊ | 45759/52002 [00:09<00:01, 4637.97it/s]Packing dataset:  89%|████████▉ | 46223/52002 [00:10<00:01, 4575.08it/s]Packing dataset:  90%|████████▉ | 46681/52002 [00:10<00:01, 4575.27it/s]Packing dataset:  91%|█████████ | 47148/52002 [00:10<00:01, 4601.66it/s]Packing dataset:  92%|█████████▏| 47609/52002 [00:10<00:00, 4572.78it/s]Packing dataset:  92%|█████████▏| 48067/52002 [00:10<00:00, 4560.98it/s]Packing dataset:  93%|█████████▎| 48526/52002 [00:10<00:00, 4568.49it/s]Packing dataset:  94%|█████████▍| 48983/52002 [00:10<00:00, 4565.08it/s]Packing dataset:  95%|█████████▌| 49443/52002 [00:10<00:00, 4574.58it/s]Packing dataset:  96%|█████████▌| 49901/52002 [00:10<00:00, 4545.25it/s]Packing dataset:  97%|█████████▋| 50356/52002 [00:10<00:00, 4534.36it/s]Packing dataset:  98%|█████████▊| 50810/52002 [00:11<00:00, 4513.82it/s]Packing dataset:  99%|█████████▊| 51283/52002 [00:11<00:00, 4577.16it/s]Packing dataset:  99%|█████████▉| 51741/52002 [00:11<00:00, 4577.15it/s]Packing dataset: 100%|██████████| 52002/52002 [00:11<00:00, 4605.59it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:35<05:21, 35.70s/it]1|1|Loss: 1.6727:  10%|█         | 1/10 [00:35<05:21, 35.70s/it]1|1|Loss: 1.6727:  20%|██        | 2/10 [00:37<02:03, 15.47s/it]1|2|Loss: 1.3794:  20%|██        | 2/10 [00:37<02:03, 15.47s/it]1|2|Loss: 1.3794:  30%|███       | 3/10 [00:38<01:02,  9.00s/it]1|3|Loss: 1.2913:  30%|███       | 3/10 [00:38<01:02,  9.00s/it]1|3|Loss: 1.2913:  40%|████      | 4/10 [00:39<00:35,  5.96s/it]1|4|Loss: 1.1928:  40%|████      | 4/10 [00:39<00:35,  5.96s/it]1|4|Loss: 1.1928:  50%|█████     | 5/10 [00:40<00:21,  4.27s/it]1|5|Loss: 0.9952:  50%|█████     | 5/10 [00:40<00:21,  4.27s/it]1|5|Loss: 0.9952:  60%|██████    | 6/10 [00:42<00:13,  3.26s/it]1|6|Loss: 1.0051:  60%|██████    | 6/10 [00:42<00:13,  3.26s/it]1|6|Loss: 1.0051:  70%|███████   | 7/10 [00:43<00:07,  2.62s/it]1|7|Loss: 0.9863:  70%|███████   | 7/10 [00:43<00:07,  2.62s/it]1|7|Loss: 0.9863:  80%|████████  | 8/10 [00:44<00:04,  2.19s/it]1|8|Loss: 1.1878:  80%|████████  | 8/10 [00:44<00:04,  2.19s/it]1|8|Loss: 1.1878:  90%|█████████ | 9/10 [00:46<00:01,  1.91s/it]1|9|Loss: 0.9454:  90%|█████████ | 9/10 [00:46<00:01,  1.91s/it]1|9|Loss: 0.9454: 100%|██████████| 10/10 [00:47<00:00,  1.72s/it]1|10|Loss: 0.7516: 100%|██████████| 10/10 [00:47<00:00,  1.72s/it]1|10|Loss: 0.7516: 100%|██████████| 10/10 [00:47<00:00,  4.74s/it]
iteration:  1 tokens:  900 time:  35.70400286698714 tokens_per_second_on_single_device:  25.21
iteration:  2 tokens:  845 time:  1.2871009451337159 tokens_per_second_on_single_device:  656.51
iteration:  3 tokens:  916 time:  1.3020044770091772 tokens_per_second_on_single_device:  703.53
iteration:  4 tokens:  867 time:  1.2912354511208832 tokens_per_second_on_single_device:  671.45
iteration:  5 tokens:  831 time:  1.2876872532069683 tokens_per_second_on_single_device:  645.34
iteration:  6 tokens:  859 time:  1.2856534528546035 tokens_per_second_on_single_device:  668.14
iteration:  7 tokens:  917 time:  1.304748066700995 tokens_per_second_on_single_device:  702.82
iteration:  8 tokens:  835 time:  1.2853661640547216 tokens_per_second_on_single_device:  649.62
iteration:  9 tokens:  848 time:  1.2889774548821151 tokens_per_second_on_single_device:  657.89
iteration:  10 tokens:  892 time:  1.3003262770362198 tokens_per_second_on_single_device:  685.98
avg tokens_per_second_on_single_device:  184.0
[W901 15:35:53.696896077 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
