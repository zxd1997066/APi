Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1759137023.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   1%|          | 401/52002 [00:00<00:12, 4006.60it/s]Packing dataset:   2%|▏         | 868/52002 [00:00<00:11, 4394.74it/s]Packing dataset:   3%|▎         | 1354/52002 [00:00<00:10, 4606.43it/s]Packing dataset:   3%|▎         | 1820/52002 [00:00<00:10, 4624.09it/s]Packing dataset:   4%|▍         | 2302/52002 [00:00<00:10, 4692.52it/s]Packing dataset:   5%|▌         | 2784/52002 [00:00<00:10, 4735.02it/s]Packing dataset:   6%|▋         | 3259/52002 [00:00<00:10, 4736.86it/s]Packing dataset:   7%|▋         | 3733/52002 [00:00<00:10, 4681.78it/s]Packing dataset:   8%|▊         | 4202/52002 [00:00<00:10, 4678.78it/s]Packing dataset:   9%|▉         | 4672/52002 [00:01<00:10, 4680.37it/s]Packing dataset:  10%|▉         | 5149/52002 [00:01<00:09, 4707.34it/s]Packing dataset:  11%|█         | 5629/52002 [00:01<00:09, 4731.89it/s]Packing dataset:  12%|█▏        | 6103/52002 [00:01<00:09, 4677.31it/s]Packing dataset:  13%|█▎        | 6571/52002 [00:01<00:09, 4643.90it/s]Packing dataset:  14%|█▎        | 7036/52002 [00:01<00:09, 4572.32it/s]Packing dataset:  14%|█▍        | 7494/52002 [00:01<00:09, 4551.80it/s]Packing dataset:  15%|█▌        | 7959/52002 [00:01<00:09, 4579.44it/s]Packing dataset:  16%|█▌        | 8418/52002 [00:01<00:09, 4554.83it/s]Packing dataset:  17%|█▋        | 8914/52002 [00:01<00:09, 4672.09it/s]Packing dataset:  18%|█▊        | 9382/52002 [00:02<00:09, 4671.91it/s]Packing dataset:  19%|█▉        | 9850/52002 [00:02<00:09, 4623.23it/s]Packing dataset:  20%|█▉        | 10322/52002 [00:02<00:08, 4651.74it/s]Packing dataset:  21%|██        | 10788/52002 [00:02<00:08, 4641.90it/s]Packing dataset:  22%|██▏       | 11253/52002 [00:02<00:08, 4597.39it/s]Packing dataset:  23%|██▎       | 11722/52002 [00:02<00:08, 4623.34it/s]Packing dataset:  23%|██▎       | 12185/52002 [00:02<00:08, 4583.68it/s]Packing dataset:  24%|██▍       | 12658/52002 [00:02<00:08, 4625.65it/s]Packing dataset:  25%|██▌       | 13123/52002 [00:02<00:08, 4629.42it/s]Packing dataset:  26%|██▌       | 13591/52002 [00:02<00:08, 4643.48it/s]Packing dataset:  27%|██▋       | 14056/52002 [00:03<00:08, 4595.79it/s]Packing dataset:  28%|██▊       | 14529/52002 [00:03<00:08, 4633.92it/s]Packing dataset:  29%|██▉       | 14993/52002 [00:03<00:08, 4602.96it/s]Packing dataset:  30%|██▉       | 15454/52002 [00:03<00:07, 4600.26it/s]Packing dataset:  31%|███       | 15915/52002 [00:03<00:07, 4548.81it/s]Packing dataset:  32%|███▏      | 16397/52002 [00:03<00:07, 4627.64it/s]Packing dataset:  32%|███▏      | 16866/52002 [00:03<00:07, 4645.28it/s]Packing dataset:  33%|███▎      | 17333/52002 [00:03<00:07, 4650.67it/s]Packing dataset:  34%|███▍      | 17802/52002 [00:03<00:07, 4660.36it/s]Packing dataset:  35%|███▌      | 18269/52002 [00:03<00:07, 4609.87it/s]Packing dataset:  36%|███▌      | 18731/52002 [00:04<00:07, 4549.77it/s]Packing dataset:  37%|███▋      | 19193/52002 [00:04<00:07, 4566.10it/s]Packing dataset:  38%|███▊      | 19652/52002 [00:04<00:07, 4571.36it/s]Packing dataset:  39%|███▊      | 20115/52002 [00:04<00:06, 4586.32it/s]Packing dataset:  40%|███▉      | 20584/52002 [00:04<00:06, 4616.51it/s]Packing dataset:  41%|████      | 21070/52002 [00:04<00:06, 4688.05it/s]Packing dataset:  41%|████▏     | 21543/52002 [00:04<00:06, 4699.57it/s]Packing dataset:  42%|████▏     | 22014/52002 [00:04<00:06, 4669.77it/s]Packing dataset:  43%|████▎     | 22482/52002 [00:04<00:06, 4668.46it/s]Packing dataset:  44%|████▍     | 22949/52002 [00:04<00:06, 4610.24it/s]Packing dataset:  45%|████▌     | 23425/52002 [00:05<00:06, 4653.09it/s]Packing dataset:  46%|████▌     | 23897/52002 [00:05<00:06, 4672.86it/s]Packing dataset:  47%|████▋     | 24365/52002 [00:05<00:05, 4657.26it/s]Packing dataset:  48%|████▊     | 24831/52002 [00:05<00:05, 4635.70it/s]Packing dataset:  49%|████▊     | 25295/52002 [00:05<00:05, 4626.88it/s]Packing dataset:  50%|████▉     | 25758/52002 [00:05<00:05, 4625.16it/s]Packing dataset:  50%|█████     | 26223/52002 [00:05<00:05, 4631.59it/s]Packing dataset:  51%|█████▏    | 26687/52002 [00:05<00:05, 4626.23it/s]Packing dataset:  52%|█████▏    | 27150/52002 [00:05<00:05, 4584.09it/s]Packing dataset:  53%|█████▎    | 27617/52002 [00:05<00:05, 4608.85it/s]Packing dataset:  54%|█████▍    | 28078/52002 [00:06<00:05, 4579.72it/s]Packing dataset:  55%|█████▍    | 28537/52002 [00:06<00:05, 4500.59it/s]Packing dataset:  56%|█████▌    | 28998/52002 [00:06<00:05, 4531.00it/s]Packing dataset:  57%|█████▋    | 29471/52002 [00:06<00:04, 4589.60it/s]Packing dataset:  58%|█████▊    | 29931/52002 [00:06<00:04, 4560.84it/s]Packing dataset:  58%|█████▊    | 30397/52002 [00:06<00:04, 4588.93it/s]Packing dataset:  59%|█████▉    | 30857/52002 [00:06<00:04, 4548.45it/s]Packing dataset:  60%|██████    | 31313/52002 [00:06<00:04, 4537.48it/s]Packing dataset:  61%|██████    | 31782/52002 [00:06<00:04, 4582.64it/s]Packing dataset:  62%|██████▏   | 32262/52002 [00:06<00:04, 4646.56it/s]Packing dataset:  63%|██████▎   | 32727/52002 [00:07<00:04, 4626.15it/s]Packing dataset:  64%|██████▍   | 33193/52002 [00:07<00:04, 4634.72it/s]Packing dataset:  65%|██████▍   | 33664/52002 [00:07<00:03, 4656.29it/s]Packing dataset:  66%|██████▌   | 34130/52002 [00:07<00:03, 4625.29it/s]Packing dataset:  67%|██████▋   | 34593/52002 [00:07<00:03, 4591.41it/s]Packing dataset:  67%|██████▋   | 35053/52002 [00:07<00:03, 4573.36it/s]Packing dataset:  68%|██████▊   | 35520/52002 [00:07<00:03, 4601.49it/s]Packing dataset:  69%|██████▉   | 35981/52002 [00:07<00:03, 4545.72it/s]Packing dataset:  70%|███████   | 36436/52002 [00:07<00:03, 4513.53it/s]Packing dataset:  71%|███████   | 36914/52002 [00:07<00:03, 4589.87it/s]Packing dataset:  72%|███████▏  | 37385/52002 [00:08<00:03, 4623.90it/s]Packing dataset:  73%|███████▎  | 37848/52002 [00:08<00:03, 4600.34it/s]Packing dataset:  74%|███████▎  | 38324/52002 [00:08<00:02, 4643.06it/s]Packing dataset:  75%|███████▍  | 38789/52002 [00:08<00:02, 4606.03it/s]Packing dataset:  75%|███████▌  | 39250/52002 [00:08<00:02, 4588.49it/s]Packing dataset:  76%|███████▋  | 39710/52002 [00:08<00:02, 4590.59it/s]Packing dataset:  77%|███████▋  | 40175/52002 [00:08<00:02, 4606.41it/s]Packing dataset:  78%|███████▊  | 40637/52002 [00:08<00:02, 4605.41it/s]Packing dataset:  79%|███████▉  | 41106/52002 [00:08<00:02, 4626.19it/s]Packing dataset:  80%|███████▉  | 41571/52002 [00:09<00:02, 4633.19it/s]Packing dataset:  81%|████████  | 42035/52002 [00:09<00:02, 4559.66it/s]Packing dataset:  82%|████████▏ | 42497/52002 [00:09<00:02, 4576.67it/s]Packing dataset:  83%|████████▎ | 42955/52002 [00:09<00:01, 4530.84it/s]Packing dataset:  83%|████████▎ | 43409/52002 [00:09<00:01, 4480.92it/s]Packing dataset:  84%|████████▍ | 43876/52002 [00:09<00:01, 4536.10it/s]Packing dataset:  85%|████████▌ | 44339/52002 [00:09<00:01, 4562.09it/s]Packing dataset:  86%|████████▌ | 44802/52002 [00:09<00:01, 4582.22it/s]Packing dataset:  87%|████████▋ | 45265/52002 [00:09<00:01, 4592.78it/s]Packing dataset:  88%|████████▊ | 45740/52002 [00:09<00:01, 4635.30it/s]Packing dataset:  89%|████████▉ | 46204/52002 [00:10<00:01, 4573.10it/s]Packing dataset:  90%|████████▉ | 46666/52002 [00:10<00:01, 4584.97it/s]Packing dataset:  91%|█████████ | 47135/52002 [00:10<00:01, 4614.73it/s]Packing dataset:  92%|█████████▏| 47597/52002 [00:10<00:00, 4558.70it/s]Packing dataset:  92%|█████████▏| 48054/52002 [00:10<00:00, 4553.14it/s]Packing dataset:  93%|█████████▎| 48513/52002 [00:10<00:00, 4563.78it/s]Packing dataset:  94%|█████████▍| 48977/52002 [00:10<00:00, 4584.79it/s]Packing dataset:  95%|█████████▌| 49436/52002 [00:10<00:00, 4573.10it/s]Packing dataset:  96%|█████████▌| 49894/52002 [00:10<00:00, 4550.73it/s]Packing dataset:  97%|█████████▋| 50350/52002 [00:10<00:00, 4535.32it/s]Packing dataset:  98%|█████████▊| 50806/52002 [00:11<00:00, 4541.30it/s]Packing dataset:  99%|█████████▊| 51274/52002 [00:11<00:00, 4582.18it/s]Packing dataset:  99%|█████████▉| 51733/52002 [00:11<00:00, 4580.14it/s]Packing dataset: 100%|██████████| 52002/52002 [00:11<00:00, 4603.97it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [01:11<10:39, 71.04s/it]1|1|Loss: 1.6724:  10%|█         | 1/10 [01:11<10:39, 71.04s/it]1|1|Loss: 1.6724:  20%|██        | 2/10 [01:31<05:28, 41.04s/it]1|2|Loss: 1.3797:  20%|██        | 2/10 [01:31<05:28, 41.04s/it]1|2|Loss: 1.3797:  30%|███       | 3/10 [01:51<03:40, 31.48s/it]1|3|Loss: 1.2925:  30%|███       | 3/10 [01:51<03:40, 31.48s/it]1|3|Loss: 1.2925:  40%|████      | 4/10 [02:11<02:41, 26.94s/it]1|4|Loss: 1.1819:  40%|████      | 4/10 [02:11<02:41, 26.94s/it]1|4|Loss: 1.1819:  50%|█████     | 5/10 [02:31<02:02, 24.45s/it]1|5|Loss: 0.9941:  50%|█████     | 5/10 [02:31<02:02, 24.45s/it]1|5|Loss: 0.9941:  60%|██████    | 6/10 [02:51<01:31, 22.98s/it]1|6|Loss: 1.0008:  60%|██████    | 6/10 [02:51<01:31, 22.98s/it]1|6|Loss: 1.0008:  70%|███████   | 7/10 [03:11<01:05, 21.98s/it]1|7|Loss: 0.9818:  70%|███████   | 7/10 [03:11<01:05, 21.98s/it]1|7|Loss: 0.9818:  80%|████████  | 8/10 [03:31<00:42, 21.35s/it]1|8|Loss: 1.1862:  80%|████████  | 8/10 [03:31<00:42, 21.35s/it]1|8|Loss: 1.1862:  90%|█████████ | 9/10 [03:51<00:20, 20.94s/it]1|9|Loss: 0.9450:  90%|█████████ | 9/10 [03:51<00:20, 20.94s/it]1|9|Loss: 0.9450: 100%|██████████| 10/10 [04:11<00:00, 20.65s/it]1|10|Loss: 0.7484: 100%|██████████| 10/10 [04:11<00:00, 20.65s/it]1|10|Loss: 0.7484: 100%|██████████| 10/10 [04:11<00:00, 25.13s/it]
iteration:  1 tokens:  900 time:  71.03914123796858 tokens_per_second_on_single_device:  12.67
iteration:  2 tokens:  845 time:  20.02822962985374 tokens_per_second_on_single_device:  42.19
iteration:  3 tokens:  916 time:  20.097195134032518 tokens_per_second_on_single_device:  45.58
iteration:  4 tokens:  867 time:  19.992138989968225 tokens_per_second_on_single_device:  43.37
iteration:  5 tokens:  831 time:  20.04043055092916 tokens_per_second_on_single_device:  41.47
iteration:  6 tokens:  859 time:  20.105521054938436 tokens_per_second_on_single_device:  42.72
iteration:  7 tokens:  917 time:  19.917143751867115 tokens_per_second_on_single_device:  46.04
iteration:  8 tokens:  835 time:  20.01415608613752 tokens_per_second_on_single_device:  41.72
iteration:  9 tokens:  848 time:  20.035644041141495 tokens_per_second_on_single_device:  42.32
iteration:  10 tokens:  892 time:  19.987665879074484 tokens_per_second_on_single_device:  44.63
avg tokens_per_second_on_single_device:  34.67
[W929 09:14:51.616674398 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
