Running FullFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: true
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 1
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/full_single_device/logs
model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
optimizer:
  _component_: torchao.optim.AdamW8bit
  lr: 2.0e-05
optimizer_in_bwd: true
output_dir: /tmp/torchtune/llama3_1_8B/full_single_device
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/full_single_device/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.02 GiB
	XPU peak memory alloc: 15.02 GiB
	XPU peak memory reserved: 15.14 GiB
Tokenizer is initialized from file.
Optimizer is initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/full_single_device/logs/log_1757304846.txt
Packing dataset:   0%|          | 0/52002 [00:00<?, ?it/s]Packing dataset:   1%|          | 419/52002 [00:00<00:12, 4184.67it/s]Packing dataset:   2%|▏         | 887/52002 [00:00<00:11, 4473.64it/s]Packing dataset:   3%|▎         | 1372/52002 [00:00<00:10, 4643.74it/s]Packing dataset:   4%|▎         | 1839/52002 [00:00<00:10, 4650.62it/s]Packing dataset:   4%|▍         | 2335/52002 [00:00<00:10, 4760.22it/s]Packing dataset:   5%|▌         | 2814/52002 [00:00<00:10, 4770.20it/s]Packing dataset:   6%|▋         | 3292/52002 [00:00<00:10, 4753.28it/s]Packing dataset:   7%|▋         | 3768/52002 [00:00<00:10, 4672.24it/s]Packing dataset:   8%|▊         | 4236/52002 [00:00<00:10, 4668.79it/s]Packing dataset:   9%|▉         | 4714/52002 [00:01<00:10, 4702.02it/s]Packing dataset:  10%|▉         | 5185/52002 [00:01<00:09, 4701.57it/s]Packing dataset:  11%|█         | 5670/52002 [00:01<00:09, 4745.60it/s]Packing dataset:  12%|█▏        | 6145/52002 [00:01<00:09, 4627.02it/s]Packing dataset:  13%|█▎        | 6611/52002 [00:01<00:09, 4636.36it/s]Packing dataset:  14%|█▎        | 7076/52002 [00:01<00:09, 4577.37it/s]Packing dataset:  14%|█▍        | 7535/52002 [00:01<00:09, 4558.89it/s]Packing dataset:  15%|█▌        | 7992/52002 [00:01<00:09, 4548.00it/s]Packing dataset:  16%|█▌        | 8450/52002 [00:01<00:09, 4557.26it/s]Packing dataset:  17%|█▋        | 8945/52002 [00:01<00:09, 4670.17it/s]Packing dataset:  18%|█▊        | 9415/52002 [00:02<00:09, 4677.73it/s]Packing dataset:  19%|█▉        | 9883/52002 [00:02<00:09, 4594.30it/s]Packing dataset:  20%|█▉        | 10352/52002 [00:02<00:09, 4621.46it/s]Packing dataset:  21%|██        | 10824/52002 [00:02<00:08, 4646.26it/s]Packing dataset:  22%|██▏       | 11289/52002 [00:02<00:08, 4588.74it/s]Packing dataset:  23%|██▎       | 11759/52002 [00:02<00:08, 4619.21it/s]Packing dataset:  24%|██▎       | 12222/52002 [00:02<00:08, 4558.82it/s]Packing dataset:  24%|██▍       | 12697/52002 [00:02<00:08, 4613.91it/s]Packing dataset:  25%|██▌       | 13163/52002 [00:02<00:08, 4627.01it/s]Packing dataset:  26%|██▌       | 13626/52002 [00:02<00:08, 4625.18it/s]Packing dataset:  27%|██▋       | 14089/52002 [00:03<00:08, 4531.87it/s]Packing dataset:  28%|██▊       | 14560/52002 [00:03<00:08, 4583.10it/s]Packing dataset:  29%|██▉       | 15024/52002 [00:03<00:08, 4598.66it/s]Packing dataset:  30%|██▉       | 15485/52002 [00:03<00:07, 4585.45it/s]Packing dataset:  31%|███       | 15944/52002 [00:03<00:08, 4500.07it/s]Packing dataset:  32%|███▏      | 16419/52002 [00:03<00:07, 4572.54it/s]Packing dataset:  32%|███▏      | 16893/52002 [00:03<00:07, 4621.70it/s]Packing dataset:  33%|███▎      | 17363/52002 [00:03<00:07, 4640.62it/s]Packing dataset:  34%|███▍      | 17830/52002 [00:03<00:07, 4648.17it/s]Packing dataset:  35%|███▌      | 18296/52002 [00:03<00:07, 4541.79it/s]Packing dataset:  36%|███▌      | 18751/52002 [00:04<00:07, 4534.81it/s]Packing dataset:  37%|███▋      | 19215/52002 [00:04<00:07, 4564.09it/s]Packing dataset:  38%|███▊      | 19672/52002 [00:04<00:07, 4554.70it/s]Packing dataset:  39%|███▊      | 20128/52002 [00:04<00:07, 4547.12it/s]Packing dataset:  40%|███▉      | 20593/52002 [00:04<00:06, 4577.67it/s]Packing dataset:  41%|████      | 21086/52002 [00:04<00:06, 4681.59it/s]Packing dataset:  41%|████▏     | 21556/52002 [00:04<00:06, 4686.09it/s]Packing dataset:  42%|████▏     | 22025/52002 [00:04<00:06, 4645.63it/s]Packing dataset:  43%|████▎     | 22490/52002 [00:04<00:06, 4638.76it/s]Packing dataset:  44%|████▍     | 22954/52002 [00:04<00:06, 4611.43it/s]Packing dataset:  45%|████▌     | 23429/52002 [00:05<00:06, 4650.66it/s]Packing dataset:  46%|████▌     | 23900/52002 [00:05<00:06, 4668.31it/s]Packing dataset:  47%|████▋     | 24367/52002 [00:05<00:05, 4635.00it/s]Packing dataset:  48%|████▊     | 24831/52002 [00:05<00:05, 4635.42it/s]Packing dataset:  49%|████▊     | 25295/52002 [00:05<00:05, 4629.34it/s]Packing dataset:  50%|████▉     | 25758/52002 [00:05<00:05, 4624.04it/s]Packing dataset:  50%|█████     | 26221/52002 [00:05<00:05, 4613.06it/s]Packing dataset:  51%|█████▏    | 26684/52002 [00:05<00:05, 4616.00it/s]Packing dataset:  52%|█████▏    | 27146/52002 [00:05<00:05, 4587.89it/s]Packing dataset:  53%|█████▎    | 27611/52002 [00:05<00:05, 4604.21it/s]Packing dataset:  54%|█████▍    | 28072/52002 [00:06<00:05, 4580.10it/s]Packing dataset:  55%|█████▍    | 28531/52002 [00:06<00:05, 4527.07it/s]Packing dataset:  56%|█████▌    | 28994/52002 [00:06<00:05, 4556.67it/s]Packing dataset:  57%|█████▋    | 29469/52002 [00:06<00:04, 4610.79it/s]Packing dataset:  58%|█████▊    | 29931/52002 [00:06<00:04, 4579.93it/s]Packing dataset:  58%|█████▊    | 30390/52002 [00:06<00:04, 4582.21it/s]Packing dataset:  59%|█████▉    | 30849/52002 [00:06<00:04, 4545.51it/s]Packing dataset:  60%|██████    | 31308/52002 [00:06<00:04, 4558.10it/s]Packing dataset:  61%|██████    | 31774/52002 [00:06<00:04, 4584.21it/s]Packing dataset:  62%|██████▏   | 32250/52002 [00:06<00:04, 4634.15it/s]Packing dataset:  63%|██████▎   | 32714/52002 [00:07<00:04, 4618.65it/s]Packing dataset:  64%|██████▍   | 33184/52002 [00:07<00:04, 4642.36it/s]Packing dataset:  65%|██████▍   | 33657/52002 [00:07<00:03, 4667.38it/s]Packing dataset:  66%|██████▌   | 34124/52002 [00:07<00:03, 4636.35it/s]Packing dataset:  67%|██████▋   | 34588/52002 [00:07<00:03, 4586.83it/s]Packing dataset:  67%|██████▋   | 35049/52002 [00:07<00:03, 4593.42it/s]Packing dataset:  68%|██████▊   | 35518/52002 [00:07<00:03, 4619.63it/s]Packing dataset:  69%|██████▉   | 35981/52002 [00:07<00:03, 4560.73it/s]Packing dataset:  70%|███████   | 36438/52002 [00:07<00:03, 4504.99it/s]Packing dataset:  71%|███████   | 36917/52002 [00:08<00:03, 4587.24it/s]Packing dataset:  72%|███████▏  | 37392/52002 [00:08<00:03, 4633.03it/s]Packing dataset:  73%|███████▎  | 37856/52002 [00:08<00:03, 4612.38it/s]Packing dataset:  74%|███████▎  | 38326/52002 [00:08<00:02, 4637.30it/s]Packing dataset:  75%|███████▍  | 38790/52002 [00:08<00:02, 4604.63it/s]Packing dataset:  75%|███████▌  | 39252/52002 [00:08<00:02, 4607.61it/s]Packing dataset:  76%|███████▋  | 39713/52002 [00:08<00:02, 4603.99it/s]Packing dataset:  77%|███████▋  | 40180/52002 [00:08<00:02, 4621.18it/s]Packing dataset:  78%|███████▊  | 40643/52002 [00:08<00:02, 4577.28it/s]Packing dataset:  79%|███████▉  | 41116/52002 [00:08<00:02, 4620.08it/s]Packing dataset:  80%|███████▉  | 41585/52002 [00:09<00:02, 4639.56it/s]Packing dataset:  81%|████████  | 42050/52002 [00:09<00:02, 4575.03it/s]Packing dataset:  82%|████████▏ | 42508/52002 [00:09<00:02, 4572.38it/s]Packing dataset:  83%|████████▎ | 42966/52002 [00:09<00:01, 4547.45it/s]Packing dataset:  83%|████████▎ | 43421/52002 [00:09<00:01, 4525.45it/s]Packing dataset:  84%|████████▍ | 43891/52002 [00:09<00:01, 4575.33it/s]Packing dataset:  85%|████████▌ | 44352/52002 [00:09<00:01, 4584.37it/s]Packing dataset:  86%|████████▌ | 44812/52002 [00:09<00:01, 4588.62it/s]Packing dataset:  87%|████████▋ | 45282/52002 [00:09<00:01, 4621.73it/s]Packing dataset:  88%|████████▊ | 45755/52002 [00:09<00:01, 4653.01it/s]Packing dataset:  89%|████████▉ | 46221/52002 [00:10<00:01, 4586.32it/s]Packing dataset:  90%|████████▉ | 46680/52002 [00:10<00:01, 4576.60it/s]Packing dataset:  91%|█████████ | 47150/52002 [00:10<00:01, 4611.88it/s]Packing dataset:  92%|█████████▏| 47612/52002 [00:10<00:00, 4575.06it/s]Packing dataset:  92%|█████████▏| 48070/52002 [00:10<00:00, 4566.40it/s]Packing dataset:  93%|█████████▎| 48527/52002 [00:10<00:00, 4557.70it/s]Packing dataset:  94%|█████████▍| 48989/52002 [00:10<00:00, 4575.72it/s]Packing dataset:  95%|█████████▌| 49451/52002 [00:10<00:00, 4586.64it/s]Packing dataset:  96%|█████████▌| 49910/52002 [00:10<00:00, 4550.70it/s]Packing dataset:  97%|█████████▋| 50366/52002 [00:10<00:00, 4544.87it/s]Packing dataset:  98%|█████████▊| 50821/52002 [00:11<00:00, 4525.98it/s]Packing dataset:  99%|█████████▊| 51294/52002 [00:11<00:00, 4582.15it/s]Packing dataset: 100%|█████████▉| 51753/52002 [00:11<00:00, 4580.97it/s]Packing dataset: 100%|██████████| 52002/52002 [00:11<00:00, 4602.65it/s]
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:30<04:36, 30.76s/it]1|1|Loss: 1.6726:  10%|█         | 1/10 [00:30<04:36, 30.76s/it]1|1|Loss: 1.6726:  20%|██        | 2/10 [00:32<01:47, 13.40s/it]1|2|Loss: 1.3799:  20%|██        | 2/10 [00:32<01:47, 13.40s/it]1|2|Loss: 1.3799:  30%|███       | 3/10 [00:33<00:55,  7.86s/it]1|3|Loss: 1.2942:  30%|███       | 3/10 [00:33<00:55,  7.86s/it]1|3|Loss: 1.2942:  40%|████      | 4/10 [00:34<00:31,  5.25s/it]1|4|Loss: 1.1798:  40%|████      | 4/10 [00:34<00:31,  5.25s/it]1|4|Loss: 1.1798:  50%|█████     | 5/10 [00:35<00:19,  3.80s/it]1|5|Loss: 0.9958:  50%|█████     | 5/10 [00:35<00:19,  3.80s/it]1|5|Loss: 0.9958:  60%|██████    | 6/10 [00:36<00:11,  2.93s/it]1|6|Loss: 1.0017:  60%|██████    | 6/10 [00:36<00:11,  2.93s/it]1|6|Loss: 1.0017:  70%|███████   | 7/10 [00:38<00:07,  2.38s/it]1|7|Loss: 0.9824:  70%|███████   | 7/10 [00:38<00:07,  2.38s/it]1|7|Loss: 0.9824:  80%|████████  | 8/10 [00:39<00:04,  2.02s/it]1|8|Loss: 1.1837:  80%|████████  | 8/10 [00:39<00:04,  2.02s/it]1|8|Loss: 1.1837:  90%|█████████ | 9/10 [00:40<00:01,  1.77s/it]1|9|Loss: 0.9446:  90%|█████████ | 9/10 [00:40<00:01,  1.77s/it]1|9|Loss: 0.9446: 100%|██████████| 10/10 [00:41<00:00,  1.61s/it]1|10|Loss: 0.7472: 100%|██████████| 10/10 [00:41<00:00,  1.61s/it]1|10|Loss: 0.7472: 100%|██████████| 10/10 [00:41<00:00,  4.20s/it]
iteration:  1 tokens:  900 time:  30.762453025206923 tokens_per_second_on_single_device:  29.26
iteration:  2 tokens:  845 time:  1.237785680219531 tokens_per_second_on_single_device:  682.67
iteration:  3 tokens:  916 time:  1.2572837825864553 tokens_per_second_on_single_device:  728.55
iteration:  4 tokens:  867 time:  1.2444764468818903 tokens_per_second_on_single_device:  696.68
iteration:  5 tokens:  831 time:  1.235030840151012 tokens_per_second_on_single_device:  672.86
iteration:  6 tokens:  859 time:  1.2431067768484354 tokens_per_second_on_single_device:  691.01
iteration:  7 tokens:  917 time:  1.2507197922095656 tokens_per_second_on_single_device:  733.18
iteration:  8 tokens:  835 time:  1.232396574690938 tokens_per_second_on_single_device:  677.54
iteration:  9 tokens:  848 time:  1.2364946957677603 tokens_per_second_on_single_device:  685.81
iteration:  10 tokens:  892 time:  1.2451014909893274 tokens_per_second_on_single_device:  716.41
avg tokens_per_second_on_single_device:  207.65
[W908 04:15:04.790971427 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
