Running LoRAFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/lora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/lora/logs
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/lora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/lora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/utils/_device.py:103: UserWarning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (Triggered internally at /home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/c10/core/AllocatorConfig.cpp:28.)
  return func(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:436: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.06 GiB
	XPU peak memory alloc: 15.06 GiB
	XPU peak memory reserved: 15.18 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/lora/logs/log_1755443691.txt
Packing dataset:   0%|          | 0/49172 [00:00<?, ?it/s]Packing dataset:   1%|          | 254/49172 [00:00<00:19, 2539.12it/s]Packing dataset:   1%|          | 557/49172 [00:00<00:17, 2824.94it/s]Packing dataset:   2%|▏         | 861/49172 [00:00<00:16, 2922.85it/s]Packing dataset:   2%|▏         | 1200/49172 [00:00<00:15, 3104.70it/s]Packing dataset:   3%|▎         | 1515/49172 [00:00<00:15, 3120.50it/s]Packing dataset:   4%|▎         | 1828/49172 [00:00<00:15, 3026.87it/s]Packing dataset:   4%|▍         | 2152/49172 [00:00<00:15, 3094.80it/s]Packing dataset:   5%|▌         | 2489/49172 [00:00<00:14, 3179.87it/s]Packing dataset:   6%|▌         | 2845/49172 [00:00<00:14, 3292.61it/s]Packing dataset:   6%|▋         | 3175/49172 [00:01<00:14, 3280.74it/s]Packing dataset:   7%|▋         | 3504/49172 [00:01<00:14, 3227.25it/s]Packing dataset:   8%|▊         | 3828/49172 [00:01<00:14, 3218.09it/s]Packing dataset:   8%|▊         | 4151/49172 [00:01<00:14, 3175.97it/s]Packing dataset:   9%|▉         | 4477/49172 [00:01<00:13, 3200.46it/s]Packing dataset:  10%|▉         | 4798/49172 [00:01<00:13, 3186.76it/s]Packing dataset:  10%|█         | 5120/49172 [00:01<00:13, 3195.65it/s]Packing dataset:  11%|█         | 5440/49172 [00:01<00:13, 3185.49it/s]Packing dataset:  12%|█▏        | 5774/49172 [00:01<00:13, 3230.79it/s]Packing dataset:  12%|█▏        | 6098/49172 [00:01<00:13, 3209.56it/s]Packing dataset:  13%|█▎        | 6421/49172 [00:02<00:13, 3213.51it/s]Packing dataset:  14%|█▎        | 6743/49172 [00:02<00:13, 3152.52it/s]Packing dataset:  14%|█▍        | 7060/49172 [00:02<00:13, 3157.45it/s]Packing dataset:  15%|█▌        | 7376/49172 [00:02<00:13, 3143.02it/s]Packing dataset:  16%|█▌        | 7710/49172 [00:02<00:12, 3200.95it/s]Packing dataset:  16%|█▋        | 8031/49172 [00:02<00:13, 3130.93it/s]Packing dataset:  17%|█▋        | 8345/49172 [00:02<00:13, 3092.01it/s]Packing dataset:  18%|█▊        | 8675/49172 [00:02<00:12, 3149.49it/s]Packing dataset:  18%|█▊        | 9005/49172 [00:02<00:12, 3192.53it/s]Packing dataset:  19%|█▉        | 9325/49172 [00:02<00:12, 3172.21it/s]Packing dataset:  20%|█▉        | 9643/49172 [00:03<00:12, 3089.79it/s]Packing dataset:  20%|██        | 9953/49172 [00:03<00:12, 3066.23it/s]Packing dataset:  21%|██        | 10285/49172 [00:03<00:12, 3138.15it/s]Packing dataset:  22%|██▏       | 10604/49172 [00:03<00:12, 3153.19it/s]Packing dataset:  22%|██▏       | 10920/49172 [00:03<00:12, 3147.45it/s]Packing dataset:  23%|██▎       | 11235/49172 [00:03<00:12, 3126.32it/s]Packing dataset:  23%|██▎       | 11548/49172 [00:03<00:12, 3125.56it/s]Packing dataset:  24%|██▍       | 11869/49172 [00:03<00:11, 3149.01it/s]Packing dataset:  25%|██▍       | 12185/49172 [00:03<00:12, 3077.56it/s]Packing dataset:  25%|██▌       | 12504/49172 [00:03<00:11, 3108.31it/s]Packing dataset:  26%|██▌       | 12816/49172 [00:04<00:11, 3107.94it/s]Packing dataset:  27%|██▋       | 13132/49172 [00:04<00:11, 3120.27it/s]Packing dataset:  27%|██▋       | 13445/49172 [00:04<00:11, 3102.72it/s]Packing dataset:  28%|██▊       | 13756/49172 [00:04<00:11, 3102.55it/s]Packing dataset:  29%|██▊       | 14067/49172 [00:04<00:11, 3042.87it/s]Packing dataset:  29%|██▉       | 14404/49172 [00:04<00:11, 3138.39it/s]Packing dataset:  30%|██▉       | 14719/49172 [00:04<00:11, 3123.36it/s]Packing dataset:  31%|███       | 15032/49172 [00:04<00:11, 3103.14it/s]Packing dataset:  31%|███       | 15349/49172 [00:04<00:10, 3122.59it/s]Packing dataset:  32%|███▏      | 15678/49172 [00:04<00:10, 3170.51it/s]Packing dataset:  33%|███▎      | 16009/49172 [00:05<00:10, 3211.58it/s]Packing dataset:  33%|███▎      | 16331/49172 [00:05<00:10, 3145.86it/s]Packing dataset:  34%|███▍      | 16646/49172 [00:05<00:10, 3140.41it/s]Packing dataset:  34%|███▍      | 16961/49172 [00:05<00:10, 3124.01it/s]Packing dataset:  35%|███▌      | 17274/49172 [00:05<00:10, 3112.12it/s]Packing dataset:  36%|███▌      | 17586/49172 [00:05<00:10, 3083.54it/s]Packing dataset:  36%|███▋      | 17895/49172 [00:05<00:10, 3048.47it/s]Packing dataset:  37%|███▋      | 18201/49172 [00:05<00:10, 3051.29it/s]Packing dataset:  38%|███▊      | 18507/49172 [00:05<00:10, 3033.21it/s]Packing dataset:  38%|███▊      | 18837/49172 [00:06<00:09, 3110.42it/s]Packing dataset:  39%|███▉      | 19151/49172 [00:06<00:09, 3118.21it/s]Packing dataset:  40%|███▉      | 19463/49172 [00:06<00:09, 3067.83it/s]Packing dataset:  40%|████      | 19771/49172 [00:06<00:09, 3049.14it/s]Packing dataset:  41%|████      | 20083/49172 [00:06<00:09, 3069.66it/s]Packing dataset:  41%|████▏     | 20391/49172 [00:06<00:09, 3057.26it/s]Packing dataset:  42%|████▏     | 20700/49172 [00:06<00:09, 3066.75it/s]Packing dataset:  43%|████▎     | 21031/49172 [00:06<00:08, 3138.11it/s]Packing dataset:  43%|████▎     | 21345/49172 [00:06<00:08, 3119.49it/s]Packing dataset:  44%|████▍     | 21660/49172 [00:06<00:08, 3127.90it/s]Packing dataset:  45%|████▍     | 21973/49172 [00:07<00:08, 3091.17it/s]Packing dataset:  45%|████▌     | 22283/49172 [00:07<00:08, 3041.48it/s]Packing dataset:  46%|████▌     | 22601/49172 [00:07<00:08, 3082.17it/s]Packing dataset:  47%|████▋     | 22910/49172 [00:07<00:08, 3049.48it/s]Packing dataset:  47%|████▋     | 23216/49172 [00:07<00:08, 3046.24it/s]Packing dataset:  48%|████▊     | 23543/49172 [00:07<00:08, 3109.64it/s]Packing dataset:  49%|████▊     | 23855/49172 [00:07<00:08, 3092.91it/s]Packing dataset:  49%|████▉     | 24169/49172 [00:07<00:08, 3106.47it/s]Packing dataset:  50%|████▉     | 24496/49172 [00:07<00:07, 3154.44it/s]Packing dataset:  50%|█████     | 24812/49172 [00:07<00:07, 3146.34it/s]Packing dataset:  51%|█████     | 25127/49172 [00:08<00:07, 3114.66it/s]Packing dataset:  52%|█████▏    | 25444/49172 [00:08<00:07, 3127.52it/s]Packing dataset:  52%|█████▏    | 25757/49172 [00:08<00:07, 3078.55it/s]Packing dataset:  53%|█████▎    | 26067/49172 [00:08<00:07, 3084.08it/s]Packing dataset:  54%|█████▎    | 26376/49172 [00:08<00:07, 3066.99it/s]Packing dataset:  54%|█████▍    | 26683/49172 [00:08<00:07, 3067.81it/s]Packing dataset:  55%|█████▍    | 27018/49172 [00:08<00:07, 3146.05it/s]Packing dataset:  56%|█████▌    | 27333/49172 [00:08<00:06, 3144.62it/s]Packing dataset:  56%|█████▌    | 27648/49172 [00:08<00:06, 3093.37it/s]Packing dataset:  57%|█████▋    | 27958/49172 [00:08<00:06, 3076.30it/s]Packing dataset:  57%|█████▋    | 28266/49172 [00:09<00:06, 3035.51it/s]Packing dataset:  58%|█████▊    | 28570/49172 [00:09<00:06, 3023.19it/s]Packing dataset:  59%|█████▊    | 28874/49172 [00:09<00:06, 3025.20it/s]Packing dataset:  59%|█████▉    | 29177/49172 [00:09<00:06, 3026.07it/s]Packing dataset:  60%|█████▉    | 29481/49172 [00:09<00:06, 3026.29it/s]Packing dataset:  61%|██████    | 29792/49172 [00:09<00:06, 3049.12it/s]Packing dataset:  61%|██████    | 30097/49172 [00:09<00:06, 3032.15it/s]Packing dataset:  62%|██████▏   | 30405/49172 [00:09<00:06, 3045.25it/s]Packing dataset:  62%|██████▏   | 30710/49172 [00:09<00:08, 2250.45it/s]Packing dataset:  63%|██████▎   | 31005/49172 [00:10<00:07, 2417.45it/s]Packing dataset:  64%|██████▎   | 31301/49172 [00:10<00:06, 2554.30it/s]Packing dataset:  64%|██████▍   | 31626/49172 [00:10<00:06, 2737.20it/s]Packing dataset:  65%|██████▍   | 31942/49172 [00:10<00:06, 2852.64it/s]Packing dataset:  66%|██████▌   | 32240/49172 [00:10<00:05, 2868.62it/s]Packing dataset:  66%|██████▌   | 32551/49172 [00:10<00:05, 2933.94it/s]Packing dataset:  67%|██████▋   | 32873/49172 [00:10<00:05, 3014.42it/s]Packing dataset:  67%|██████▋   | 33180/49172 [00:10<00:05, 3009.25it/s]Packing dataset:  68%|██████▊   | 33505/49172 [00:10<00:05, 3077.11it/s]Packing dataset:  69%|██████▉   | 33816/49172 [00:11<00:05, 3056.43it/s]Packing dataset:  69%|██████▉   | 34129/49172 [00:11<00:04, 3077.19it/s]Packing dataset:  70%|███████   | 34438/49172 [00:11<00:04, 3038.72it/s]Packing dataset:  71%|███████   | 34757/49172 [00:11<00:04, 3079.28it/s]Packing dataset:  71%|███████▏  | 35066/49172 [00:11<00:04, 3081.51it/s]Packing dataset:  72%|███████▏  | 35381/49172 [00:11<00:04, 3099.37it/s]Packing dataset:  73%|███████▎  | 35692/49172 [00:11<00:04, 3089.18it/s]Packing dataset:  73%|███████▎  | 36002/49172 [00:11<00:04, 3022.99it/s]Packing dataset:  74%|███████▍  | 36305/49172 [00:11<00:04, 3002.39it/s]Packing dataset:  74%|███████▍  | 36628/49172 [00:11<00:04, 3066.43it/s]Packing dataset:  75%|███████▌  | 36936/49172 [00:12<00:03, 3068.49it/s]Packing dataset:  76%|███████▌  | 37251/49172 [00:12<00:03, 3091.27it/s]Packing dataset:  76%|███████▋  | 37565/49172 [00:12<00:03, 3105.51it/s]Packing dataset:  77%|███████▋  | 37879/49172 [00:12<00:03, 3114.24it/s]Packing dataset:  78%|███████▊  | 38191/49172 [00:12<00:03, 3082.72it/s]Packing dataset:  78%|███████▊  | 38500/49172 [00:12<00:03, 3069.16it/s]Packing dataset:  79%|███████▉  | 38808/49172 [00:12<00:03, 3036.94it/s]Packing dataset:  80%|███████▉  | 39130/49172 [00:12<00:03, 3090.09it/s]Packing dataset:  80%|████████  | 39440/49172 [00:12<00:03, 3077.16it/s]Packing dataset:  81%|████████  | 39749/49172 [00:12<00:03, 3080.12it/s]Packing dataset:  81%|████████▏ | 40065/49172 [00:13<00:02, 3100.96it/s]Packing dataset:  82%|████████▏ | 40376/49172 [00:13<00:02, 3082.74it/s]Packing dataset:  83%|████████▎ | 40695/49172 [00:13<00:02, 3114.40it/s]Packing dataset:  83%|████████▎ | 41012/49172 [00:13<00:02, 3127.96it/s]Packing dataset:  84%|████████▍ | 41325/49172 [00:13<00:02, 3122.49it/s]Packing dataset:  85%|████████▍ | 41638/49172 [00:13<00:02, 3052.53it/s]Packing dataset:  85%|████████▌ | 41944/49172 [00:13<00:02, 3033.54it/s]Packing dataset:  86%|████████▌ | 42251/49172 [00:13<00:02, 3043.96it/s]Packing dataset:  87%|████████▋ | 42556/49172 [00:13<00:02, 3001.08it/s]Packing dataset:  87%|████████▋ | 42857/49172 [00:13<00:02, 2995.85it/s]Packing dataset:  88%|████████▊ | 43157/49172 [00:14<00:02, 2981.10it/s]Packing dataset:  88%|████████▊ | 43456/49172 [00:14<00:01, 2976.82it/s]Packing dataset:  89%|████████▉ | 43766/49172 [00:14<00:01, 3012.92it/s]Packing dataset:  90%|████████▉ | 44069/49172 [00:14<00:01, 3016.61it/s]Packing dataset:  90%|█████████ | 44375/49172 [00:14<00:01, 3027.61it/s]Packing dataset:  91%|█████████ | 44678/49172 [00:14<00:01, 3020.65it/s]Packing dataset:  92%|█████████▏| 44993/49172 [00:14<00:01, 3056.99it/s]Packing dataset:  92%|█████████▏| 45315/49172 [00:14<00:01, 3102.21it/s]Packing dataset:  93%|█████████▎| 45626/49172 [00:14<00:01, 3085.26it/s]Packing dataset:  93%|█████████▎| 45935/49172 [00:14<00:01, 3044.84it/s]Packing dataset:  94%|█████████▍| 46247/49172 [00:15<00:00, 3062.93it/s]Packing dataset:  95%|█████████▍| 46554/49172 [00:15<00:00, 3026.74it/s]Packing dataset:  95%|█████████▌| 46878/49172 [00:15<00:00, 3085.58it/s]Packing dataset:  96%|█████████▌| 47187/49172 [00:15<00:00, 3058.14it/s]Packing dataset:  97%|█████████▋| 47493/49172 [00:15<00:00, 3043.88it/s]Packing dataset:  97%|█████████▋| 47798/49172 [00:15<00:00, 3039.02it/s]Packing dataset:  98%|█████████▊| 48102/49172 [00:15<00:00, 3018.27it/s]Packing dataset:  98%|█████████▊| 48408/49172 [00:15<00:00, 3029.10it/s]Packing dataset:  99%|█████████▉| 48729/49172 [00:15<00:00, 3081.16it/s]Packing dataset: 100%|█████████▉| 49038/49172 [00:15<00:00, 3072.09it/s]Packing dataset: 100%|██████████| 49172/49172 [00:16<00:00, 3069.54it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:626: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:05<00:49,  5.51s/it]1|1|Loss: 1.7898133993148804:  10%|█         | 1/10 [00:05<00:49,  5.51s/it]1|1|Loss: 1.7898133993148804:  20%|██        | 2/10 [00:09<00:37,  4.65s/it]1|2|Loss: 1.7190043926239014:  20%|██        | 2/10 [00:09<00:37,  4.65s/it]1|2|Loss: 1.7190043926239014:  30%|███       | 3/10 [00:13<00:30,  4.32s/it]1|3|Loss: 1.526566743850708:  30%|███       | 3/10 [00:13<00:30,  4.32s/it] 1|3|Loss: 1.526566743850708:  40%|████      | 4/10 [00:17<00:24,  4.16s/it]1|4|Loss: 1.8982152938842773:  40%|████      | 4/10 [00:17<00:24,  4.16s/it]1|4|Loss: 1.8982152938842773:  50%|█████     | 5/10 [00:21<00:20,  4.08s/it]1|5|Loss: 1.6341384649276733:  50%|█████     | 5/10 [00:21<00:20,  4.08s/it]1|5|Loss: 1.6341384649276733:  60%|██████    | 6/10 [00:25<00:16,  4.04s/it]1|6|Loss: 1.7735509872436523:  60%|██████    | 6/10 [00:25<00:16,  4.04s/it]1|6|Loss: 1.7735509872436523:  70%|███████   | 7/10 [00:29<00:12,  4.00s/it]1|7|Loss: 1.7892916202545166:  70%|███████   | 7/10 [00:29<00:12,  4.00s/it]1|7|Loss: 1.7892916202545166:  80%|████████  | 8/10 [00:33<00:07,  3.96s/it]1|8|Loss: 1.721407175064087:  80%|████████  | 8/10 [00:33<00:07,  3.96s/it] 1|8|Loss: 1.721407175064087:  90%|█████████ | 9/10 [00:37<00:03,  3.96s/it]1|9|Loss: 1.6448736190795898:  90%|█████████ | 9/10 [00:37<00:03,  3.96s/it]1|9|Loss: 1.6448736190795898: 100%|██████████| 10/10 [00:40<00:00,  3.95s/it]1|10|Loss: 1.6638606786727905: 100%|██████████| 10/10 [00:40<00:00,  3.95s/it]Starting checkpoint save...
Checkpoint saved in 0.00 seconds.
1|10|Loss: 1.6638606786727905: 100%|██████████| 10/10 [00:41<00:00,  4.10s/it]
iteration:  1 tokens:  6269 time:  5.509711740072817 tokens_per_second_on_single_device:  1137.81
iteration:  2 tokens:  6245 time:  4.013349640648812 tokens_per_second_on_single_device:  1556.06
iteration:  3 tokens:  6258 time:  3.9249737840145826 tokens_per_second_on_single_device:  1594.41
iteration:  4 tokens:  5878 time:  3.8997156070545316 tokens_per_second_on_single_device:  1507.29
iteration:  5 tokens:  6035 time:  3.9205355937592685 tokens_per_second_on_single_device:  1539.33
iteration:  6 tokens:  6485 time:  3.9388823793269694 tokens_per_second_on_single_device:  1646.41
iteration:  7 tokens:  6039 time:  3.89958073804155 tokens_per_second_on_single_device:  1548.63
iteration:  8 tokens:  5467 time:  3.855949741322547 tokens_per_second_on_single_device:  1417.81
iteration:  9 tokens:  6381 time:  3.9322115597315133 tokens_per_second_on_single_device:  1622.75
iteration:  10 tokens:  6351 time:  3.931052438914776 tokens_per_second_on_single_device:  1615.6
avg tokens_per_second_on_single_device:  1561.96
