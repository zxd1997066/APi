Running LoRAFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/lora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/lora/logs
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/lora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/lora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:436: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.06 GiB
	XPU peak memory alloc: 15.06 GiB
	XPU peak memory reserved: 15.18 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/lora/logs/log_1753630122.txt
Packing dataset:   0%|          | 0/49172 [00:00<?, ?it/s]Packing dataset:   0%|          | 162/49172 [00:00<00:30, 1595.86it/s]Packing dataset:   1%|          | 357/49172 [00:00<00:27, 1801.82it/s]Packing dataset:   1%|          | 544/49172 [00:00<00:26, 1828.17it/s]Packing dataset:   1%|▏         | 727/49172 [00:00<00:27, 1792.96it/s]Packing dataset:   2%|▏         | 907/49172 [00:00<00:27, 1782.30it/s]Packing dataset:   2%|▏         | 1103/49172 [00:00<00:26, 1837.23it/s]Packing dataset:   3%|▎         | 1295/49172 [00:00<00:25, 1861.43it/s]Packing dataset:   3%|▎         | 1482/49172 [00:00<00:25, 1841.06it/s]Packing dataset:   3%|▎         | 1667/49172 [00:00<00:26, 1779.04it/s]Packing dataset:   4%|▍         | 1846/49172 [00:01<00:27, 1748.82it/s]Packing dataset:   4%|▍         | 2022/49172 [00:01<00:26, 1751.55it/s]Packing dataset:   5%|▍         | 2216/49172 [00:01<00:25, 1806.00it/s]Packing dataset:   5%|▍         | 2411/49172 [00:01<00:25, 1848.61it/s]Packing dataset:   5%|▌         | 2605/49172 [00:01<00:24, 1875.47it/s]Packing dataset:   6%|▌         | 2806/49172 [00:01<00:24, 1913.02it/s]Packing dataset:   6%|▌         | 2998/49172 [00:01<00:24, 1885.88it/s]Packing dataset:   6%|▋         | 3190/49172 [00:01<00:24, 1895.59it/s]Packing dataset:   7%|▋         | 3380/49172 [00:01<00:24, 1839.64it/s]Packing dataset:   7%|▋         | 3568/49172 [00:01<00:24, 1847.28it/s]Packing dataset:   8%|▊         | 3754/49172 [00:02<00:24, 1849.96it/s]Packing dataset:   8%|▊         | 3949/49172 [00:02<00:24, 1878.97it/s]Packing dataset:   8%|▊         | 4138/49172 [00:02<00:25, 1794.37it/s]Packing dataset:   9%|▉         | 4339/49172 [00:02<00:24, 1847.90it/s]Packing dataset:   9%|▉         | 4525/49172 [00:02<00:24, 1837.25it/s]Packing dataset:  10%|▉         | 4710/49172 [00:02<00:24, 1815.66it/s]Packing dataset:  10%|▉         | 4893/49172 [00:02<00:24, 1819.14it/s]Packing dataset:  10%|█         | 5083/49172 [00:02<00:23, 1839.34it/s]Packing dataset:  11%|█         | 5268/49172 [00:02<00:24, 1801.63it/s]Packing dataset:  11%|█         | 5461/49172 [00:02<00:23, 1837.97it/s]Packing dataset:  11%|█▏        | 5646/49172 [00:03<00:23, 1836.66it/s]Packing dataset:  12%|█▏        | 5841/49172 [00:03<00:23, 1866.94it/s]Packing dataset:  12%|█▏        | 6028/49172 [00:03<00:23, 1827.31it/s]Packing dataset:  13%|█▎        | 6220/49172 [00:03<00:23, 1851.31it/s]Packing dataset:  13%|█▎        | 6406/49172 [00:03<00:23, 1825.79it/s]Packing dataset:  13%|█▎        | 6589/49172 [00:03<00:23, 1787.32it/s]Packing dataset:  14%|█▍        | 6768/49172 [00:03<00:23, 1785.06it/s]Packing dataset:  14%|█▍        | 6950/49172 [00:03<00:23, 1794.30it/s]Packing dataset:  15%|█▍        | 7133/49172 [00:03<00:23, 1801.38it/s]Packing dataset:  15%|█▍        | 7314/49172 [00:04<00:23, 1787.22it/s]Packing dataset:  15%|█▌        | 7493/49172 [00:04<00:23, 1785.27it/s]Packing dataset:  16%|█▌        | 7690/49172 [00:04<00:22, 1838.63it/s]Packing dataset:  16%|█▌        | 7874/49172 [00:04<00:22, 1798.51it/s]Packing dataset:  16%|█▋        | 8055/49172 [00:04<00:22, 1792.28it/s]Packing dataset:  17%|█▋        | 8235/49172 [00:04<00:23, 1767.87it/s]Packing dataset:  17%|█▋        | 8412/49172 [00:04<00:23, 1767.99it/s]Packing dataset:  17%|█▋        | 8603/49172 [00:04<00:22, 1809.25it/s]Packing dataset:  18%|█▊        | 8797/49172 [00:04<00:21, 1846.07it/s]Packing dataset:  18%|█▊        | 8983/49172 [00:04<00:21, 1848.54it/s]Packing dataset:  19%|█▊        | 9168/49172 [00:05<00:21, 1828.96it/s]Packing dataset:  19%|█▉        | 9408/49172 [00:05<00:19, 1997.81it/s]Packing dataset:  20%|█▉        | 9671/49172 [00:05<00:18, 2185.58it/s]Packing dataset:  20%|██        | 9968/49172 [00:05<00:16, 2419.36it/s]Packing dataset:  21%|██        | 10280/49172 [00:05<00:14, 2625.27it/s]Packing dataset:  22%|██▏       | 10587/49172 [00:05<00:13, 2756.13it/s]Packing dataset:  22%|██▏       | 10880/49172 [00:05<00:13, 2806.83it/s]Packing dataset:  23%|██▎       | 11180/49172 [00:05<00:13, 2862.93it/s]Packing dataset:  23%|██▎       | 11476/49172 [00:05<00:13, 2889.05it/s]Packing dataset:  24%|██▍       | 11780/49172 [00:05<00:12, 2933.79it/s]Packing dataset:  25%|██▍       | 12074/49172 [00:06<00:12, 2899.00it/s]Packing dataset:  25%|██▌       | 12375/49172 [00:06<00:12, 2930.22it/s]Packing dataset:  26%|██▌       | 12673/49172 [00:06<00:12, 2944.29it/s]Packing dataset:  26%|██▋       | 12968/49172 [00:06<00:12, 2894.85it/s]Packing dataset:  27%|██▋       | 13266/49172 [00:06<00:12, 2918.62it/s]Packing dataset:  28%|██▊       | 13560/49172 [00:06<00:12, 2924.33it/s]Packing dataset:  28%|██▊       | 13862/49172 [00:06<00:11, 2951.96it/s]Packing dataset:  29%|██▉       | 14158/49172 [00:06<00:12, 2916.17it/s]Packing dataset:  29%|██▉       | 14475/49172 [00:06<00:11, 2991.24it/s]Packing dataset:  30%|███       | 14775/49172 [00:06<00:11, 2963.94it/s]Packing dataset:  31%|███       | 15072/49172 [00:07<00:11, 2959.10it/s]Packing dataset:  31%|███▏      | 15382/49172 [00:07<00:11, 2999.45it/s]Packing dataset:  32%|███▏      | 15692/49172 [00:07<00:11, 3026.65it/s]Packing dataset:  33%|███▎      | 16009/49172 [00:07<00:10, 3067.23it/s]Packing dataset:  33%|███▎      | 16316/49172 [00:07<00:10, 3005.83it/s]Packing dataset:  34%|███▍      | 16617/49172 [00:07<00:10, 2966.37it/s]Packing dataset:  34%|███▍      | 16921/49172 [00:07<00:10, 2986.63it/s]Packing dataset:  35%|███▌      | 17220/49172 [00:07<00:10, 2969.48it/s]Packing dataset:  36%|███▌      | 17518/49172 [00:07<00:10, 2924.08it/s]Packing dataset:  36%|███▌      | 17815/49172 [00:07<00:10, 2936.31it/s]Packing dataset:  37%|███▋      | 18112/49172 [00:08<00:10, 2945.64it/s]Packing dataset:  37%|███▋      | 18407/49172 [00:08<00:10, 2879.61it/s]Packing dataset:  38%|███▊      | 18721/49172 [00:08<00:10, 2954.16it/s]Packing dataset:  39%|███▊      | 19017/49172 [00:08<00:10, 2927.48it/s]Packing dataset:  39%|███▉      | 19311/49172 [00:08<00:10, 2920.49it/s]Packing dataset:  40%|███▉      | 19604/49172 [00:08<00:10, 2882.53it/s]Packing dataset:  40%|████      | 19893/49172 [00:08<00:10, 2866.76it/s]Packing dataset:  41%|████      | 20190/49172 [00:08<00:10, 2896.11it/s]Packing dataset:  42%|████▏     | 20480/49172 [00:08<00:10, 2862.34it/s]Packing dataset:  42%|████▏     | 20771/49172 [00:08<00:09, 2873.80it/s]Packing dataset:  43%|████▎     | 21075/49172 [00:09<00:09, 2920.42it/s]Packing dataset:  43%|████▎     | 21377/49172 [00:09<00:09, 2946.19it/s]Packing dataset:  44%|████▍     | 21672/49172 [00:09<00:09, 2925.77it/s]Packing dataset:  45%|████▍     | 21965/49172 [00:09<00:09, 2916.71it/s]Packing dataset:  45%|████▌     | 22257/49172 [00:09<00:09, 2875.25it/s]Packing dataset:  46%|████▌     | 22560/49172 [00:09<00:09, 2918.80it/s]Packing dataset:  46%|████▋     | 22853/49172 [00:09<00:09, 2858.14it/s]Packing dataset:  47%|████▋     | 23148/49172 [00:09<00:09, 2884.25it/s]Packing dataset:  48%|████▊     | 23461/49172 [00:09<00:08, 2951.09it/s]Packing dataset:  48%|████▊     | 23761/49172 [00:10<00:08, 2962.87it/s]Packing dataset:  49%|████▉     | 24062/49172 [00:10<00:08, 2976.56it/s]Packing dataset:  50%|████▉     | 24374/49172 [00:10<00:08, 3018.37it/s]Packing dataset:  50%|█████     | 24689/49172 [00:10<00:08, 3056.71it/s]Packing dataset:  51%|█████     | 24995/49172 [00:10<00:08, 2994.53it/s]Packing dataset:  51%|█████▏    | 25295/49172 [00:10<00:07, 2986.53it/s]Packing dataset:  52%|█████▏    | 25595/49172 [00:10<00:07, 2990.20it/s]Packing dataset:  53%|█████▎    | 25895/49172 [00:10<00:07, 2930.89it/s]Packing dataset:  53%|█████▎    | 26213/49172 [00:10<00:07, 3003.19it/s]Packing dataset:  54%|█████▍    | 26514/49172 [00:10<00:07, 2967.96it/s]Packing dataset:  55%|█████▍    | 26812/49172 [00:11<00:07, 2933.28it/s]Packing dataset:  55%|█████▌    | 27123/49172 [00:11<00:07, 2982.46it/s]Packing dataset:  56%|█████▌    | 27422/49172 [00:11<00:07, 2976.98it/s]Packing dataset:  56%|█████▋    | 27720/49172 [00:11<00:07, 2945.64it/s]Packing dataset:  57%|█████▋    | 28015/49172 [00:11<00:07, 2894.72it/s]Packing dataset:  58%|█████▊    | 28305/49172 [00:11<00:07, 2886.25it/s]Packing dataset:  58%|█████▊    | 28594/49172 [00:11<00:07, 2859.99it/s]Packing dataset:  59%|█████▊    | 28886/49172 [00:11<00:07, 2877.45it/s]Packing dataset:  59%|█████▉    | 29177/49172 [00:11<00:06, 2884.35it/s]Packing dataset:  60%|█████▉    | 29468/49172 [00:11<00:06, 2891.77it/s]Packing dataset:  61%|██████    | 29758/49172 [00:12<00:06, 2877.25it/s]Packing dataset:  61%|██████    | 30046/49172 [00:12<00:06, 2836.46it/s]Packing dataset:  62%|██████▏   | 30347/49172 [00:12<00:06, 2885.43it/s]Packing dataset:  62%|██████▏   | 30636/49172 [00:12<00:06, 2882.83it/s]Packing dataset:  63%|██████▎   | 30925/49172 [00:12<00:06, 2882.02it/s]Packing dataset:  63%|██████▎   | 31214/49172 [00:12<00:06, 2864.87it/s]Packing dataset:  64%|██████▍   | 31514/49172 [00:12<00:06, 2902.91it/s]Packing dataset:  65%|██████▍   | 31832/49172 [00:12<00:05, 2984.97it/s]Packing dataset:  65%|██████▌   | 32131/49172 [00:12<00:05, 2911.41it/s]Packing dataset:  66%|██████▌   | 32433/49172 [00:12<00:05, 2942.15it/s]Packing dataset:  67%|██████▋   | 32728/49172 [00:13<00:05, 2929.42it/s]Packing dataset:  67%|██████▋   | 33022/49172 [00:13<00:05, 2917.65it/s]Packing dataset:  68%|██████▊   | 33328/49172 [00:13<00:05, 2959.42it/s]Packing dataset:  68%|██████▊   | 33625/49172 [00:13<00:05, 2956.00it/s]Packing dataset:  69%|██████▉   | 33921/49172 [00:13<00:05, 2939.72it/s]Packing dataset:  70%|██████▉   | 34216/49172 [00:13<00:05, 2925.90it/s]Packing dataset:  70%|███████   | 34513/49172 [00:13<00:04, 2937.26it/s]Packing dataset:  71%|███████   | 34811/49172 [00:13<00:04, 2947.58it/s]Packing dataset:  71%|███████▏  | 35106/49172 [00:13<00:04, 2928.28it/s]Packing dataset:  72%|███████▏  | 35422/49172 [00:13<00:04, 2995.38it/s]Packing dataset:  73%|███████▎  | 35722/49172 [00:14<00:04, 2929.18it/s]Packing dataset:  73%|███████▎  | 36016/49172 [00:14<00:04, 2877.92it/s]Packing dataset:  74%|███████▍  | 36308/49172 [00:14<00:04, 2886.88it/s]Packing dataset:  74%|███████▍  | 36614/49172 [00:14<00:04, 2936.21it/s]Packing dataset:  75%|███████▌  | 36908/49172 [00:14<00:04, 2916.57it/s]Packing dataset:  76%|███████▌  | 37201/49172 [00:14<00:04, 2918.75it/s]Packing dataset:  76%|███████▋  | 37500/49172 [00:14<00:03, 2937.97it/s]Packing dataset:  77%|███████▋  | 37794/49172 [00:14<00:03, 2922.53it/s]Packing dataset:  77%|███████▋  | 38092/49172 [00:14<00:03, 2938.12it/s]Packing dataset:  78%|███████▊  | 38386/49172 [00:15<00:03, 2938.61it/s]Packing dataset:  79%|███████▊  | 38680/49172 [00:15<00:03, 2908.17it/s]Packing dataset:  79%|███████▉  | 38971/49172 [00:15<00:03, 2867.21it/s]Packing dataset:  80%|███████▉  | 39281/49172 [00:15<00:03, 2932.84it/s]Packing dataset:  80%|████████  | 39575/49172 [00:15<00:03, 2923.25it/s]Packing dataset:  81%|████████  | 39868/49172 [00:15<00:03, 2924.72it/s]Packing dataset:  82%|████████▏ | 40161/49172 [00:15<00:03, 2924.97it/s]Packing dataset:  82%|████████▏ | 40457/49172 [00:15<00:02, 2931.65it/s]Packing dataset:  83%|████████▎ | 40762/49172 [00:15<00:02, 2966.09it/s]Packing dataset:  84%|████████▎ | 41061/49172 [00:15<00:02, 2970.71it/s]Packing dataset:  84%|████████▍ | 41364/49172 [00:16<00:02, 2987.89it/s]Packing dataset:  85%|████████▍ | 41663/49172 [00:16<00:02, 2936.14it/s]Packing dataset:  85%|████████▌ | 41957/49172 [00:16<00:02, 2913.43it/s]Packing dataset:  86%|████████▌ | 42249/49172 [00:16<00:02, 2913.77it/s]Packing dataset:  87%|████████▋ | 42541/49172 [00:16<00:02, 2903.40it/s]Packing dataset:  87%|████████▋ | 42832/49172 [00:16<00:02, 2868.66it/s]Packing dataset:  88%|████████▊ | 43119/49172 [00:16<00:02, 2850.58it/s]Packing dataset:  88%|████████▊ | 43416/49172 [00:16<00:01, 2885.03it/s]Packing dataset:  89%|████████▉ | 43705/49172 [00:16<00:01, 2877.35it/s]Packing dataset:  89%|████████▉ | 43997/49172 [00:16<00:01, 2886.12it/s]Packing dataset:  90%|█████████ | 44286/49172 [00:17<00:01, 2875.44it/s]Packing dataset:  91%|█████████ | 44577/49172 [00:17<00:01, 2884.98it/s]Packing dataset:  91%|█████████▏| 44874/49172 [00:17<00:01, 2908.23it/s]Packing dataset:  92%|█████████▏| 45169/49172 [00:17<00:01, 2916.79it/s]Packing dataset:  93%|█████████▎| 45486/49172 [00:17<00:01, 2989.18it/s]Packing dataset:  93%|█████████▎| 45785/49172 [00:17<00:01, 2900.12it/s]Packing dataset:  94%|█████████▎| 46076/49172 [00:17<00:01, 2879.06it/s]Packing dataset:  94%|█████████▍| 46369/49172 [00:17<00:00, 2893.63it/s]Packing dataset:  95%|█████████▍| 46659/49172 [00:17<00:00, 2893.84it/s]Packing dataset:  96%|█████████▌| 46960/49172 [00:17<00:00, 2926.64it/s]Packing dataset:  96%|█████████▌| 47253/49172 [00:18<00:00, 2898.45it/s]Packing dataset:  97%|█████████▋| 47558/49172 [00:18<00:00, 2940.46it/s]Packing dataset:  97%|█████████▋| 47853/49172 [00:18<00:00, 2910.58it/s]Packing dataset:  98%|█████████▊| 48145/49172 [00:18<00:00, 2889.35it/s]Packing dataset:  99%|█████████▊| 48441/49172 [00:18<00:00, 2908.97it/s]Packing dataset:  99%|█████████▉| 48750/49172 [00:18<00:00, 2958.56it/s]Packing dataset: 100%|█████████▉| 49046/49172 [00:18<00:00, 2931.73it/s]Packing dataset: 100%|██████████| 49172/49172 [00:18<00:00, 2628.70it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:626: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:05<00:49,  5.51s/it]1|1|Loss: 1.790740728378296:  10%|█         | 1/10 [00:05<00:49,  5.51s/it]1|1|Loss: 1.790740728378296:  20%|██        | 2/10 [00:09<00:37,  4.64s/it]1|2|Loss: 1.7195229530334473:  20%|██        | 2/10 [00:09<00:37,  4.64s/it]1|2|Loss: 1.7195229530334473:  30%|███       | 3/10 [00:13<00:30,  4.32s/it]1|3|Loss: 1.5262477397918701:  30%|███       | 3/10 [00:13<00:30,  4.32s/it]1|3|Loss: 1.5262477397918701:  40%|████      | 4/10 [00:17<00:25,  4.17s/it]1|4|Loss: 1.897428035736084:  40%|████      | 4/10 [00:17<00:25,  4.17s/it] 1|4|Loss: 1.897428035736084:  50%|█████     | 5/10 [00:21<00:20,  4.08s/it]1|5|Loss: 1.6352999210357666:  50%|█████     | 5/10 [00:21<00:20,  4.08s/it]1|5|Loss: 1.6352999210357666:  60%|██████    | 6/10 [00:25<00:16,  4.04s/it]1|6|Loss: 1.7712498903274536:  60%|██████    | 6/10 [00:25<00:16,  4.04s/it]1|6|Loss: 1.7712498903274536:  70%|███████   | 7/10 [00:29<00:12,  4.00s/it]1|7|Loss: 1.7900664806365967:  70%|███████   | 7/10 [00:29<00:12,  4.00s/it]1|7|Loss: 1.7900664806365967:  80%|████████  | 8/10 [00:33<00:07,  3.96s/it]1|8|Loss: 1.7183294296264648:  80%|████████  | 8/10 [00:33<00:07,  3.96s/it]1|8|Loss: 1.7183294296264648:  90%|█████████ | 9/10 [00:37<00:03,  3.96s/it]1|9|Loss: 1.6448384523391724:  90%|█████████ | 9/10 [00:37<00:03,  3.96s/it]1|9|Loss: 1.6448384523391724: 100%|██████████| 10/10 [00:40<00:00,  3.95s/it]1|10|Loss: 1.664828896522522: 100%|██████████| 10/10 [00:41<00:00,  3.95s/it]Starting checkpoint save...
Checkpoint saved in 0.00 seconds.
1|10|Loss: 1.664828896522522: 100%|██████████| 10/10 [00:41<00:00,  4.10s/it]
iteration:  1 tokens:  6269 time:  5.513083037105389 tokens_per_second_on_single_device:  1137.11
iteration:  2 tokens:  6245 time:  3.992930067004636 tokens_per_second_on_single_device:  1564.01
iteration:  3 tokens:  6258 time:  3.9288293649442494 tokens_per_second_on_single_device:  1592.84
iteration:  4 tokens:  5878 time:  3.911350504960865 tokens_per_second_on_single_device:  1502.81
iteration:  5 tokens:  6035 time:  3.9172733599552885 tokens_per_second_on_single_device:  1540.61
iteration:  6 tokens:  6485 time:  3.962665200000629 tokens_per_second_on_single_device:  1636.52
iteration:  7 tokens:  6039 time:  3.902969994000159 tokens_per_second_on_single_device:  1547.28
iteration:  8 tokens:  5467 time:  3.8545863449107856 tokens_per_second_on_single_device:  1418.31
iteration:  9 tokens:  6381 time:  3.9365639459574595 tokens_per_second_on_single_device:  1620.96
iteration:  10 tokens:  6351 time:  3.927094544051215 tokens_per_second_on_single_device:  1617.23
avg tokens_per_second_on_single_device:  1560.05
