Running LoRAFinetuneRecipeSingleDevice with resolved config:

batch_size: 2
batch_size_val: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_1_8B/lora
  recipe_checkpoint: null
clip_grad_norm: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
  split: train[:95%]
dataset_val:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  split: train[95%:]
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: true
loss:
  _component_: torchtune.modules.loss.LinearCrossEntropyLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_1_8B/lora/logs
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_1_8B/lora
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_1_8B/lora/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 3
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
run_val_every_n_steps: null
save_adapter_weights_only: false
seed: 123
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Meta-Llama-3.1-8B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:436: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Model is initialized with precision torch.bfloat16.
Memory stats after model init:
	XPU peak memory active: 15.06 GiB
	XPU peak memory alloc: 15.06 GiB
	XPU peak memory reserved: 15.18 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_1_8B/lora/logs/log_1756740956.txt
Packing dataset:   0%|          | 0/49172 [00:00<?, ?it/s]Packing dataset:   0%|          | 220/49172 [00:00<00:22, 2195.57it/s]Packing dataset:   1%|          | 539/49172 [00:00<00:17, 2763.98it/s]Packing dataset:   2%|▏         | 846/49172 [00:00<00:16, 2895.78it/s]Packing dataset:   2%|▏         | 1168/49172 [00:00<00:15, 3023.23it/s]Packing dataset:   3%|▎         | 1478/49172 [00:00<00:15, 3050.70it/s]Packing dataset:   4%|▎         | 1784/49172 [00:00<00:16, 2961.12it/s]Packing dataset:   4%|▍         | 2081/49172 [00:00<00:15, 2958.86it/s]Packing dataset:   5%|▍         | 2423/49172 [00:00<00:15, 3102.35it/s]Packing dataset:   6%|▌         | 2761/49172 [00:00<00:14, 3187.39it/s]Packing dataset:   6%|▋         | 3081/49172 [00:01<00:14, 3179.08it/s]Packing dataset:   7%|▋         | 3400/49172 [00:01<00:14, 3168.41it/s]Packing dataset:   8%|▊         | 3718/49172 [00:01<00:14, 3167.19it/s]Packing dataset:   8%|▊         | 4035/49172 [00:01<00:14, 3152.96it/s]Packing dataset:   9%|▉         | 4363/49172 [00:01<00:14, 3189.72it/s]Packing dataset:  10%|▉         | 4683/49172 [00:01<00:14, 3133.94it/s]Packing dataset:  10%|█         | 5005/49172 [00:01<00:13, 3158.37it/s]Packing dataset:  11%|█         | 5322/49172 [00:01<00:14, 3114.73it/s]Packing dataset:  12%|█▏        | 5658/49172 [00:01<00:13, 3182.21it/s]Packing dataset:  12%|█▏        | 5977/49172 [00:01<00:13, 3179.87it/s]Packing dataset:  13%|█▎        | 6309/49172 [00:02<00:13, 3220.52it/s]Packing dataset:  13%|█▎        | 6632/49172 [00:02<00:13, 3148.11it/s]Packing dataset:  14%|█▍        | 6948/49172 [00:02<00:13, 3130.79it/s]Packing dataset:  15%|█▍        | 7262/49172 [00:02<00:13, 3127.63it/s]Packing dataset:  15%|█▌        | 7580/49172 [00:02<00:13, 3142.82it/s]Packing dataset:  16%|█▌        | 7895/49172 [00:02<00:13, 3117.64it/s]Packing dataset:  17%|█▋        | 8207/49172 [00:02<00:13, 3093.92it/s]Packing dataset:  17%|█▋        | 8519/49172 [00:02<00:13, 3101.21it/s]Packing dataset:  18%|█▊        | 8847/49172 [00:02<00:12, 3153.10it/s]Packing dataset:  19%|█▊        | 9163/49172 [00:02<00:12, 3135.97it/s]Packing dataset:  19%|█▉        | 9477/49172 [00:03<00:12, 3130.44it/s]Packing dataset:  20%|█▉        | 9791/49172 [00:03<00:13, 3027.87it/s]Packing dataset:  21%|██        | 10113/49172 [00:03<00:12, 3080.04it/s]Packing dataset:  21%|██        | 10425/49172 [00:03<00:12, 3089.14it/s]Packing dataset:  22%|██▏       | 10754/49172 [00:03<00:12, 3147.85it/s]Packing dataset:  23%|██▎       | 11070/49172 [00:03<00:12, 3130.45it/s]Packing dataset:  23%|██▎       | 11384/49172 [00:03<00:12, 3096.42it/s]Packing dataset:  24%|██▍       | 11695/49172 [00:03<00:12, 3097.71it/s]Packing dataset:  24%|██▍       | 12006/49172 [00:03<00:11, 3098.55it/s]Packing dataset:  25%|██▌       | 12316/49172 [00:03<00:11, 3072.61it/s]Packing dataset:  26%|██▌       | 12629/49172 [00:04<00:11, 3089.28it/s]Packing dataset:  26%|██▋       | 12939/49172 [00:04<00:11, 3061.57it/s]Packing dataset:  27%|██▋       | 13250/49172 [00:04<00:11, 3074.72it/s]Packing dataset:  28%|██▊       | 13558/49172 [00:04<00:11, 3057.87it/s]Packing dataset:  28%|██▊       | 13868/49172 [00:04<00:11, 3067.01it/s]Packing dataset:  29%|██▉       | 14175/49172 [00:04<00:11, 3023.81it/s]Packing dataset:  29%|██▉       | 14502/49172 [00:04<00:11, 3093.63it/s]Packing dataset:  30%|███       | 14812/49172 [00:04<00:11, 3073.19it/s]Packing dataset:  31%|███       | 15122/49172 [00:04<00:11, 3080.69it/s]Packing dataset:  31%|███▏      | 15437/49172 [00:04<00:10, 3100.95it/s]Packing dataset:  32%|███▏      | 15751/49172 [00:05<00:10, 3111.73it/s]Packing dataset:  33%|███▎      | 16080/49172 [00:05<00:10, 3160.81it/s]Packing dataset:  33%|███▎      | 16397/49172 [00:05<00:10, 3098.23it/s]Packing dataset:  34%|███▍      | 16708/49172 [00:05<00:10, 3098.49it/s]Packing dataset:  35%|███▍      | 17019/49172 [00:05<00:10, 3068.07it/s]Packing dataset:  35%|███▌      | 17326/49172 [00:05<00:10, 3063.71it/s]Packing dataset:  36%|███▌      | 17633/49172 [00:05<00:10, 3055.54it/s]Packing dataset:  36%|███▋      | 17939/49172 [00:05<00:10, 3018.83it/s]Packing dataset:  37%|███▋      | 18251/49172 [00:05<00:10, 3046.82it/s]Packing dataset:  38%|███▊      | 18556/49172 [00:06<00:10, 3022.24it/s]Packing dataset:  38%|███▊      | 18875/49172 [00:06<00:09, 3066.24it/s]Packing dataset:  39%|███▉      | 19194/49172 [00:06<00:09, 3102.55it/s]Packing dataset:  40%|███▉      | 19505/49172 [00:06<00:09, 3039.51it/s]Packing dataset:  40%|████      | 19810/49172 [00:06<00:09, 3000.41it/s]Packing dataset:  41%|████      | 20111/49172 [00:06<00:09, 2993.33it/s]Packing dataset:  42%|████▏     | 20411/49172 [00:06<00:09, 2977.67it/s]Packing dataset:  42%|████▏     | 20712/49172 [00:06<00:09, 2985.82it/s]Packing dataset:  43%|████▎     | 21033/49172 [00:06<00:09, 3051.11it/s]Packing dataset:  43%|████▎     | 21343/49172 [00:06<00:09, 3062.04it/s]Packing dataset:  44%|████▍     | 21653/49172 [00:07<00:08, 3072.10it/s]Packing dataset:  45%|████▍     | 21961/49172 [00:07<00:08, 3054.71it/s]Packing dataset:  45%|████▌     | 22267/49172 [00:07<00:08, 2989.66it/s]Packing dataset:  46%|████▌     | 22588/49172 [00:07<00:08, 3052.57it/s]Packing dataset:  47%|████▋     | 22894/49172 [00:07<00:08, 3024.14it/s]Packing dataset:  47%|████▋     | 23202/49172 [00:07<00:08, 3039.63it/s]Packing dataset:  48%|████▊     | 23530/49172 [00:07<00:08, 3110.71it/s]Packing dataset:  48%|████▊     | 23842/49172 [00:07<00:08, 3069.20it/s]Packing dataset:  49%|████▉     | 24151/49172 [00:07<00:08, 3074.38it/s]Packing dataset:  50%|████▉     | 24486/49172 [00:07<00:07, 3151.66it/s]Packing dataset:  50%|█████     | 24802/49172 [00:08<00:07, 3132.58it/s]Packing dataset:  51%|█████     | 25116/49172 [00:08<00:07, 3098.23it/s]Packing dataset:  52%|█████▏    | 25440/49172 [00:08<00:07, 3139.43it/s]Packing dataset:  52%|█████▏    | 25755/49172 [00:08<00:07, 3077.76it/s]Packing dataset:  53%|█████▎    | 26064/49172 [00:08<00:07, 3076.71it/s]Packing dataset:  54%|█████▎    | 26372/49172 [00:08<00:07, 3071.31it/s]Packing dataset:  54%|█████▍    | 26680/49172 [00:08<00:07, 3071.97it/s]Packing dataset:  55%|█████▍    | 27013/49172 [00:08<00:07, 3148.02it/s]Packing dataset:  56%|█████▌    | 27329/49172 [00:08<00:06, 3149.87it/s]Packing dataset:  56%|█████▌    | 27645/49172 [00:08<00:06, 3105.45it/s]Packing dataset:  57%|█████▋    | 27956/49172 [00:09<00:06, 3083.52it/s]Packing dataset:  57%|█████▋    | 28265/49172 [00:09<00:06, 3021.85it/s]Packing dataset:  58%|█████▊    | 28568/49172 [00:09<00:06, 2997.61it/s]Packing dataset:  59%|█████▊    | 28868/49172 [00:09<00:06, 2984.93it/s]Packing dataset:  59%|█████▉    | 29167/49172 [00:09<00:06, 2974.56it/s]Packing dataset:  60%|█████▉    | 29470/49172 [00:09<00:06, 2986.89it/s]Packing dataset:  61%|██████    | 29777/49172 [00:09<00:06, 3010.20it/s]Packing dataset:  61%|██████    | 30079/49172 [00:09<00:06, 2980.11it/s]Packing dataset:  62%|██████▏   | 30387/49172 [00:09<00:06, 3008.95it/s]Packing dataset:  62%|██████▏   | 30689/49172 [00:10<00:08, 2123.52it/s]Packing dataset:  63%|██████▎   | 30981/49172 [00:10<00:07, 2306.01it/s]Packing dataset:  64%|██████▎   | 31278/49172 [00:10<00:07, 2469.95it/s]Packing dataset:  64%|██████▍   | 31601/49172 [00:10<00:06, 2667.96it/s]Packing dataset:  65%|██████▍   | 31917/49172 [00:10<00:06, 2798.85it/s]Packing dataset:  66%|██████▌   | 32213/49172 [00:10<00:05, 2834.21it/s]Packing dataset:  66%|██████▌   | 32529/49172 [00:10<00:05, 2922.79it/s]Packing dataset:  67%|██████▋   | 32832/49172 [00:10<00:05, 2951.68it/s]Packing dataset:  67%|██████▋   | 33139/49172 [00:10<00:05, 2984.30it/s]Packing dataset:  68%|██████▊   | 33463/49172 [00:11<00:05, 3058.50it/s]Packing dataset:  69%|██████▊   | 33772/49172 [00:11<00:05, 3040.08it/s]Packing dataset:  69%|██████▉   | 34079/49172 [00:11<00:04, 3043.87it/s]Packing dataset:  70%|██████▉   | 34385/49172 [00:11<00:04, 3037.00it/s]Packing dataset:  71%|███████   | 34697/49172 [00:11<00:04, 3059.97it/s]Packing dataset:  71%|███████   | 35004/49172 [00:11<00:04, 3055.01it/s]Packing dataset:  72%|███████▏  | 35322/49172 [00:11<00:04, 3089.14it/s]Packing dataset:  72%|███████▏  | 35632/49172 [00:11<00:04, 3057.63it/s]Packing dataset:  73%|███████▎  | 35939/49172 [00:11<00:04, 3023.65it/s]Packing dataset:  74%|███████▎  | 36242/49172 [00:11<00:04, 2997.80it/s]Packing dataset:  74%|███████▍  | 36567/49172 [00:12<00:04, 3070.97it/s]Packing dataset:  75%|███████▍  | 36875/49172 [00:12<00:04, 3037.44it/s]Packing dataset:  76%|███████▌  | 37192/49172 [00:12<00:03, 3073.45it/s]Packing dataset:  76%|███████▋  | 37500/49172 [00:12<00:03, 3074.44it/s]Packing dataset:  77%|███████▋  | 37808/49172 [00:12<00:03, 3056.75it/s]Packing dataset:  78%|███████▊  | 38118/49172 [00:12<00:03, 3066.66it/s]Packing dataset:  78%|███████▊  | 38425/49172 [00:12<00:03, 3061.98it/s]Packing dataset:  79%|███████▉  | 38732/49172 [00:12<00:03, 3031.75it/s]Packing dataset:  79%|███████▉  | 39036/49172 [00:12<00:03, 2978.49it/s]Packing dataset:  80%|████████  | 39355/49172 [00:12<00:03, 3038.76it/s]Packing dataset:  81%|████████  | 39660/49172 [00:13<00:03, 3020.10it/s]Packing dataset:  81%|████████▏ | 39968/49172 [00:13<00:03, 3037.75it/s]Packing dataset:  82%|████████▏ | 40272/49172 [00:13<00:02, 3023.89it/s]Packing dataset:  83%|████████▎ | 40588/49172 [00:13<00:02, 3063.73it/s]Packing dataset:  83%|████████▎ | 40898/49172 [00:13<00:02, 3072.77it/s]Packing dataset:  84%|████████▍ | 41208/49172 [00:13<00:02, 3080.18it/s]Packing dataset:  84%|████████▍ | 41517/49172 [00:13<00:02, 3043.96it/s]Packing dataset:  85%|████████▌ | 41822/49172 [00:13<00:02, 3023.74it/s]Packing dataset:  86%|████████▌ | 42125/49172 [00:13<00:02, 2971.10it/s]Packing dataset:  86%|████████▋ | 42435/49172 [00:13<00:02, 3006.25it/s]Packing dataset:  87%|████████▋ | 42736/49172 [00:14<00:02, 2932.62it/s]Packing dataset:  88%|████████▊ | 43030/49172 [00:14<00:02, 2917.68it/s]Packing dataset:  88%|████████▊ | 43323/49172 [00:14<00:02, 2903.35it/s]Packing dataset:  89%|████████▊ | 43629/49172 [00:14<00:01, 2929.29it/s]Packing dataset:  89%|████████▉ | 43930/49172 [00:14<00:01, 2949.46it/s]Packing dataset:  90%|████████▉ | 44226/49172 [00:14<00:01, 2947.60it/s]Packing dataset:  91%|█████████ | 44543/49172 [00:14<00:01, 3011.60it/s]Packing dataset:  91%|█████████ | 44845/49172 [00:14<00:01, 3011.32it/s]Packing dataset:  92%|█████████▏| 45155/49172 [00:14<00:01, 3035.35it/s]Packing dataset:  92%|█████████▏| 45476/49172 [00:14<00:01, 3085.67it/s]Packing dataset:  93%|█████████▎| 45785/49172 [00:15<00:01, 3014.40it/s]Packing dataset:  94%|█████████▎| 46087/49172 [00:15<00:01, 2986.11it/s]Packing dataset:  94%|█████████▍| 46399/49172 [00:15<00:00, 3023.50it/s]Packing dataset:  95%|█████████▍| 46702/49172 [00:15<00:00, 3023.39it/s]Packing dataset:  96%|█████████▌| 47008/49172 [00:15<00:00, 3032.34it/s]Packing dataset:  96%|█████████▌| 47314/49172 [00:15<00:00, 3037.33it/s]Packing dataset:  97%|█████████▋| 47618/49172 [00:15<00:00, 3010.84it/s]Packing dataset:  97%|█████████▋| 47920/49172 [00:15<00:00, 3001.77it/s]Packing dataset:  98%|█████████▊| 48221/49172 [00:15<00:00, 2987.44it/s]Packing dataset:  99%|█████████▊| 48523/49172 [00:16<00:00, 2996.92it/s]Packing dataset:  99%|█████████▉| 48828/49172 [00:16<00:00, 3009.28it/s]Packing dataset: 100%|█████████▉| 49131/49172 [00:16<00:00, 3013.74it/s]Packing dataset: 100%|██████████| 49172/49172 [00:16<00:00, 3030.48it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/recipes/lora_finetune_single_device.py:626: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:05<00:51,  5.74s/it]1|1|Loss: 1.7903565168380737:  10%|█         | 1/10 [00:05<00:51,  5.74s/it]1|1|Loss: 1.7903565168380737:  20%|██        | 2/10 [00:09<00:37,  4.74s/it]1|2|Loss: 1.7195602655410767:  20%|██        | 2/10 [00:09<00:37,  4.74s/it]1|2|Loss: 1.7195602655410767:  30%|███       | 3/10 [00:13<00:30,  4.39s/it]1|3|Loss: 1.5266660451889038:  30%|███       | 3/10 [00:13<00:30,  4.39s/it]1|3|Loss: 1.5266660451889038:  40%|████      | 4/10 [00:17<00:25,  4.21s/it]1|4|Loss: 1.8974155187606812:  40%|████      | 4/10 [00:17<00:25,  4.21s/it]1|4|Loss: 1.8974155187606812:  50%|█████     | 5/10 [00:21<00:20,  4.11s/it]1|5|Loss: 1.6358444690704346:  50%|█████     | 5/10 [00:21<00:20,  4.11s/it]1|5|Loss: 1.6358444690704346:  60%|██████    | 6/10 [00:25<00:16,  4.06s/it]1|6|Loss: 1.772036075592041:  60%|██████    | 6/10 [00:25<00:16,  4.06s/it] 1|6|Loss: 1.772036075592041:  70%|███████   | 7/10 [00:29<00:12,  4.02s/it]1|7|Loss: 1.790213704109192:  70%|███████   | 7/10 [00:29<00:12,  4.02s/it]1|7|Loss: 1.790213704109192:  80%|████████  | 8/10 [00:33<00:07,  3.97s/it]1|8|Loss: 1.7205606698989868:  80%|████████  | 8/10 [00:33<00:07,  3.97s/it]1|8|Loss: 1.7205606698989868:  90%|█████████ | 9/10 [00:37<00:03,  3.97s/it]1|9|Loss: 1.644374132156372:  90%|█████████ | 9/10 [00:37<00:03,  3.97s/it] 1|9|Loss: 1.644374132156372: 100%|██████████| 10/10 [00:41<00:00,  3.97s/it]1|10|Loss: 1.663140058517456: 100%|██████████| 10/10 [00:41<00:00,  3.97s/it]Starting checkpoint save...
Checkpoint saved in 0.00 seconds.
1|10|Loss: 1.663140058517456: 100%|██████████| 10/10 [00:41<00:00,  4.13s/it]
iteration:  1 tokens:  6269 time:  5.742691751103848 tokens_per_second_on_single_device:  1091.65
iteration:  2 tokens:  6245 time:  4.004937290679663 tokens_per_second_on_single_device:  1559.33
iteration:  3 tokens:  6258 time:  3.9477941216900945 tokens_per_second_on_single_device:  1585.19
iteration:  4 tokens:  5878 time:  3.9147617258131504 tokens_per_second_on_single_device:  1501.5
iteration:  5 tokens:  6035 time:  3.9192161560058594 tokens_per_second_on_single_device:  1539.85
iteration:  6 tokens:  6485 time:  3.95855645975098 tokens_per_second_on_single_device:  1638.22
iteration:  7 tokens:  6039 time:  3.9141080314293504 tokens_per_second_on_single_device:  1542.88
iteration:  8 tokens:  5467 time:  3.862575550097972 tokens_per_second_on_single_device:  1415.38
iteration:  9 tokens:  6381 time:  3.94328428292647 tokens_per_second_on_single_device:  1618.19
iteration:  10 tokens:  6351 time:  3.946609554812312 tokens_per_second_on_single_device:  1609.23
avg tokens_per_second_on_single_device:  1556.79
[W901 15:36:58.378460986 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
