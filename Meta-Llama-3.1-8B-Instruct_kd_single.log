Running KDRecipeSingleDevice with resolved config:

batch_size: 4
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
  checkpoint_files:
  - model.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
kd_loss:
  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss
kd_ratio: 0.5
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 128
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 64
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 5
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
teacher_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
teacher_model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:418: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Student model is initialized with precision torch.bfloat16.
Memory stats initializing student model:
Memory stats after student model init:
	XPU peak memory active: 2.46 GiB
	XPU peak memory alloc: 2.46 GiB
	XPU peak memory reserved: 2.46 GiB
Teacher model is initialized with precision torch.bfloat16.
Memory stats after teacher model init:
	XPU peak memory active: 17.48 GiB
	XPU peak memory alloc: 17.48 GiB
	XPU peak memory reserved: 17.59 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
/home/jenkins/xiangdong/torchtune/torchtune/config/_instantiate.py:24: FutureWarning: CEWithChunkedOutputLoss is deprecated and will be removed in future versions. Please use `torchtune.modules.loss.LinearCrossEntropyLoss` instead.
  return _component_(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:282: FutureWarning: set_num_output_chunks is deprecated and will be removed in future versions. Please use LinearCrossEntropyLoss instead
  self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs/log_1757305391.txt
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 218/51760 [00:00<00:23, 2175.61it/s]Packing dataset:   1%|          | 537/51760 [00:00<00:18, 2771.65it/s]Packing dataset:   2%|▏         | 844/51760 [00:00<00:17, 2901.97it/s]Packing dataset:   2%|▏         | 1171/51760 [00:00<00:16, 3042.64it/s]Packing dataset:   3%|▎         | 1487/51760 [00:00<00:16, 3081.88it/s]Packing dataset:   3%|▎         | 1796/51760 [00:00<00:16, 2993.84it/s]Packing dataset:   4%|▍         | 2098/51760 [00:00<00:16, 3002.00it/s]Packing dataset:   5%|▍         | 2444/51760 [00:00<00:15, 3144.13it/s]Packing dataset:   5%|▌         | 2794/51760 [00:00<00:15, 3252.66it/s]Packing dataset:   6%|▌         | 3120/51760 [00:01<00:15, 3242.40it/s]Packing dataset:   7%|▋         | 3445/51760 [00:01<00:15, 3206.48it/s]Packing dataset:   7%|▋         | 3766/51760 [00:01<00:15, 3192.45it/s]Packing dataset:   8%|▊         | 4086/51760 [00:01<00:15, 3157.58it/s]Packing dataset:   9%|▊         | 4407/51760 [00:01<00:14, 3172.03it/s]Packing dataset:   9%|▉         | 4725/51760 [00:01<00:14, 3150.22it/s]Packing dataset:  10%|▉         | 5041/51760 [00:01<00:14, 3148.91it/s]Packing dataset:  10%|█         | 5356/51760 [00:01<00:14, 3127.81it/s]Packing dataset:  11%|█         | 5689/51760 [00:01<00:14, 3186.33it/s]Packing dataset:  12%|█▏        | 6008/51760 [00:01<00:14, 3171.51it/s]Packing dataset:  12%|█▏        | 6333/51760 [00:02<00:14, 3191.51it/s]Packing dataset:  13%|█▎        | 6653/51760 [00:02<00:14, 3122.90it/s]Packing dataset:  13%|█▎        | 6968/51760 [00:02<00:14, 3129.45it/s]Packing dataset:  14%|█▍        | 7282/51760 [00:02<00:14, 3125.28it/s]Packing dataset:  15%|█▍        | 7607/51760 [00:02<00:13, 3161.62it/s]Packing dataset:  15%|█▌        | 7924/51760 [00:02<00:13, 3151.44it/s]Packing dataset:  16%|█▌        | 8240/51760 [00:02<00:14, 3067.54it/s]Packing dataset:  17%|█▋        | 8568/51760 [00:02<00:13, 3125.62it/s]Packing dataset:  17%|█▋        | 8892/51760 [00:02<00:13, 3153.67it/s]Packing dataset:  18%|█▊        | 9208/51760 [00:02<00:13, 3143.50it/s]Packing dataset:  18%|█▊        | 9523/51760 [00:03<00:13, 3111.06it/s]Packing dataset:  19%|█▉        | 9835/51760 [00:03<00:13, 3053.78it/s]Packing dataset:  20%|█▉        | 10151/51760 [00:03<00:13, 3083.28it/s]Packing dataset:  20%|██        | 10469/51760 [00:03<00:13, 3108.96it/s]Packing dataset:  21%|██        | 10796/51760 [00:03<00:12, 3153.20it/s]Packing dataset:  21%|██▏       | 11112/51760 [00:03<00:13, 3122.65it/s]Packing dataset:  22%|██▏       | 11425/51760 [00:03<00:13, 3095.70it/s]Packing dataset:  23%|██▎       | 11735/51760 [00:03<00:12, 3087.64it/s]Packing dataset:  23%|██▎       | 12044/51760 [00:03<00:12, 3071.44it/s]Packing dataset:  24%|██▍       | 12352/51760 [00:03<00:12, 3072.87it/s]Packing dataset:  24%|██▍       | 12662/51760 [00:04<00:12, 3078.89it/s]Packing dataset:  25%|██▌       | 12970/51760 [00:04<00:12, 3059.42it/s]Packing dataset:  26%|██▌       | 13284/51760 [00:04<00:12, 3080.30it/s]Packing dataset:  26%|██▋       | 13593/51760 [00:04<00:12, 3061.51it/s]Packing dataset:  27%|██▋       | 13900/51760 [00:04<00:12, 3063.03it/s]Packing dataset:  27%|██▋       | 14207/51760 [00:04<00:12, 3033.35it/s]Packing dataset:  28%|██▊       | 14530/51760 [00:04<00:12, 3090.78it/s]Packing dataset:  29%|██▊       | 14840/51760 [00:04<00:12, 3069.02it/s]Packing dataset:  29%|██▉       | 15155/51760 [00:04<00:11, 3092.73it/s]Packing dataset:  30%|██▉       | 15467/51760 [00:04<00:11, 3099.15it/s]Packing dataset:  30%|███       | 15784/51760 [00:05<00:11, 3119.52it/s]Packing dataset:  31%|███       | 16105/51760 [00:05<00:11, 3145.41it/s]Packing dataset:  32%|███▏      | 16420/51760 [00:05<00:11, 3100.38it/s]Packing dataset:  32%|███▏      | 16731/51760 [00:05<00:11, 3090.75it/s]Packing dataset:  33%|███▎      | 17041/51760 [00:05<00:11, 3072.11it/s]Packing dataset:  34%|███▎      | 17349/51760 [00:05<00:11, 3052.68it/s]Packing dataset:  34%|███▍      | 17655/51760 [00:05<00:11, 3034.12it/s]Packing dataset:  35%|███▍      | 17959/51760 [00:05<00:11, 3005.78it/s]Packing dataset:  35%|███▌      | 18272/51760 [00:05<00:11, 3039.74it/s]Packing dataset:  36%|███▌      | 18577/51760 [00:06<00:10, 3029.51it/s]Packing dataset:  37%|███▋      | 18896/51760 [00:06<00:10, 3077.07it/s]Packing dataset:  37%|███▋      | 19209/51760 [00:06<00:10, 3089.21it/s]Packing dataset:  38%|███▊      | 19518/51760 [00:06<00:10, 3016.48it/s]Packing dataset:  38%|███▊      | 19821/51760 [00:06<00:10, 2978.06it/s]Packing dataset:  39%|███▉      | 20120/51760 [00:06<00:10, 2973.14it/s]Packing dataset:  39%|███▉      | 20419/51760 [00:06<00:10, 2978.03it/s]Packing dataset:  40%|████      | 20727/51760 [00:06<00:10, 3004.04it/s]Packing dataset:  41%|████      | 21052/51760 [00:06<00:09, 3074.54it/s]Packing dataset:  41%|████▏     | 21360/51760 [00:06<00:09, 3054.03it/s]Packing dataset:  42%|████▏     | 21667/51760 [00:07<00:09, 3055.69it/s]Packing dataset:  42%|████▏     | 21973/51760 [00:07<00:09, 3037.04it/s]Packing dataset:  43%|████▎     | 22277/51760 [00:07<00:09, 3000.03it/s]Packing dataset:  44%|████▎     | 22589/51760 [00:07<00:09, 3035.31it/s]Packing dataset:  44%|████▍     | 22893/51760 [00:07<00:09, 2991.57it/s]Packing dataset:  45%|████▍     | 23193/51760 [00:07<00:09, 2970.27it/s]Packing dataset:  45%|████▌     | 23522/51760 [00:07<00:09, 3040.56it/s]Packing dataset:  46%|████▌     | 23827/51760 [00:07<00:09, 3041.02it/s]Packing dataset:  47%|████▋     | 24132/51760 [00:07<00:09, 3022.28it/s]Packing dataset:  47%|████▋     | 24468/51760 [00:07<00:08, 3121.44it/s]Packing dataset:  48%|████▊     | 24781/51760 [00:08<00:08, 3117.87it/s]Packing dataset:  48%|████▊     | 25093/51760 [00:08<00:08, 3051.82it/s]Packing dataset:  49%|████▉     | 25413/51760 [00:08<00:08, 3094.25it/s]Packing dataset:  50%|████▉     | 25723/51760 [00:08<00:08, 3069.05it/s]Packing dataset:  50%|█████     | 26031/51760 [00:08<00:08, 3045.17it/s]Packing dataset:  51%|█████     | 26336/51760 [00:08<00:08, 3039.99it/s]Packing dataset:  51%|█████▏    | 26649/51760 [00:08<00:08, 3065.25it/s]Packing dataset:  52%|█████▏    | 26967/51760 [00:08<00:08, 3097.23it/s]Packing dataset:  53%|█████▎    | 27277/51760 [00:08<00:07, 3093.74it/s]Packing dataset:  53%|█████▎    | 27587/51760 [00:08<00:07, 3063.72it/s]Packing dataset:  54%|█████▍    | 27894/51760 [00:09<00:07, 3020.22it/s]Packing dataset:  54%|█████▍    | 28197/51760 [00:09<00:07, 2969.50it/s]Packing dataset:  55%|█████▌    | 28495/51760 [00:09<00:07, 2942.64it/s]Packing dataset:  56%|█████▌    | 28807/51760 [00:09<00:07, 2992.68it/s]Packing dataset:  56%|█████▌    | 29107/51760 [00:09<00:07, 2974.95it/s]Packing dataset:  57%|█████▋    | 29409/51760 [00:09<00:07, 2987.59it/s]Packing dataset:  57%|█████▋    | 29708/51760 [00:09<00:07, 2983.42it/s]Packing dataset:  58%|█████▊    | 30008/51760 [00:09<00:07, 2985.84it/s]Packing dataset:  59%|█████▊    | 30328/51760 [00:09<00:07, 3045.25it/s]Packing dataset:  59%|█████▉    | 30633/51760 [00:09<00:06, 3025.66it/s]Packing dataset:  60%|█████▉    | 30936/51760 [00:10<00:06, 3022.18it/s]Packing dataset:  60%|██████    | 31239/51760 [00:10<00:09, 2124.01it/s]Packing dataset:  61%|██████    | 31556/51760 [00:10<00:08, 2364.07it/s]Packing dataset:  62%|██████▏   | 31877/51760 [00:10<00:07, 2572.37it/s]Packing dataset:  62%|██████▏   | 32166/51760 [00:10<00:07, 2654.97it/s]Packing dataset:  63%|██████▎   | 32479/51760 [00:10<00:06, 2782.42it/s]Packing dataset:  63%|██████▎   | 32780/51760 [00:10<00:06, 2845.00it/s]Packing dataset:  64%|██████▍   | 33092/51760 [00:10<00:06, 2922.99it/s]Packing dataset:  65%|██████▍   | 33410/51760 [00:11<00:06, 2996.78it/s]Packing dataset:  65%|██████▌   | 33716/51760 [00:11<00:06, 2978.94it/s]Packing dataset:  66%|██████▌   | 34034/51760 [00:11<00:05, 3035.68it/s]Packing dataset:  66%|██████▋   | 34341/51760 [00:11<00:05, 3014.00it/s]Packing dataset:  67%|██████▋   | 34648/51760 [00:11<00:05, 3029.95it/s]Packing dataset:  68%|██████▊   | 34953/51760 [00:11<00:05, 3033.23it/s]Packing dataset:  68%|██████▊   | 35266/51760 [00:11<00:05, 3059.67it/s]Packing dataset:  69%|██████▊   | 35583/51760 [00:11<00:05, 3091.44it/s]Packing dataset:  69%|██████▉   | 35893/51760 [00:11<00:05, 3016.60it/s]Packing dataset:  70%|██████▉   | 36196/51760 [00:11<00:05, 2999.17it/s]Packing dataset:  71%|███████   | 36514/51760 [00:12<00:05, 3048.48it/s]Packing dataset:  71%|███████   | 36828/51760 [00:12<00:04, 3074.40it/s]Packing dataset:  72%|███████▏  | 37136/51760 [00:12<00:04, 3069.15it/s]Packing dataset:  72%|███████▏  | 37444/51760 [00:12<00:04, 3064.69it/s]Packing dataset:  73%|███████▎  | 37752/51760 [00:12<00:04, 3067.88it/s]Packing dataset:  74%|███████▎  | 38067/51760 [00:12<00:04, 3090.34it/s]Packing dataset:  74%|███████▍  | 38377/51760 [00:12<00:04, 3073.50it/s]Packing dataset:  75%|███████▍  | 38685/51760 [00:12<00:04, 3052.56it/s]Packing dataset:  75%|███████▌  | 38991/51760 [00:12<00:04, 2997.84it/s]Packing dataset:  76%|███████▌  | 39313/51760 [00:12<00:04, 3062.37it/s]Packing dataset:  77%|███████▋  | 39620/51760 [00:13<00:03, 3060.31it/s]Packing dataset:  77%|███████▋  | 39930/51760 [00:13<00:03, 3069.93it/s]Packing dataset:  78%|███████▊  | 40238/51760 [00:13<00:03, 3053.45it/s]Packing dataset:  78%|███████▊  | 40558/51760 [00:13<00:03, 3094.11it/s]Packing dataset:  79%|███████▉  | 40875/51760 [00:13<00:03, 3116.20it/s]Packing dataset:  80%|███████▉  | 41187/51760 [00:13<00:03, 3101.08it/s]Packing dataset:  80%|████████  | 41498/51760 [00:13<00:03, 3087.08it/s]Packing dataset:  81%|████████  | 41807/51760 [00:13<00:03, 3075.24it/s]Packing dataset:  81%|████████▏ | 42115/51760 [00:13<00:03, 3024.28it/s]Packing dataset:  82%|████████▏ | 42434/51760 [00:13<00:03, 3070.56it/s]Packing dataset:  83%|████████▎ | 42742/51760 [00:14<00:02, 3020.14it/s]Packing dataset:  83%|████████▎ | 43045/51760 [00:14<00:02, 2979.16it/s]Packing dataset:  84%|████████▎ | 43344/51760 [00:14<00:02, 2971.20it/s]Packing dataset:  84%|████████▍ | 43655/51760 [00:14<00:02, 3011.72it/s]Packing dataset:  85%|████████▍ | 43957/51760 [00:14<00:02, 2982.10it/s]Packing dataset:  86%|████████▌ | 44261/51760 [00:14<00:02, 2998.71it/s]Packing dataset:  86%|████████▌ | 44567/51760 [00:14<00:02, 3015.73it/s]Packing dataset:  87%|████████▋ | 44884/51760 [00:14<00:02, 3061.25it/s]Packing dataset:  87%|████████▋ | 45191/51760 [00:14<00:02, 3034.13it/s]Packing dataset:  88%|████████▊ | 45523/51760 [00:14<00:02, 3117.23it/s]Packing dataset:  89%|████████▊ | 45835/51760 [00:15<00:01, 3038.43it/s]Packing dataset:  89%|████████▉ | 46140/51760 [00:15<00:01, 3008.13it/s]Packing dataset:  90%|████████▉ | 46460/51760 [00:15<00:01, 3062.96it/s]Packing dataset:  90%|█████████ | 46767/51760 [00:15<00:01, 3043.26it/s]Packing dataset:  91%|█████████ | 47072/51760 [00:15<00:01, 3011.04it/s]Packing dataset:  92%|█████████▏| 47375/51760 [00:15<00:01, 3012.15it/s]Packing dataset:  92%|█████████▏| 47677/51760 [00:15<00:01, 2968.97it/s]Packing dataset:  93%|█████████▎| 47976/51760 [00:15<00:01, 2971.10it/s]Packing dataset:  93%|█████████▎| 48279/51760 [00:15<00:01, 2984.00it/s]Packing dataset:  94%|█████████▍| 48585/51760 [00:16<00:01, 3005.64it/s]Packing dataset:  94%|█████████▍| 48887/51760 [00:16<00:00, 3008.08it/s]Packing dataset:  95%|█████████▌| 49196/51760 [00:16<00:00, 3030.08it/s]Packing dataset:  96%|█████████▌| 49500/51760 [00:16<00:00, 3029.09it/s]Packing dataset:  96%|█████████▌| 49804/51760 [00:16<00:00, 3030.96it/s]Packing dataset:  97%|█████████▋| 50108/51760 [00:16<00:00, 3029.14it/s]Packing dataset:  97%|█████████▋| 50411/51760 [00:16<00:00, 2993.33it/s]Packing dataset:  98%|█████████▊| 50734/51760 [00:16<00:00, 3061.36it/s]Packing dataset:  99%|█████████▊| 51041/51760 [00:16<00:00, 3056.62it/s]Packing dataset:  99%|█████████▉| 51348/51760 [00:16<00:00, 3057.80it/s]Packing dataset: 100%|█████████▉| 51658/51760 [00:17<00:00, 3068.48it/s]Packing dataset: 100%|██████████| 51760/51760 [00:17<00:00, 3034.88it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/torchtune/modules/transformer.py:684: FutureWarning: chunked_output is deprecated and will be removed in future versions. Please use self.skip_output_layer=True and use a linear loss instead
  output = self.chunked_output(h)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:647: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:05<00:53,  5.95s/it]1|1|Loss: 1.8852365016937256:  10%|█         | 1/10 [00:05<00:53,  5.95s/it]1|1|Loss: 1.8852365016937256:  20%|██        | 2/10 [00:10<00:41,  5.15s/it]1|2|Loss: 2.012436628341675:  20%|██        | 2/10 [00:10<00:41,  5.15s/it] 1|2|Loss: 2.012436628341675:  30%|███       | 3/10 [00:15<00:34,  4.89s/it]1|3|Loss: 1.7873938083648682:  30%|███       | 3/10 [00:15<00:34,  4.89s/it]1|3|Loss: 1.7873938083648682:  40%|████      | 4/10 [00:19<00:28,  4.79s/it]1|4|Loss: 1.872104525566101:  40%|████      | 4/10 [00:19<00:28,  4.79s/it] 1|4|Loss: 1.872104525566101:  50%|█████     | 5/10 [00:24<00:23,  4.73s/it]1|5|Loss: 1.9336891174316406:  50%|█████     | 5/10 [00:24<00:23,  4.73s/it]1|5|Loss: 1.9336891174316406:  60%|██████    | 6/10 [00:28<00:18,  4.69s/it]1|6|Loss: 2.001716136932373:  60%|██████    | 6/10 [00:28<00:18,  4.69s/it] 1|6|Loss: 2.001716136932373:  70%|███████   | 7/10 [00:33<00:13,  4.66s/it]1|7|Loss: 1.8977017402648926:  70%|███████   | 7/10 [00:33<00:13,  4.66s/it]1|7|Loss: 1.8977017402648926:  80%|████████  | 8/10 [00:38<00:09,  4.65s/it]1|8|Loss: 1.790173053741455:  80%|████████  | 8/10 [00:38<00:09,  4.65s/it] 1|8|Loss: 1.790173053741455:  90%|█████████ | 9/10 [00:42<00:04,  4.63s/it]1|9|Loss: 1.6855238676071167:  90%|█████████ | 9/10 [00:42<00:04,  4.63s/it]1|9|Loss: 1.6855238676071167: 100%|██████████| 10/10 [00:47<00:00,  4.61s/it]1|10|Loss: 1.7577898502349854: 100%|██████████| 10/10 [00:47<00:00,  4.61s/it]1|10|Loss: 1.7577898502349854: 100%|██████████| 10/10 [00:47<00:00,  4.74s/it]
iteration:  3 tokens:  11612 time:  4.576073482632637 tokens_per_second_on_single_device:  2537.55
iteration:  4 tokens:  12699 time:  4.61916059628129 tokens_per_second_on_single_device:  2749.2
iteration:  5 tokens:  13246 time:  4.628589938394725 tokens_per_second_on_single_device:  2861.78
iteration:  6 tokens:  12370 time:  4.595706761814654 tokens_per_second_on_single_device:  2691.64
iteration:  7 tokens:  12510 time:  4.594483804889023 tokens_per_second_on_single_device:  2722.83
iteration:  8 tokens:  12921 time:  4.607628958299756 tokens_per_second_on_single_device:  2804.26
iteration:  9 tokens:  12637 time:  4.60650672391057 tokens_per_second_on_single_device:  2743.29
iteration:  10 tokens:  12364 time:  4.562065267004073 tokens_per_second_on_single_device:  2710.18
avg tokens_per_second_on_single_device:  2727.87
[W908 04:24:20.918500811 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
