Running KDRecipeSingleDevice with resolved config:

batch_size: 4
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
  checkpoint_files:
  - model.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
kd_loss:
  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss
kd_ratio: 0.5
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 128
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 64
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 5
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
teacher_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
teacher_model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:418: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Student model is initialized with precision torch.bfloat16.
Memory stats initializing student model:
Memory stats after student model init:
	XPU peak memory active: 2.46 GiB
	XPU peak memory alloc: 2.46 GiB
	XPU peak memory reserved: 2.46 GiB
Teacher model is initialized with precision torch.bfloat16.
Memory stats after teacher model init:
	XPU peak memory active: 17.48 GiB
	XPU peak memory alloc: 17.48 GiB
	XPU peak memory reserved: 17.59 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
/home/jenkins/xiangdong/torchtune/torchtune/config/_instantiate.py:24: FutureWarning: CEWithChunkedOutputLoss is deprecated and will be removed in future versions. Please use `torchtune.modules.loss.LinearCrossEntropyLoss` instead.
  return _component_(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:282: FutureWarning: set_num_output_chunks is deprecated and will be removed in future versions. Please use LinearCrossEntropyLoss instead
  self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs/log_1753630625.txt
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 159/51760 [00:00<00:32, 1575.24it/s]Packing dataset:   1%|          | 348/51760 [00:00<00:29, 1759.64it/s]Packing dataset:   1%|          | 540/51760 [00:00<00:28, 1823.46it/s]Packing dataset:   1%|▏         | 723/51760 [00:00<00:28, 1796.14it/s]Packing dataset:   2%|▏         | 903/51760 [00:00<00:28, 1790.46it/s]Packing dataset:   2%|▏         | 1101/51760 [00:00<00:27, 1853.76it/s]Packing dataset:   2%|▏         | 1287/51760 [00:00<00:27, 1836.42it/s]Packing dataset:   3%|▎         | 1471/51760 [00:00<00:27, 1822.73it/s]Packing dataset:   3%|▎         | 1654/51760 [00:00<00:28, 1774.70it/s]Packing dataset:   4%|▎         | 1832/51760 [00:01<00:28, 1738.90it/s]Packing dataset:   4%|▍         | 2014/51760 [00:01<00:28, 1762.44it/s]Packing dataset:   4%|▍         | 2213/51760 [00:01<00:27, 1829.33it/s]Packing dataset:   5%|▍         | 2405/51760 [00:01<00:26, 1854.22it/s]Packing dataset:   5%|▌         | 2598/51760 [00:01<00:26, 1875.69it/s]Packing dataset:   5%|▌         | 2803/51760 [00:01<00:25, 1926.32it/s]Packing dataset:   6%|▌         | 2996/51760 [00:01<00:25, 1903.18it/s]Packing dataset:   6%|▌         | 3188/51760 [00:01<00:25, 1908.13it/s]Packing dataset:   7%|▋         | 3379/51760 [00:01<00:26, 1855.41it/s]Packing dataset:   7%|▋         | 3568/51760 [00:01<00:25, 1862.34it/s]Packing dataset:   7%|▋         | 3755/51760 [00:02<00:25, 1854.50it/s]Packing dataset:   8%|▊         | 3948/51760 [00:02<00:25, 1876.22it/s]Packing dataset:   8%|▊         | 4136/51760 [00:02<00:26, 1814.64it/s]Packing dataset:   8%|▊         | 4336/51760 [00:02<00:25, 1866.79it/s]Packing dataset:   9%|▊         | 4524/51760 [00:02<00:25, 1845.12it/s]Packing dataset:   9%|▉         | 4709/51760 [00:02<00:25, 1831.18it/s]Packing dataset:   9%|▉         | 4893/51760 [00:02<00:25, 1815.63it/s]Packing dataset:  10%|▉         | 5085/51760 [00:02<00:25, 1846.05it/s]Packing dataset:  10%|█         | 5270/51760 [00:02<00:25, 1806.66it/s]Packing dataset:  11%|█         | 5466/51760 [00:02<00:25, 1847.45it/s]Packing dataset:  11%|█         | 5655/51760 [00:03<00:24, 1858.16it/s]Packing dataset:  11%|█▏        | 5849/51760 [00:03<00:24, 1880.51it/s]Packing dataset:  12%|█▏        | 6038/51760 [00:03<00:24, 1872.40it/s]Packing dataset:  12%|█▏        | 6226/51760 [00:03<00:24, 1858.58it/s]Packing dataset:  12%|█▏        | 6412/51760 [00:03<00:24, 1857.71it/s]Packing dataset:  13%|█▎        | 6598/51760 [00:03<00:24, 1809.50it/s]Packing dataset:  13%|█▎        | 6782/51760 [00:03<00:24, 1814.70it/s]Packing dataset:  13%|█▎        | 6964/51760 [00:03<00:24, 1807.25it/s]Packing dataset:  14%|█▍        | 7145/51760 [00:03<00:24, 1801.96it/s]Packing dataset:  14%|█▍        | 7328/51760 [00:03<00:24, 1808.22it/s]Packing dataset:  15%|█▍        | 7509/51760 [00:04<00:24, 1792.93it/s]Packing dataset:  15%|█▍        | 7715/51760 [00:04<00:23, 1869.89it/s]Packing dataset:  15%|█▌        | 7903/51760 [00:04<00:23, 1832.78it/s]Packing dataset:  16%|█▌        | 8087/51760 [00:04<00:24, 1815.72it/s]Packing dataset:  16%|█▌        | 8269/51760 [00:04<00:24, 1779.22it/s]Packing dataset:  16%|█▋        | 8448/51760 [00:04<00:24, 1778.85it/s]Packing dataset:  17%|█▋        | 8637/51760 [00:04<00:23, 1811.02it/s]Packing dataset:  17%|█▋        | 8834/51760 [00:04<00:23, 1857.56it/s]Packing dataset:  17%|█▋        | 9027/51760 [00:04<00:22, 1877.64it/s]Packing dataset:  18%|█▊        | 9215/51760 [00:05<00:23, 1812.18it/s]Packing dataset:  18%|█▊        | 9485/51760 [00:05<00:20, 2070.24it/s]Packing dataset:  19%|█▉        | 9748/51760 [00:05<00:18, 2234.11it/s]Packing dataset:  19%|█▉        | 10047/51760 [00:05<00:16, 2456.87it/s]Packing dataset:  20%|█▉        | 10350/51760 [00:05<00:15, 2625.81it/s]Packing dataset:  21%|██        | 10669/51760 [00:05<00:14, 2793.28it/s]Packing dataset:  21%|██        | 10975/51760 [00:05<00:14, 2867.55it/s]Packing dataset:  22%|██▏       | 11263/51760 [00:05<00:14, 2865.78it/s]Packing dataset:  22%|██▏       | 11561/51760 [00:05<00:13, 2898.88it/s]Packing dataset:  23%|██▎       | 11862/51760 [00:05<00:13, 2927.59it/s]Packing dataset:  23%|██▎       | 12155/51760 [00:06<00:13, 2910.38it/s]Packing dataset:  24%|██▍       | 12462/51760 [00:06<00:13, 2957.46it/s]Packing dataset:  25%|██▍       | 12758/51760 [00:06<00:13, 2956.68it/s]Packing dataset:  25%|██▌       | 13054/51760 [00:06<00:13, 2956.03it/s]Packing dataset:  26%|██▌       | 13350/51760 [00:06<00:13, 2937.76it/s]Packing dataset:  26%|██▋       | 13645/51760 [00:06<00:12, 2940.08it/s]Packing dataset:  27%|██▋       | 13946/51760 [00:06<00:12, 2959.34it/s]Packing dataset:  28%|██▊       | 14242/51760 [00:06<00:12, 2952.12it/s]Packing dataset:  28%|██▊       | 14553/51760 [00:06<00:12, 2998.84it/s]Packing dataset:  29%|██▊       | 14853/51760 [00:06<00:12, 2960.34it/s]Packing dataset:  29%|██▉       | 15158/51760 [00:07<00:12, 2985.03it/s]Packing dataset:  30%|██▉       | 15459/51760 [00:07<00:12, 2992.34it/s]Packing dataset:  30%|███       | 15759/51760 [00:07<00:12, 2991.72it/s]Packing dataset:  31%|███       | 16079/51760 [00:07<00:11, 3050.25it/s]Packing dataset:  32%|███▏      | 16385/51760 [00:07<00:11, 2978.71it/s]Packing dataset:  32%|███▏      | 16688/51760 [00:07<00:11, 2992.35it/s]Packing dataset:  33%|███▎      | 16988/51760 [00:07<00:11, 2963.97it/s]Packing dataset:  33%|███▎      | 17286/51760 [00:07<00:11, 2963.73it/s]Packing dataset:  34%|███▍      | 17583/51760 [00:07<00:11, 2930.51it/s]Packing dataset:  35%|███▍      | 17877/51760 [00:07<00:11, 2891.14it/s]Packing dataset:  35%|███▌      | 18167/51760 [00:08<00:11, 2889.57it/s]Packing dataset:  36%|███▌      | 18457/51760 [00:08<00:11, 2876.40it/s]Packing dataset:  36%|███▋      | 18770/51760 [00:08<00:11, 2948.87it/s]Packing dataset:  37%|███▋      | 19066/51760 [00:08<00:11, 2920.57it/s]Packing dataset:  37%|███▋      | 19372/51760 [00:08<00:10, 2959.23it/s]Packing dataset:  38%|███▊      | 19669/51760 [00:08<00:11, 2910.83it/s]Packing dataset:  39%|███▊      | 19965/51760 [00:08<00:10, 2923.79it/s]Packing dataset:  39%|███▉      | 20260/51760 [00:08<00:10, 2931.02it/s]Packing dataset:  40%|███▉      | 20554/51760 [00:08<00:10, 2888.70it/s]Packing dataset:  40%|████      | 20862/51760 [00:08<00:10, 2941.09it/s]Packing dataset:  41%|████      | 21182/51760 [00:09<00:10, 3016.61it/s]Packing dataset:  42%|████▏     | 21484/51760 [00:09<00:10, 2987.68it/s]Packing dataset:  42%|████▏     | 21783/51760 [00:09<00:10, 2940.43it/s]Packing dataset:  43%|████▎     | 22078/51760 [00:09<00:10, 2908.96it/s]Packing dataset:  43%|████▎     | 22373/51760 [00:09<00:10, 2920.79it/s]Packing dataset:  44%|████▍     | 22666/51760 [00:09<00:10, 2908.76it/s]Packing dataset:  44%|████▍     | 22971/51760 [00:09<00:09, 2949.91it/s]Packing dataset:  45%|████▍     | 23267/51760 [00:09<00:09, 2906.51it/s]Packing dataset:  46%|████▌     | 23580/51760 [00:09<00:09, 2970.42it/s]Packing dataset:  46%|████▌     | 23878/51760 [00:10<00:09, 2941.25it/s]Packing dataset:  47%|████▋     | 24180/51760 [00:10<00:09, 2962.66it/s]Packing dataset:  47%|████▋     | 24491/51760 [00:10<00:09, 3000.63it/s]Packing dataset:  48%|████▊     | 24795/51760 [00:10<00:08, 3007.56it/s]Packing dataset:  48%|████▊     | 25096/51760 [00:10<00:09, 2940.84it/s]Packing dataset:  49%|████▉     | 25407/51760 [00:10<00:08, 2990.45it/s]Packing dataset:  50%|████▉     | 25707/51760 [00:10<00:08, 2959.09it/s]Packing dataset:  50%|█████     | 26004/51760 [00:10<00:08, 2933.54it/s]Packing dataset:  51%|█████     | 26298/51760 [00:10<00:08, 2920.79it/s]Packing dataset:  51%|█████▏    | 26598/51760 [00:10<00:08, 2943.78it/s]Packing dataset:  52%|█████▏    | 26901/51760 [00:11<00:08, 2965.75it/s]Packing dataset:  53%|█████▎    | 27206/51760 [00:11<00:08, 2990.37it/s]Packing dataset:  53%|█████▎    | 27506/51760 [00:11<00:08, 2960.91it/s]Packing dataset:  54%|█████▎    | 27803/51760 [00:11<00:08, 2945.06it/s]Packing dataset:  54%|█████▍    | 28098/51760 [00:11<00:08, 2899.69it/s]Packing dataset:  55%|█████▍    | 28390/51760 [00:11<00:08, 2905.12it/s]Packing dataset:  55%|█████▌    | 28681/51760 [00:11<00:08, 2880.06it/s]Packing dataset:  56%|█████▌    | 28970/51760 [00:11<00:07, 2881.76it/s]Packing dataset:  57%|█████▋    | 29259/51760 [00:11<00:07, 2883.50it/s]Packing dataset:  57%|█████▋    | 29552/51760 [00:11<00:07, 2894.96it/s]Packing dataset:  58%|█████▊    | 29845/51760 [00:12<00:07, 2902.41it/s]Packing dataset:  58%|█████▊    | 30139/51760 [00:12<00:07, 2912.62it/s]Packing dataset:  59%|█████▉    | 30431/51760 [00:12<00:07, 2889.54it/s]Packing dataset:  59%|█████▉    | 30723/51760 [00:12<00:07, 2895.75it/s]Packing dataset:  60%|█████▉    | 31013/51760 [00:12<00:07, 2890.28it/s]Packing dataset:  60%|██████    | 31303/51760 [00:12<00:07, 2873.05it/s]Packing dataset:  61%|██████    | 31618/51760 [00:12<00:06, 2954.86it/s]Packing dataset:  62%|██████▏   | 31926/51760 [00:12<00:06, 2991.08it/s]Packing dataset:  62%|██████▏   | 32226/51760 [00:12<00:06, 2930.61it/s]Packing dataset:  63%|██████▎   | 32531/51760 [00:12<00:06, 2963.20it/s]Packing dataset:  63%|██████▎   | 32828/51760 [00:13<00:06, 2957.93it/s]Packing dataset:  64%|██████▍   | 33126/51760 [00:13<00:06, 2960.78it/s]Packing dataset:  65%|██████▍   | 33437/51760 [00:13<00:06, 3004.30it/s]Packing dataset:  65%|██████▌   | 33738/51760 [00:13<00:06, 2973.49it/s]Packing dataset:  66%|██████▌   | 34039/51760 [00:13<00:05, 2980.10it/s]Packing dataset:  66%|██████▋   | 34338/51760 [00:13<00:05, 2960.32it/s]Packing dataset:  67%|██████▋   | 34635/51760 [00:13<00:05, 2956.12it/s]Packing dataset:  67%|██████▋   | 34933/51760 [00:13<00:05, 2958.31it/s]Packing dataset:  68%|██████▊   | 35229/51760 [00:13<00:05, 2954.96it/s]Packing dataset:  69%|██████▊   | 35542/51760 [00:13<00:05, 3002.94it/s]Packing dataset:  69%|██████▉   | 35843/51760 [00:14<00:05, 2920.40it/s]Packing dataset:  70%|██████▉   | 36137/51760 [00:14<00:05, 2921.33it/s]Packing dataset:  70%|███████   | 36438/51760 [00:14<00:05, 2947.12it/s]Packing dataset:  71%|███████   | 36751/51760 [00:14<00:05, 3001.25it/s]Packing dataset:  72%|███████▏  | 37052/51760 [00:14<00:04, 2953.35it/s]Packing dataset:  72%|███████▏  | 37349/51760 [00:14<00:04, 2955.95it/s]Packing dataset:  73%|███████▎  | 37655/51760 [00:14<00:04, 2982.38it/s]Packing dataset:  73%|███████▎  | 37957/51760 [00:14<00:04, 2992.54it/s]Packing dataset:  74%|███████▍  | 38257/51760 [00:14<00:04, 2990.99it/s]Packing dataset:  74%|███████▍  | 38557/51760 [00:14<00:04, 2959.91it/s]Packing dataset:  75%|███████▌  | 38854/51760 [00:15<00:04, 2932.38it/s]Packing dataset:  76%|███████▌  | 39156/51760 [00:15<00:04, 2956.89it/s]Packing dataset:  76%|███████▌  | 39453/51760 [00:15<00:04, 2957.82it/s]Packing dataset:  77%|███████▋  | 39755/51760 [00:15<00:04, 2972.75it/s]Packing dataset:  77%|███████▋  | 40059/51760 [00:15<00:03, 2992.28it/s]Packing dataset:  78%|███████▊  | 40359/51760 [00:15<00:03, 2974.06it/s]Packing dataset:  79%|███████▊  | 40665/51760 [00:15<00:03, 2998.47it/s]Packing dataset:  79%|███████▉  | 40965/51760 [00:15<00:03, 2989.10it/s]Packing dataset:  80%|███████▉  | 41264/51760 [00:15<00:03, 2972.96it/s]Packing dataset:  80%|████████  | 41563/51760 [00:15<00:03, 2974.90it/s]Packing dataset:  81%|████████  | 41861/51760 [00:16<00:03, 2963.92it/s]Packing dataset:  81%|████████▏ | 42158/51760 [00:16<00:03, 2912.65it/s]Packing dataset:  82%|████████▏ | 42470/51760 [00:16<00:03, 2973.50it/s]Packing dataset:  83%|████████▎ | 42768/51760 [00:16<00:03, 2908.32it/s]Packing dataset:  83%|████████▎ | 43060/51760 [00:16<00:03, 2868.89it/s]Packing dataset:  84%|████████▎ | 43348/51760 [00:16<00:02, 2857.24it/s]Packing dataset:  84%|████████▍ | 43649/51760 [00:16<00:02, 2899.91it/s]Packing dataset:  85%|████████▍ | 43940/51760 [00:16<00:02, 2896.93it/s]Packing dataset:  85%|████████▌ | 44232/51760 [00:16<00:02, 2903.15it/s]Packing dataset:  86%|████████▌ | 44537/51760 [00:17<00:02, 2946.52it/s]Packing dataset:  87%|████████▋ | 44834/51760 [00:17<00:02, 2950.83it/s]Packing dataset:  87%|████████▋ | 45141/51760 [00:17<00:02, 2982.16it/s]Packing dataset:  88%|████████▊ | 45451/51760 [00:17<00:02, 3016.53it/s]Packing dataset:  88%|████████▊ | 45753/51760 [00:17<00:02, 2959.89it/s]Packing dataset:  89%|████████▉ | 46050/51760 [00:17<00:01, 2918.50it/s]Packing dataset:  90%|████████▉ | 46356/51760 [00:17<00:01, 2959.88it/s]Packing dataset:  90%|█████████ | 46653/51760 [00:17<00:01, 2955.37it/s]Packing dataset:  91%|█████████ | 46953/51760 [00:17<00:01, 2967.27it/s]Packing dataset:  91%|█████████▏| 47250/51760 [00:17<00:01, 2930.28it/s]Packing dataset:  92%|█████████▏| 47558/51760 [00:18<00:01, 2970.21it/s]Packing dataset:  92%|█████████▏| 47856/51760 [00:18<00:01, 2947.03it/s]Packing dataset:  93%|█████████▎| 48151/51760 [00:18<00:01, 2920.45it/s]Packing dataset:  94%|█████████▎| 48453/51760 [00:18<00:01, 2947.57it/s]Packing dataset:  94%|█████████▍| 48758/51760 [00:18<00:01, 2975.50it/s]Packing dataset:  95%|█████████▍| 49056/51760 [00:18<00:00, 2960.68it/s]Packing dataset:  95%|█████████▌| 49356/51760 [00:18<00:00, 2967.94it/s]Packing dataset:  96%|█████████▌| 49663/51760 [00:18<00:00, 2997.58it/s]Packing dataset:  97%|█████████▋| 49963/51760 [00:18<00:00, 2992.38it/s]Packing dataset:  97%|█████████▋| 50263/51760 [00:18<00:00, 2945.60it/s]Packing dataset:  98%|█████████▊| 50558/51760 [00:19<00:00, 2015.02it/s]Packing dataset:  98%|█████████▊| 50854/51760 [00:19<00:00, 2226.26it/s]Packing dataset:  99%|█████████▉| 51156/51760 [00:19<00:00, 2418.37it/s]Packing dataset:  99%|█████████▉| 51448/51760 [00:19<00:00, 2544.50it/s]Packing dataset: 100%|█████████▉| 51747/51760 [00:19<00:00, 2662.04it/s]Packing dataset: 100%|██████████| 51760/51760 [00:19<00:00, 2638.47it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/torchtune/modules/transformer.py:684: FutureWarning: chunked_output is deprecated and will be removed in future versions. Please use self.skip_output_layer=True and use a linear loss instead
  output = self.chunked_output(h)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:647: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:05<00:53,  5.90s/it]1|1|Loss: 1.8860052824020386:  10%|█         | 1/10 [00:05<00:53,  5.90s/it]1|1|Loss: 1.8860052824020386:  20%|██        | 2/10 [00:10<00:41,  5.15s/it]1|2|Loss: 2.0138602256774902:  20%|██        | 2/10 [00:10<00:41,  5.15s/it]1|2|Loss: 2.0138602256774902:  30%|███       | 3/10 [00:15<00:34,  4.90s/it]1|3|Loss: 1.788622260093689:  30%|███       | 3/10 [00:15<00:34,  4.90s/it] 1|3|Loss: 1.788622260093689:  40%|████      | 4/10 [00:19<00:28,  4.80s/it]1|4|Loss: 1.8722302913665771:  40%|████      | 4/10 [00:19<00:28,  4.80s/it]1|4|Loss: 1.8722302913665771:  50%|█████     | 5/10 [00:24<00:23,  4.75s/it]1|5|Loss: 1.933995246887207:  50%|█████     | 5/10 [00:24<00:23,  4.75s/it] 1|5|Loss: 1.933995246887207:  60%|██████    | 6/10 [00:29<00:18,  4.71s/it]1|6|Loss: 2.0017309188842773:  60%|██████    | 6/10 [00:29<00:18,  4.71s/it]1|6|Loss: 2.0017309188842773:  70%|███████   | 7/10 [00:33<00:14,  4.68s/it]1|7|Loss: 1.8968682289123535:  70%|███████   | 7/10 [00:33<00:14,  4.68s/it]1|7|Loss: 1.8968682289123535:  80%|████████  | 8/10 [00:38<00:09,  4.67s/it]1|8|Loss: 1.7879400253295898:  80%|████████  | 8/10 [00:38<00:09,  4.67s/it]1|8|Loss: 1.7879400253295898:  90%|█████████ | 9/10 [00:42<00:04,  4.66s/it]1|9|Loss: 1.6826136112213135:  90%|█████████ | 9/10 [00:42<00:04,  4.66s/it]1|9|Loss: 1.6826136112213135: 100%|██████████| 10/10 [00:47<00:00,  4.65s/it]1|10|Loss: 1.7530020475387573: 100%|██████████| 10/10 [00:47<00:00,  4.65s/it]1|10|Loss: 1.7530020475387573: 100%|██████████| 10/10 [00:47<00:00,  4.76s/it]
iteration:  3 tokens:  11612 time:  4.602041953010485 tokens_per_second_on_single_device:  2523.23
iteration:  4 tokens:  12699 time:  4.630269138026051 tokens_per_second_on_single_device:  2742.61
iteration:  5 tokens:  13246 time:  4.673153694020584 tokens_per_second_on_single_device:  2834.49
iteration:  6 tokens:  12370 time:  4.623536181985401 tokens_per_second_on_single_device:  2675.44
iteration:  7 tokens:  12510 time:  4.615567127941176 tokens_per_second_on_single_device:  2710.39
iteration:  8 tokens:  12921 time:  4.63932390906848 tokens_per_second_on_single_device:  2785.1
iteration:  9 tokens:  12637 time:  4.637046142015606 tokens_per_second_on_single_device:  2725.23
iteration:  10 tokens:  12364 time:  4.622285494930111 tokens_per_second_on_single_device:  2674.87
avg tokens_per_second_on_single_device:  2709.24
