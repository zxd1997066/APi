Running KDRecipeSingleDevice with resolved config:

batch_size: 4
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Llama-3.2-1B-Instruct/
  checkpoint_files:
  - model.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
compile: false
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: true
device: xpu
dtype: bf16
enable_activation_checkpointing: false
enable_activation_offloading: false
epochs: 1
gradient_accumulation_steps: 8
kd_loss:
  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss
kd_ratio: 0.5
log_every_n_steps: 1
log_level: INFO
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: 10
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
  apply_lora_to_mlp: true
  apply_lora_to_output: false
  lora_alpha: 128
  lora_attn_modules:
  - q_proj
  - v_proj
  - output_proj
  lora_dropout: 0.0
  lora_rank: 64
optimizer:
  _component_: torch.optim.AdamW
  fused: true
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
profiler:
  _component_: torchtune.training.setup_torch_profiler
  active_steps: 2
  cpu: true
  cuda: true
  enabled: false
  num_cycles: 1
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/profiling_outputs
  profile_memory: false
  record_shapes: true
  wait_steps: 5
  warmup_steps: 5
  with_flops: false
  with_stack: false
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: 123
shuffle: true
teacher_checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /tmp/Meta-Llama-3.1-8B-Instruct/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed
  recipe_checkpoint: null
teacher_model:
  _component_: torchtune.models.llama3_1.llama3_1_8b
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: 512
  path: /tmp/Llama-3.2-1B-Instruct/original/tokenizer.model

/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:418: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  validate_missing_and_unexpected_for_lora(
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.
  return obj(*args, **kwargs)
Student model is initialized with precision torch.bfloat16.
Memory stats initializing student model:
Memory stats after student model init:
	XPU peak memory active: 2.46 GiB
	XPU peak memory alloc: 2.46 GiB
	XPU peak memory reserved: 2.46 GiB
Teacher model is initialized with precision torch.bfloat16.
Memory stats after teacher model init:
	XPU peak memory active: 17.48 GiB
	XPU peak memory alloc: 17.48 GiB
	XPU peak memory reserved: 17.59 GiB
Tokenizer is initialized from file.
Optimizer and loss are initialized.
/home/jenkins/xiangdong/torchtune/torchtune/config/_instantiate.py:24: FutureWarning: CEWithChunkedOutputLoss is deprecated and will be removed in future versions. Please use `torchtune.modules.loss.LinearCrossEntropyLoss` instead.
  return _component_(*args, **kwargs)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:282: FutureWarning: set_num_output_chunks is deprecated and will be removed in future versions. Please use LinearCrossEntropyLoss instead
  self._model.set_num_output_chunks(self._loss_fn.num_output_chunks)
Loss is initialized.
Writing logs to /tmp/torchtune/llama3_2_8B_to_1B/KD_lora_distributed/logs/log_1754828419.txt
Packing dataset:   0%|          | 0/51760 [00:00<?, ?it/s]Packing dataset:   0%|          | 164/51760 [00:00<00:31, 1636.19it/s]Packing dataset:   1%|          | 369/51760 [00:00<00:27, 1876.76it/s]Packing dataset:   1%|          | 557/51760 [00:00<00:27, 1863.65it/s]Packing dataset:   1%|▏         | 744/51760 [00:00<00:27, 1857.20it/s]Packing dataset:   2%|▏         | 930/51760 [00:00<00:27, 1854.93it/s]Packing dataset:   2%|▏         | 1137/51760 [00:00<00:26, 1924.90it/s]Packing dataset:   3%|▎         | 1330/51760 [00:00<00:26, 1914.54it/s]Packing dataset:   3%|▎         | 1522/51760 [00:00<00:26, 1895.33it/s]Packing dataset:   3%|▎         | 1712/51760 [00:00<00:27, 1816.37it/s]Packing dataset:   4%|▎         | 1901/51760 [00:01<00:27, 1838.03it/s]Packing dataset:   4%|▍         | 2086/51760 [00:01<00:27, 1814.68it/s]Packing dataset:   4%|▍         | 2300/51760 [00:01<00:25, 1910.18it/s]Packing dataset:   5%|▍         | 2499/51760 [00:01<00:25, 1923.21it/s]Packing dataset:   5%|▌         | 2708/51760 [00:01<00:24, 1971.64it/s]Packing dataset:   6%|▌         | 2907/51760 [00:01<00:24, 1976.94it/s]Packing dataset:   6%|▌         | 3105/51760 [00:01<00:24, 1963.91it/s]Packing dataset:   6%|▋         | 3302/51760 [00:01<00:24, 1959.12it/s]Packing dataset:   7%|▋         | 3499/51760 [00:01<00:25, 1908.03it/s]Packing dataset:   7%|▋         | 3691/51760 [00:01<00:25, 1900.67it/s]Packing dataset:   8%|▊         | 3892/51760 [00:02<00:24, 1930.78it/s]Packing dataset:   8%|▊         | 4086/51760 [00:02<00:25, 1899.17it/s]Packing dataset:   8%|▊         | 4284/51760 [00:02<00:24, 1920.80it/s]Packing dataset:   9%|▊         | 4477/51760 [00:02<00:24, 1895.47it/s]Packing dataset:   9%|▉         | 4667/51760 [00:02<00:25, 1871.86it/s]Packing dataset:   9%|▉         | 4864/51760 [00:02<00:24, 1899.24it/s]Packing dataset:  10%|▉         | 5059/51760 [00:02<00:24, 1913.57it/s]Packing dataset:  10%|█         | 5251/51760 [00:02<00:24, 1867.54it/s]Packing dataset:  11%|█         | 5455/51760 [00:02<00:24, 1915.02it/s]Packing dataset:  11%|█         | 5650/51760 [00:02<00:23, 1924.36it/s]Packing dataset:  11%|█▏        | 5849/51760 [00:03<00:23, 1942.88it/s]Packing dataset:  12%|█▏        | 6044/51760 [00:03<00:23, 1928.93it/s]Packing dataset:  12%|█▏        | 6238/51760 [00:03<00:23, 1923.61it/s]Packing dataset:  12%|█▏        | 6431/51760 [00:03<00:23, 1923.06it/s]Packing dataset:  13%|█▎        | 6624/51760 [00:03<00:24, 1871.52it/s]Packing dataset:  13%|█▎        | 6812/51760 [00:03<00:24, 1861.57it/s]Packing dataset:  14%|█▎        | 6999/51760 [00:03<00:24, 1862.16it/s]Packing dataset:  14%|█▍        | 7190/51760 [00:03<00:23, 1875.80it/s]Packing dataset:  14%|█▍        | 7378/51760 [00:03<00:23, 1865.83it/s]Packing dataset:  15%|█▍        | 7580/51760 [00:03<00:23, 1910.81it/s]Packing dataset:  15%|█▌        | 7772/51760 [00:04<00:23, 1897.00it/s]Packing dataset:  15%|█▌        | 7962/51760 [00:04<00:23, 1892.62it/s]Packing dataset:  16%|█▌        | 8152/51760 [00:04<00:23, 1854.65it/s]Packing dataset:  16%|█▌        | 8338/51760 [00:04<00:24, 1804.72it/s]Packing dataset:  16%|█▋        | 8536/51760 [00:04<00:23, 1855.44it/s]Packing dataset:  17%|█▋        | 8722/51760 [00:04<00:23, 1855.82it/s]Packing dataset:  17%|█▋        | 8925/51760 [00:04<00:22, 1905.84it/s]Packing dataset:  18%|█▊        | 9116/51760 [00:04<00:22, 1887.49it/s]Packing dataset:  18%|█▊        | 9305/51760 [00:04<00:22, 1869.22it/s]Packing dataset:  18%|█▊        | 9494/51760 [00:05<00:22, 1874.03it/s]Packing dataset:  19%|█▉        | 9735/51760 [00:05<00:20, 2031.05it/s]Packing dataset:  19%|█▉        | 10034/51760 [00:05<00:18, 2314.72it/s]Packing dataset:  20%|█▉        | 10342/51760 [00:05<00:16, 2540.55it/s]Packing dataset:  21%|██        | 10674/51760 [00:05<00:14, 2772.22it/s]Packing dataset:  21%|██        | 10983/51760 [00:05<00:14, 2866.26it/s]Packing dataset:  22%|██▏       | 11278/51760 [00:05<00:14, 2888.65it/s]Packing dataset:  22%|██▏       | 11596/51760 [00:05<00:13, 2973.28it/s]Packing dataset:  23%|██▎       | 11900/51760 [00:05<00:13, 2988.69it/s]Packing dataset:  24%|██▎       | 12199/51760 [00:05<00:13, 2978.80it/s]Packing dataset:  24%|██▍       | 12508/51760 [00:06<00:13, 3011.59it/s]Packing dataset:  25%|██▍       | 12820/51760 [00:06<00:12, 3042.99it/s]Packing dataset:  25%|██▌       | 13131/51760 [00:06<00:12, 3062.77it/s]Packing dataset:  26%|██▌       | 13438/51760 [00:06<00:12, 3031.16it/s]Packing dataset:  27%|██▋       | 13743/51760 [00:06<00:12, 3035.91it/s]Packing dataset:  27%|██▋       | 14047/51760 [00:06<00:12, 3019.16it/s]Packing dataset:  28%|██▊       | 14374/51760 [00:06<00:12, 3093.34it/s]Packing dataset:  28%|██▊       | 14684/51760 [00:06<00:12, 3074.16it/s]Packing dataset:  29%|██▉       | 14992/51760 [00:06<00:12, 3063.96it/s]Packing dataset:  30%|██▉       | 15299/51760 [00:06<00:11, 3058.38it/s]Packing dataset:  30%|███       | 15618/51760 [00:07<00:11, 3096.54it/s]Packing dataset:  31%|███       | 15941/51760 [00:07<00:11, 3135.86it/s]Packing dataset:  31%|███▏      | 16255/51760 [00:07<00:11, 3124.33it/s]Packing dataset:  32%|███▏      | 16568/51760 [00:07<00:11, 3060.88it/s]Packing dataset:  33%|███▎      | 16888/51760 [00:07<00:11, 3099.90it/s]Packing dataset:  33%|███▎      | 17199/51760 [00:07<00:11, 3078.10it/s]Packing dataset:  34%|███▍      | 17508/51760 [00:07<00:11, 3057.74it/s]Packing dataset:  34%|███▍      | 17814/51760 [00:07<00:11, 3043.52it/s]Packing dataset:  35%|███▌      | 18119/51760 [00:07<00:11, 3042.68it/s]Packing dataset:  36%|███▌      | 18424/51760 [00:07<00:11, 3001.14it/s]Packing dataset:  36%|███▌      | 18753/51760 [00:08<00:10, 3085.07it/s]Packing dataset:  37%|███▋      | 19062/51760 [00:08<00:10, 3060.81it/s]Packing dataset:  37%|███▋      | 19371/51760 [00:08<00:10, 3068.49it/s]Packing dataset:  38%|███▊      | 19678/51760 [00:08<00:10, 3009.95it/s]Packing dataset:  39%|███▊      | 19989/51760 [00:08<00:10, 3037.43it/s]Packing dataset:  39%|███▉      | 20294/51760 [00:08<00:10, 3019.50it/s]Packing dataset:  40%|███▉      | 20597/51760 [00:08<00:10, 2975.69it/s]Packing dataset:  40%|████      | 20907/51760 [00:08<00:10, 3011.30it/s]Packing dataset:  41%|████      | 21239/51760 [00:08<00:09, 3099.03it/s]Packing dataset:  42%|████▏     | 21550/51760 [00:08<00:09, 3056.55it/s]Packing dataset:  42%|████▏     | 21861/51760 [00:09<00:09, 3071.05it/s]Packing dataset:  43%|████▎     | 22169/51760 [00:09<00:09, 2978.14it/s]Packing dataset:  43%|████▎     | 22492/51760 [00:09<00:09, 3050.97it/s]Packing dataset:  44%|████▍     | 22798/51760 [00:09<00:09, 2973.56it/s]Packing dataset:  45%|████▍     | 23097/51760 [00:09<00:09, 2964.75it/s]Packing dataset:  45%|████▌     | 23410/51760 [00:09<00:09, 3012.82it/s]Packing dataset:  46%|████▌     | 23729/51760 [00:09<00:09, 3063.87it/s]Packing dataset:  46%|████▋     | 24036/51760 [00:09<00:09, 3053.57it/s]Packing dataset:  47%|████▋     | 24346/51760 [00:09<00:08, 3065.85it/s]Packing dataset:  48%|████▊     | 24670/51760 [00:10<00:08, 3091.27it/s]Packing dataset:  48%|████▊     | 24980/51760 [00:10<00:08, 3061.51it/s]Packing dataset:  49%|████▉     | 25287/51760 [00:10<00:08, 3042.91it/s]Packing dataset:  49%|████▉     | 25592/51760 [00:10<00:08, 3039.45it/s]Packing dataset:  50%|█████     | 25897/51760 [00:10<00:08, 3009.75it/s]Packing dataset:  51%|█████     | 26226/51760 [00:10<00:08, 3091.28it/s]Packing dataset:  51%|█████▏    | 26536/51760 [00:10<00:08, 3076.12it/s]Packing dataset:  52%|█████▏    | 26844/51760 [00:10<00:08, 3057.81it/s]Packing dataset:  52%|█████▏    | 27171/51760 [00:10<00:07, 3118.60it/s]Packing dataset:  53%|█████▎    | 27483/51760 [00:10<00:07, 3106.53it/s]Packing dataset:  54%|█████▎    | 27794/51760 [00:11<00:07, 3078.41it/s]Packing dataset:  54%|█████▍    | 28102/51760 [00:11<00:07, 3041.25it/s]Packing dataset:  55%|█████▍    | 28407/51760 [00:11<00:07, 3026.66it/s]Packing dataset:  55%|█████▌    | 28711/51760 [00:11<00:07, 3030.36it/s]Packing dataset:  56%|█████▌    | 29015/51760 [00:11<00:07, 2978.06it/s]Packing dataset:  57%|█████▋    | 29325/51760 [00:11<00:07, 3012.73it/s]Packing dataset:  57%|█████▋    | 29627/51760 [00:11<00:07, 2979.45it/s]Packing dataset:  58%|█████▊    | 29929/51760 [00:11<00:07, 2989.72it/s]Packing dataset:  58%|█████▊    | 30246/51760 [00:11<00:07, 3041.16it/s]Packing dataset:  59%|█████▉    | 30551/51760 [00:11<00:07, 2981.95it/s]Packing dataset:  60%|█████▉    | 30857/51760 [00:12<00:06, 3003.68it/s]Packing dataset:  60%|██████    | 31158/51760 [00:12<00:09, 2115.55it/s]Packing dataset:  61%|██████    | 31453/51760 [00:12<00:08, 2306.52it/s]Packing dataset:  61%|██████▏   | 31785/51760 [00:12<00:07, 2556.24it/s]Packing dataset:  62%|██████▏   | 32078/51760 [00:12<00:07, 2652.67it/s]Packing dataset:  63%|██████▎   | 32390/51760 [00:12<00:06, 2778.74it/s]Packing dataset:  63%|██████▎   | 32684/51760 [00:12<00:06, 2818.75it/s]Packing dataset:  64%|██████▎   | 32993/51760 [00:12<00:06, 2893.68it/s]Packing dataset:  64%|██████▍   | 33305/51760 [00:12<00:06, 2955.29it/s]Packing dataset:  65%|██████▍   | 33618/51760 [00:13<00:06, 3004.99it/s]Packing dataset:  66%|██████▌   | 33924/51760 [00:13<00:05, 3020.66it/s]Packing dataset:  66%|██████▌   | 34230/51760 [00:13<00:05, 3009.07it/s]Packing dataset:  67%|██████▋   | 34551/51760 [00:13<00:05, 3064.59it/s]Packing dataset:  67%|██████▋   | 34860/51760 [00:13<00:05, 3066.55it/s]Packing dataset:  68%|██████▊   | 35168/51760 [00:13<00:05, 3047.92it/s]Packing dataset:  69%|██████▊   | 35494/51760 [00:13<00:05, 3110.40it/s]Packing dataset:  69%|██████▉   | 35806/51760 [00:13<00:05, 3023.52it/s]Packing dataset:  70%|██████▉   | 36110/51760 [00:13<00:05, 2997.81it/s]Packing dataset:  70%|███████   | 36419/51760 [00:14<00:05, 3022.51it/s]Packing dataset:  71%|███████   | 36736/51760 [00:14<00:04, 3065.64it/s]Packing dataset:  72%|███████▏  | 37043/51760 [00:14<00:04, 3009.68it/s]Packing dataset:  72%|███████▏  | 37354/51760 [00:14<00:04, 3034.09it/s]Packing dataset:  73%|███████▎  | 37673/51760 [00:14<00:04, 3079.42it/s]Packing dataset:  73%|███████▎  | 37993/51760 [00:14<00:04, 3111.66it/s]Packing dataset:  74%|███████▍  | 38305/51760 [00:14<00:04, 3097.19it/s]Packing dataset:  75%|███████▍  | 38615/51760 [00:14<00:04, 3043.19it/s]Packing dataset:  75%|███████▌  | 38920/51760 [00:14<00:04, 3040.50it/s]Packing dataset:  76%|███████▌  | 39233/51760 [00:14<00:04, 3066.47it/s]Packing dataset:  76%|███████▋  | 39540/51760 [00:15<00:04, 3031.33it/s]Packing dataset:  77%|███████▋  | 39849/51760 [00:15<00:03, 3044.23it/s]Packing dataset:  78%|███████▊  | 40154/51760 [00:15<00:03, 2998.08it/s]Packing dataset:  78%|███████▊  | 40466/51760 [00:15<00:03, 3033.77it/s]Packing dataset:  79%|███████▉  | 40772/51760 [00:15<00:03, 3040.22it/s]Packing dataset:  79%|███████▉  | 41085/51760 [00:15<00:03, 3066.35it/s]Packing dataset:  80%|███████▉  | 41392/51760 [00:15<00:03, 3049.33it/s]Packing dataset:  81%|████████  | 41698/51760 [00:15<00:03, 3043.72it/s]Packing dataset:  81%|████████  | 42003/51760 [00:15<00:03, 3002.10it/s]Packing dataset:  82%|████████▏ | 42304/51760 [00:15<00:03, 2999.67it/s]Packing dataset:  82%|████████▏ | 42606/51760 [00:16<00:03, 3005.29it/s]Packing dataset:  83%|████████▎ | 42907/51760 [00:16<00:02, 2996.88it/s]Packing dataset:  83%|████████▎ | 43207/51760 [00:16<00:02, 2938.00it/s]Packing dataset:  84%|████████▍ | 43515/51760 [00:16<00:02, 2975.32it/s]Packing dataset:  85%|████████▍ | 43821/51760 [00:16<00:02, 2999.89it/s]Packing dataset:  85%|████████▌ | 44122/51760 [00:16<00:02, 2970.70it/s]Packing dataset:  86%|████████▌ | 44437/51760 [00:16<00:02, 3019.69it/s]Packing dataset:  86%|████████▋ | 44746/51760 [00:16<00:02, 3039.38it/s]Packing dataset:  87%|████████▋ | 45052/51760 [00:16<00:02, 3040.70it/s]Packing dataset:  88%|████████▊ | 45370/51760 [00:16<00:02, 3080.54it/s]Packing dataset:  88%|████████▊ | 45679/51760 [00:17<00:01, 3072.92it/s]Packing dataset:  89%|████████▉ | 45987/51760 [00:17<00:01, 3024.22it/s]Packing dataset:  89%|████████▉ | 46306/51760 [00:17<00:01, 3069.95it/s]Packing dataset:  90%|█████████ | 46614/51760 [00:17<00:01, 3045.39it/s]Packing dataset:  91%|█████████ | 46929/51760 [00:17<00:01, 3075.95it/s]Packing dataset:  91%|█████████▏| 47237/51760 [00:17<00:01, 3047.17it/s]Packing dataset:  92%|█████████▏| 47552/51760 [00:17<00:01, 3072.89it/s]Packing dataset:  92%|█████████▏| 47860/51760 [00:17<00:01, 3043.74it/s]Packing dataset:  93%|█████████▎| 48165/51760 [00:17<00:01, 3020.41it/s]Packing dataset:  94%|█████████▎| 48468/51760 [00:17<00:01, 3013.29it/s]Packing dataset:  94%|█████████▍| 48781/51760 [00:18<00:00, 3045.02it/s]Packing dataset:  95%|█████████▍| 49086/51760 [00:18<00:00, 3031.31it/s]Packing dataset:  95%|█████████▌| 49394/51760 [00:18<00:00, 3045.32it/s]Packing dataset:  96%|█████████▌| 49709/51760 [00:18<00:00, 3072.17it/s]Packing dataset:  97%|█████████▋| 50017/51760 [00:18<00:00, 3072.94it/s]Packing dataset:  97%|█████████▋| 50325/51760 [00:18<00:00, 3032.22it/s]Packing dataset:  98%|█████████▊| 50635/51760 [00:18<00:00, 3049.05it/s]Packing dataset:  98%|█████████▊| 50943/51760 [00:18<00:00, 3055.82it/s]Packing dataset:  99%|█████████▉| 51253/51760 [00:18<00:00, 3067.90it/s]Packing dataset: 100%|█████████▉| 51573/51760 [00:18<00:00, 3106.63it/s]Packing dataset: 100%|██████████| 51760/51760 [00:19<00:00, 2716.89it/s]
Learning rate scheduler is initialized.
 Profiling disabled.
 Profiler config after instantiation: {'enabled': False}
  0%|          | 0/10 [00:00<?, ?it/s]/home/jenkins/xiangdong/torchtune/torchtune/modules/transformer.py:684: FutureWarning: chunked_output is deprecated and will be removed in future versions. Please use self.skip_output_layer=True and use a linear loss instead
  output = self.chunked_output(h)
/home/jenkins/xiangdong/torchtune/recipes/knowledge_distillation_single_device.py:647: FutureWarning: scale_grads is deprecated and will be removed in future versions. Please use `scale_grads_` instead.
  training.scale_grads(self._model, 1 / num_tokens)
 10%|█         | 1/10 [00:06<00:54,  6.03s/it]1|1|Loss: 1.8861790895462036:  10%|█         | 1/10 [00:06<00:54,  6.03s/it]1|1|Loss: 1.8861790895462036:  20%|██        | 2/10 [00:10<00:41,  5.23s/it]1|2|Loss: 2.013636589050293:  20%|██        | 2/10 [00:10<00:41,  5.23s/it] 1|2|Loss: 2.013636589050293:  30%|███       | 3/10 [00:15<00:34,  4.96s/it]1|3|Loss: 1.7881031036376953:  30%|███       | 3/10 [00:15<00:34,  4.96s/it]1|3|Loss: 1.7881031036376953:  40%|████      | 4/10 [00:20<00:29,  4.85s/it]1|4|Loss: 1.8720791339874268:  40%|████      | 4/10 [00:20<00:29,  4.85s/it]1|4|Loss: 1.8720791339874268:  50%|█████     | 5/10 [00:24<00:23,  4.79s/it]1|5|Loss: 1.933316946029663:  50%|█████     | 5/10 [00:24<00:23,  4.79s/it] 1|5|Loss: 1.933316946029663:  60%|██████    | 6/10 [00:29<00:18,  4.75s/it]1|6|Loss: 2.000927209854126:  60%|██████    | 6/10 [00:29<00:18,  4.75s/it]1|6|Loss: 2.000927209854126:  70%|███████   | 7/10 [00:34<00:14,  4.72s/it]1|7|Loss: 1.897608757019043:  70%|███████   | 7/10 [00:34<00:14,  4.72s/it]1|7|Loss: 1.897608757019043:  80%|████████  | 8/10 [00:38<00:09,  4.71s/it]1|8|Loss: 1.789167881011963:  80%|████████  | 8/10 [00:38<00:09,  4.71s/it]1|8|Loss: 1.789167881011963:  90%|█████████ | 9/10 [00:43<00:04,  4.69s/it]1|9|Loss: 1.6838974952697754:  90%|█████████ | 9/10 [00:43<00:04,  4.69s/it]1|9|Loss: 1.6838974952697754: 100%|██████████| 10/10 [00:48<00:00,  4.68s/it]1|10|Loss: 1.7544012069702148: 100%|██████████| 10/10 [00:48<00:00,  4.68s/it]1|10|Loss: 1.7544012069702148: 100%|██████████| 10/10 [00:48<00:00,  4.81s/it]
iteration:  3 tokens:  11612 time:  4.633323746966198 tokens_per_second_on_single_device:  2506.19
iteration:  4 tokens:  12699 time:  4.673242626013234 tokens_per_second_on_single_device:  2717.39
iteration:  5 tokens:  13246 time:  4.669710794929415 tokens_per_second_on_single_device:  2836.58
iteration:  6 tokens:  12370 time:  4.649231635965407 tokens_per_second_on_single_device:  2660.65
iteration:  7 tokens:  12510 time:  4.6573397340252995 tokens_per_second_on_single_device:  2686.08
iteration:  8 tokens:  12921 time:  4.664990480989218 tokens_per_second_on_single_device:  2769.78
iteration:  9 tokens:  12637 time:  4.662372031947598 tokens_per_second_on_single_device:  2710.42
iteration:  10 tokens:  12364 time:  4.653573393123224 tokens_per_second_on_single_device:  2656.88
avg tokens_per_second_on_single_device:  2693.2
