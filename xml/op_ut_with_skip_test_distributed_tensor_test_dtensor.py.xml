<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="1" skipped="3" tests="35" time="547.349" timestamp="2025-08-22T13:17:25.034438" hostname="dut7358"><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_async_output" time="16.746" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_constructor" time="15.744" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_new_empty_strided" time="16.743" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_properties" time="16.042" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_save_load" time="16.642" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_save_load_import" time="20.158" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_spec_hash" time="16.151" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_spec_read_only_after_set" time="14.947" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_stride" time="16.241" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local" time="16.347" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_negative_dim" time="16.344" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_then_to_local" time="16.049" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_uneven_sharding" time="15.648" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_uneven_sharding_raise_error" time="16.050" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_full_tensor_grad_hint" time="29.080" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_full_tensor_sync" time="16.852" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_meta_dtensor" time="16.350" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_modules_w_meta_dtensor" time="16.042" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_shard_tensor" time="16.049" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_shard_tensor_2d" time="16.048" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_to_local" time="16.450" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_to_local_grad_hint" time="28.882" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_auto_implicit_replication" time="16.152" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_default_value_sub_mesh" time="15.547" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_device_mesh_nd" time="4.222"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:696: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_2d_mesh" time="15.447" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_api_device_mesh_context_manager" time="4.418"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:633: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_device_mesh_device_conversion" time="16.242" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_spec_local_shard_offset" time="4.223"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:718: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_from_local_sub_mesh" time="16.052" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_implicit_replication" time="15.844" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_metadata_consistency_check" time="16.150" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_redistribute_sub_mesh" time="16.955" /><testcase classname="test.distributed.tensor.test_dtensor.TestDTensorPlacementTypes" name="test_split_tensor_1D" time="15.844" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorLogTest" name="test_dtensor_log" time="2.165"><failure message="AssertionError: '_dispatch.py' not found in '/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/device_mesh.py:518: UserWarning: It seems like you did not set/select the default device for the current process before the DeviceMesh initialization or use a launcher (i.e. torchrun) which populates `LOCAL_RANK` environment variable. It is recommended to set the current device for the process BEFORE the DeviceMesh initialization so that the underlying communicator (i.e. NCCL) can be initialized properly. Given that the current process has no default device selected, DeviceMesh will use a heuristic to set the device_id via `global_rank % num_devices_per_host`, assuming homogeneous hardware cluster. \n  warnings.warn(\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/test_dtensor.py DTensorLogTest.test_dtensor_log&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py", line 1019, in test_dtensor_log
    self.assertIn("_dispatch.py", stderr.decode("utf-8"))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: '_dispatch.py' not found in '/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/distributed/device_mesh.py:518: UserWarning: It seems like you did not set/select the default device for the current process before the DeviceMesh initialization or use a launcher (i.e. torchrun) which populates `LOCAL_RANK` environment variable. It is recommended to set the current device for the process BEFORE the DeviceMesh initialization so that the underlying communicator (i.e. NCCL) can be initialized properly. Given that the current process has no default device selected, DeviceMesh will use a heuristic to set the device_id via `global_rank % num_devices_per_host`, assuming homogeneous hardware cluster. \n  warnings.warn(\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/test_dtensor.py DTensorLogTest.test_dtensor_log

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase></testsuite></testsuites>