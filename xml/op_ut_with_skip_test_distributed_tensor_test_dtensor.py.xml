<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="2" skipped="3" tests="35" time="531.892" timestamp="2025-08-30T11:15:45.370482" hostname="dut7358"><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_async_output" time="16.626" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_constructor" time="16.122" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_new_empty_strided" time="16.724" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_properties" time="16.224" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_save_load" time="16.524" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_save_load_import" time="19.837" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_spec_hash" time="16.030" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_spec_read_only_after_set" time="16.231" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_dtensor_stride" time="16.132" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local" time="16.232" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_negative_dim" time="16.232" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_then_to_local" time="16.532" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_uneven_sharding" time="16.132" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_from_local_uneven_sharding_raise_error" time="16.231" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_full_tensor_grad_hint" time="31.757" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_full_tensor_sync" time="16.423" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_meta_dtensor" time="16.329" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_modules_w_meta_dtensor" time="16.633" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_shard_tensor" time="16.231" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_shard_tensor_2d" time="16.231" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_to_local" time="16.833" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorTest" name="test_to_local_grad_hint" time="31.557" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_auto_implicit_replication" time="16.533" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_default_value_sub_mesh" time="4.011"><failure message="AssertionError: Scalars are not equal!&#10;&#10;Expected 0 but got -6.&#10;Absolute difference: 6&#10;Relative difference: inf&#10;Expected exit code 0 but got -6 for pid: 2323170">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1056, in _check_return_codes
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not equal!

Expected 0 but got -6.
Absolute difference: 6
Relative difference: inf
Expected exit code 0 but got -6 for pid: 2323170</failure></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_device_mesh_nd" time="3.513"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:696: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_2d_mesh" time="16.132" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_api_device_mesh_context_manager" time="3.510"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:633: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_device_mesh_device_conversion" time="16.131" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_dtensor_spec_local_shard_offset" time="3.510"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_dtensor.py:718: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_from_local_sub_mesh" time="16.432" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_implicit_replication" time="16.231" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_metadata_consistency_check" time="3.910"><failure message="AssertionError: Scalars are not equal!&#10;&#10;Expected 0 but got -6.&#10;Absolute difference: 6&#10;Relative difference: inf&#10;Expected exit code 0 but got -6 for pid: 2325554">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1056, in _check_return_codes
    self.assertEqual(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not equal!

Expected 0 but got -6.
Absolute difference: 6
Relative difference: inf
Expected exit code 0 but got -6 for pid: 2325554</failure></testcase><testcase classname="test.distributed.tensor.test_dtensor.DTensorMeshTest" name="test_redistribute_sub_mesh" time="16.832" /><testcase classname="test.distributed.tensor.test_dtensor.TestDTensorPlacementTypes" name="test_split_tensor_1D" time="16.232" /><testcase classname="test.distributed.tensor.test_dtensor.DTensorLogTest" name="test_dtensor_log" time="2.770" /></testsuite></testsuites>