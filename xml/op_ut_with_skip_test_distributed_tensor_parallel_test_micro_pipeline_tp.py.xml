<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="28" skipped="0" tests="45" time="65.109" timestamp="2025-09-19T13:06:47.667296+00:00" hostname="dut7358"><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_dtensor_seq_par_shard_dim_0" time="5.911"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'0_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpb4146qqg/sb/csbuofcmzwnubnipmbbfwf5a2iyb3cuftzt26zlatgclmmihvwct.py\n# Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   input_tensor_2 =&gt; relu\n# Graph fragment:\n#   %mm : Tensor &quot;f32[16, 8][8, 1]xpu:0&quot; = PlaceHolder[target=mm]\n#   %relu : Tensor &quot;f32[16, 8][8, 1]xpu:0&quot;[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n#   return %relu\ntriton_poi_fused_relu_0 = async_compile.triton(\'triton_poi_fused_relu_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 128}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_relu_0\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 1536}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (8, 10), (10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((16, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf3)\n            del primals_2\n            buf4 = buf3; del buf3  # reuse\n            # Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_relu_0.run(buf4, 128, stream=stream0)\n            buf5 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf4, reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf5)\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf6 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf5, \'sum\', 2, \'0\')\n            assert_size_stride(buf6, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf6, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf6)\n            del buf5\n        return (buf6, primals_3, buf0, buf4, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 495, in test_dtensor_seq_par
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'0_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpb4146qqg/sb/csbuofcmzwnubnipmbbfwf5a2iyb3cuftzt26zlatgclmmihvwct.py\n# Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n# Source node to ATen node mapping:\n#   input_tensor_2 =&gt; relu\n# Graph fragment:\n#   %mm : Tensor "f32[16, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[16, 8][8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%mm,), kwargs = {})\n#   return %relu\ntriton_poi_fused_relu_0 = async_compile.triton(\'triton_poi_fused_relu_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 128}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_relu_0\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 1536}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_relu_0(in_out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (8, 10), (10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((16, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf3)\n            del primals_2\n            buf4 = buf3; del buf3  # reuse\n            # Topologically Sorted Source Nodes: [input_tensor_2], Original ATen: [aten.relu]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_relu_0.run(buf4, 128, stream=stream0)\n            buf5 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf4, reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf5)\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf6 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf5, \'sum\', 2, \'0\')\n            assert_size_stride(buf6, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf6, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf6)\n            del buf5\n        return (buf6, primals_3, buf0, buf4, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_dtensor_seq_par_shard_dim_1" time="3.600"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'1_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpm_8r7w82/ac/caczbg4u2wt2qjfqt2qvlu66klo44cpxqw6qqd7rsnbr2c6c72k5.py\n# Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   input_tensor_1 =&gt; cat, split\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor &quot;f32[2, 16, 10][160, 10, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 2560}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = ((xindex // 10) % 16)\n    x0 = (xindex % 10)\n    x2 = xindex // 160\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 8, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 10*(x1) + 80*x2), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 16, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (160 + x0 + 10*((-8) + x1) + 80*x2), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bv/cbvspzcwnfjrment6kwdueb4bulxnusse73xap6o5lkenhkbfjzq.py\n# Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   input_tensor_2 =&gt; relu\n#   linear =&gt; view_2\n# Graph fragment:\n#   %mm : Tensor &quot;f32[32, 8][8, 1]xpu:0&quot; = PlaceHolder[target=mm]\n#   %relu : Tensor &quot;f32[2, 16, 8][128, 8, 1]xpu:0&quot; = PlaceHolder[target=relu]\n#   %view_2 : Tensor &quot;f32[2, 16, 8][128, 8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 16, 8]), kwargs = {})\n#   %relu : Tensor &quot;f32[2, 16, 8][128, 8, 1]xpu:0&quot;[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_2,), kwargs = {})\n#   %le : Tensor &quot;b8[2, 16, 8][128, 8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})\n#   return %relu,%le\ntriton_poi_fused__unsafe_view_relu_threshold_backward_1 = async_compile.triton(\'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 256}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'out_ptr0\': \'*i1\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 3584}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_relu_threshold_backward_1(in_out_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp3 = 0.0\n    tmp4 = tmp2 &lt;= tmp3\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n    tl.store(out_ptr0 + (x0), tmp4, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bs/cbs4y5r2kem5rmfzuaeqyjqyrggdhgyzewjlahbtmrqvgeoayy3w.py\n# Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   linear_1 =&gt; view_6\n#   outputs_1 =&gt; cat_1, reduce_scatter_tensor, split_1\n# Graph fragment:\n#   %mm_1 : Tensor &quot;f32[32, 10][10, 1]xpu:0&quot; = PlaceHolder[target=mm_1]\n#   %view_6 : Tensor &quot;f32[2, 16, 10][160, 10, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm_1, [2, 16, 10]), kwargs = {})\n#   %split_1 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_6, 8, 1), kwargs = {})\n#   %cat_1 : Tensor &quot;f32[4, 8, 10][80, 10, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_2, %getitem_3],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor &quot;f32[2, 8, 10][80, 10, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat_1, sum, 2, 0), kwargs = {})\n#   return %buf7\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 5120}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 80\n    x0 = (xindex % 80)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 160*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (80 + x0 + 160*((-2) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (2, 8, 10), (80, 10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((2, 16, 10), (160, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 320, stream=stream0)\n            buf4 = empty_strided_xpu((32, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1, linear], Original ATen: [aten.split, aten.cat, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf4)\n            del primals_2\n            buf5 = reinterpret_tensor(buf4, (2, 16, 8), (128, 8, 1), 0); del buf4  # reuse\n            buf11 = empty_strided_xpu((2, 16, 8), (128, 8, 1), torch.bool)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_relu_threshold_backward_1.run(buf5, buf11, 256, stream=stream0)\n            buf6 = empty_strided_xpu((32, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2, linear_1], Original ATen: [aten._unsafe_view, aten.relu, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf5, (32, 8), (8, 1), 0), reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf6)\n            buf7 = empty_strided_xpu((4, 8, 10), (80, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2.run(buf6, buf7, 320, stream=stream0)\n            del buf6\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf8 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf7, \'sum\', 2, \'0\')\n            assert_size_stride(buf8, (2, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf8, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf8)\n            del buf7\n        return (buf8, primals_3, reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(buf5, (32, 8), (8, 1), 0), buf11, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((2, 8, 10), (80, 10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 495, in test_dtensor_seq_par
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'1_forward\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpm_8r7w82/ac/caczbg4u2wt2qjfqt2qvlu66klo44cpxqw6qqd7rsnbr2c6c72k5.py\n# Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   input_tensor_1 =&gt; cat, split\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 2560}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = ((xindex // 10) % 16)\n    x0 = (xindex % 10)\n    x2 = xindex // 160\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 8, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 10*(x1) + 80*x2), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 16, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (160 + x0 + 10*((-8) + x1) + 80*x2), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bv/cbvspzcwnfjrment6kwdueb4bulxnusse73xap6o5lkenhkbfjzq.py\n# Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n# Source node to ATen node mapping:\n#   input_tensor_2 =&gt; relu\n#   linear =&gt; view_2\n# Graph fragment:\n#   %mm : Tensor "f32[32, 8][8, 1]xpu:0" = PlaceHolder[target=mm]\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0" = PlaceHolder[target=relu]\n#   %view_2 : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 16, 8]), kwargs = {})\n#   %relu : Tensor "f32[2, 16, 8][128, 8, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.relu.default](args = (%view_2,), kwargs = {})\n#   %le : Tensor "b8[2, 16, 8][128, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.le.Scalar](args = (%relu, 0), kwargs = {})\n#   return %relu,%le\ntriton_poi_fused__unsafe_view_relu_threshold_backward_1 = async_compile.triton(\'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 256}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_out_ptr0\': \'*fp32\', \'out_ptr0\': \'*i1\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_relu_threshold_backward_1\', \'mutated_arg_names\': [\'in_out_ptr0\'], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 1, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 3584}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_relu_threshold_backward_1(in_out_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 256\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_out_ptr0 + (x0), xmask)\n    tmp1 = tl.full([1], 0, tl.int32)\n    tmp2 = triton_helpers.maximum(tmp1, tmp0)\n    tmp3 = 0.0\n    tmp4 = tmp2 &lt;= tmp3\n    tl.store(in_out_ptr0 + (x0), tmp2, xmask)\n    tl.store(out_ptr0 + (x0), tmp4, xmask)\n\'\'\', device_str=\'xpu\')\n\n\n# kernel path: /tmp/tmpm_8r7w82/bs/cbs4y5r2kem5rmfzuaeqyjqyrggdhgyzewjlahbtmrqvgeoayy3w.py\n# Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   linear_1 =&gt; view_6\n#   outputs_1 =&gt; cat_1, reduce_scatter_tensor, split_1\n# Graph fragment:\n#   %mm_1 : Tensor "f32[32, 10][10, 1]xpu:0" = PlaceHolder[target=mm_1]\n#   %view_6 : Tensor "f32[2, 16, 10][160, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm_1, [2, 16, 10]), kwargs = {})\n#   %split_1 : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_6, 8, 1), kwargs = {})\n#   %cat_1 : Tensor "f32[4, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_2, %getitem_3],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 8, 10][80, 10, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat_1, sum, 2, 0), kwargs = {})\n#   return %buf7\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 512}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2\', \'mutated_arg_names\': [], \'optimize_mem\': False, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 5120}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 320\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 80\n    x0 = (xindex % 80)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 160*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (80 + x0 + 160*((-2) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        primals_1, primals_2, primals_3 = args\n        args.clear()\n        assert_size_stride(primals_1, (2, 8, 10), (80, 10, 1))\n        assert_size_stride(primals_2, (8, 10), (10, 1))\n        assert_size_stride(primals_3, (10, 8), (8, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(primals_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del primals_1\n            buf3 = empty_strided_xpu((2, 16, 10), (160, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 320, stream=stream0)\n            buf4 = empty_strided_xpu((32, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [input_tensor_1, linear], Original ATen: [aten.split, aten.cat, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(primals_2, (10, 8), (1, 10), 0), out=buf4)\n            del primals_2\n            buf5 = reinterpret_tensor(buf4, (2, 16, 8), (128, 8, 1), 0); del buf4  # reuse\n            buf11 = empty_strided_xpu((2, 16, 8), (128, 8, 1), torch.bool)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2], Original ATen: [aten._unsafe_view, aten.relu, aten.threshold_backward]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_relu_threshold_backward_1.run(buf5, buf11, 256, stream=stream0)\n            buf6 = empty_strided_xpu((32, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear, input_tensor_2, linear_1], Original ATen: [aten._unsafe_view, aten.relu, aten.t, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf5, (32, 8), (8, 1), 0), reinterpret_tensor(primals_3, (8, 10), (1, 8), 0), out=buf6)\n            buf7 = empty_strided_xpu((4, 8, 10), (80, 10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_2.run(buf6, buf7, 320, stream=stream0)\n            del buf6\n            # Topologically Sorted Source Nodes: [linear_1, outputs_1], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf8 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf7, \'sum\', 2, \'0\')\n            assert_size_stride(buf8, (2, 8, 10), (80, 10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf8, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [outputs_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf8)\n            del buf7\n        return (buf8, primals_3, reinterpret_tensor(buf3, (32, 10), (10, 1), 0), reinterpret_tensor(buf5, (32, 8), (8, 1), 0), buf11, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    primals_1 = rand_strided((2, 8, 10), (80, 10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_2 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    primals_3 = rand_strided((10, 8), (8, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([primals_1, primals_2, primals_3])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_dtensor_seq_par_shard_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_find_all_gather_patterns" time="0.019" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_find_reduce_scatter_patterns" time="0.010" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False" time="0.122"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'2_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'2_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True" time="0.108"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'3_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'3_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (32, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (64, 32), (32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(buf0, arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, buf3, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((32, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_1_return_A_False" time="3.112" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_1_return_A_True" time="3.057" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_2_return_A_False" time="0.002" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_2_gather_dim_2_return_A_True" time="0.001" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False" time="0.120"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'6_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'6_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n            del buf0\n        return (reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True" time="0.118"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'7_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'7_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (1, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (2, 64, 32), (2048, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf0, (128, 32), (32, 1), 0), arg1_1, out=buf3)\n            del arg1_1\n        return (buf0, reinterpret_tensor(buf3, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((1, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False" time="3.099"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'8_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpiuoj7_cr/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   res_1 =&gt; cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor &quot;f32[2, 64, 32][2048, 32, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1, matmul], Original ATen: [aten.split, aten.cat, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n            del buf3\n        return (reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'8_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpiuoj7_cr/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   res_1 =&gt; cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1, matmul], Original ATen: [aten.split, aten.cat, aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n            del buf3\n        return (reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True" time="3.255"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'9_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmp_hgb2xki/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   res_1 =&gt; cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor &quot;f32[2, 64, 32][2048, 32, 1]xpu:0&quot;[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n        return (buf3, reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 243, in test_fuse_all_gather_matmul
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'9_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmp_hgb2xki/ft/cftxcsdzm4bpj677pjxw5i5exdbfmubswxc3254jbz4ujd2esojf.py\n# Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   res_1 =&gt; cat\n# Graph fragment:\n#   %buf2 : Tensor  = PlaceHolder[target=buf2]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n#   %cat : Tensor "f32[2, 64, 32][2048, 32, 1]xpu:0"[num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1], 1), kwargs = {})\n#   return %cat\ntriton_poi_fused_cat_split_0 = async_compile.triton(\'triton_poi_fused_cat_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 4096}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 4096\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = tl.full([XBLOCK], True, tl.int1)\n    x1 = ((xindex // 32) % 64)\n    x0 = (xindex % 32)\n    x2 = xindex // 2048\n    x3 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 32, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 32*(x1) + 1024*x2), tmp4, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 64, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (2048 + x0 + 32*((-32) + x1) + 1024*x2), tmp6, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, None)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 32, 32), (1024, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'0\')\n            assert_size_stride(buf0, (4, 32, 32), (1024, 32, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((2, 64, 32), (2048, 32, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, res_1], Original ATen: [aten.split, aten.cat]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_split_0.run(buf0, buf3, 4096, stream=stream0)\n            del buf0\n            buf4 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(buf3, (128, 32), (32, 1), 0), arg1_1, out=buf4)\n            del arg1_1\n        return (buf3, reinterpret_tensor(buf4, (2, 64, 16), (1024, 16, 1), 0), )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 32, 32), (1024, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_2_return_A_False" time="3.081" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_matmul_A_dims_3_gather_dim_2_return_A_True" time="3.122" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False" time="0.011"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (None, _scaled_mm)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (None, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True" time="0.008"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (wait_tensor, _scaled_mm)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%wait_tensor, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    return (wait_tensor, _scaled_mm)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_False" time="3.139" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_True" time="3.118" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_2_return_A_False" time="0.002" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_2_return_A_True" time="0.001" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False" time="0.012"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_2)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True" time="0.009"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (wait_tensor, view_2)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=2] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%wait_tensor, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (wait_tensor, view_2)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_0_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False" time="0.014"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_4)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (None, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_False

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True" time="0.014"><failure message="AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (view_2, view_4)'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 303, in test_fuse_all_gather_scaled_matmul
    self.assertIn("fused_all_gather_scaled_matmul", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n    %a_shard_1 : [num_users=1] = placeholder[target=A_shard_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %all_gather_into_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_gather_into_tensor.default](args = (%a_shard_1, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_gather_into_tensor,), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%wait_tensor, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %view : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem, torch.uint8), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.dtype](args = (%getitem_1, torch.uint8), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%view, %view_1], 1), kwargs = {})\n    %view_2 : [num_users=2] = call_function[target=torch.ops.aten.view.dtype](args = (%cat, torch.float8_e4m3fn), kwargs = {})\n    %view_3 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%view_2, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view_3, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_4 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, -1]), kwargs = {})\n    return (view_2, view_4)'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_1_return_A_True

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_False" time="3.140" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_True" time="3.143" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0" time="0.114"><failure message="AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'16_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf0, \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (32, 16), (16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'16_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf0, \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (32, 16), (16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1" time="3.133"><failure message="AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'17_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpzqx15axw/7h/c7hbjgjdiuc76eklnwumvpmy2uq6zbv5vugz7ce34uo6iuuyznpn.py\n# Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor &quot;f32[64, 16][16, 1]xpu:0&quot; = PlaceHolder[target=mm]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%mm, 8, 1), kwargs = {})\n#   %cat : Tensor &quot;f32[128, 8][8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor &quot;f32[64, 8][8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 1024}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 16384}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 1024\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 8\n    x0 = (xindex % 8)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 64, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 128, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*((-64) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((128, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 1024, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (64, 8), (8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'17_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpzqx15axw/7h/c7hbjgjdiuc76eklnwumvpmy2uq6zbv5vugz7ce34uo6iuuyznpn.py\n# Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[64, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%mm, 8, 1), kwargs = {})\n#   %cat : Tensor "f32[128, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[64, 8][8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 1024}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 16384}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 1024\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 8\n    x0 = (xindex % 8)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 64, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 128, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*((-64) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (64, 32), (32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((64, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.mm]\n            extern_kernels.mm(arg0_1, arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((128, 8), (8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 1024, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [chunk, self, tensor], Original ATen: [aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (64, 8), (8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((64, 32), (32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_2" time="0.001" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0" time="0.118"><failure message="AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'18_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [matmul, tensor], Original ATen: [aten._unsafe_view, _c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(reinterpret_tensor(buf0, (2, 64, 16), (1024, 16, 1), 0), \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (1, 64, 16), (1024, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'18_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            # Topologically Sorted Source Nodes: [matmul, tensor], Original ATen: [aten._unsafe_view, _c10d_functional.reduce_scatter_tensor]\n            buf1 = torch.ops._c10d_functional.reduce_scatter_tensor.default(reinterpret_tensor(buf0, (2, 64, 16), (1024, 16, 1), 0), \'avg\', 2, \'0\')\n            assert_size_stride(buf1, (1, 64, 16), (1024, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf1, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf1)\n            del buf0\n        return (buf1, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1" time="3.090"><failure message="AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'19_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpn__v6j38/ky/cky76qe6ulvhwz75qtbhi5mpj36h7hkyugmjzpgygruigs6pofyj.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   matmul =&gt; view_1\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor &quot;f32[128, 16][16, 1]xpu:0&quot; = PlaceHolder[target=mm]\n#   %view_1 : Tensor &quot;f32[2, 64, 16][1024, 16, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n#   %cat : Tensor &quot;f32[4, 32, 16][512, 16, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor &quot;f32[2, 32, 16][512, 16, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 512\n    x0 = (xindex % 512)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 1024*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (512 + x0 + 1024*((-2) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 32, 16), (512, 16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 32, 16), (512, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'19_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpn__v6j38/ky/cky76qe6ulvhwz75qtbhi5mpj36h7hkyugmjzpgygruigs6pofyj.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   matmul =&gt; view_1\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n#   %cat : Tensor "f32[4, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 32, 16][512, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x1 = xindex // 512\n    x0 = (xindex % 512)\n    x2 = xindex\n    tmp0 = x1\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 1024*(x1)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (512 + x0 + 1024*((-2) + x1)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x2), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 32, 16), (512, 16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 32, 16), (512, 16, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2" time="3.094"><failure message="AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'20_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpd8le4hwt/po/cpouspfi54hul3qalnekjvjux2rmijzvciqupffes76iygzetxkq.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   matmul =&gt; view_1\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor &quot;f32[128, 16][16, 1]xpu:0&quot; = PlaceHolder[target=mm]\n#   %view_1 : Tensor &quot;f32[2, 64, 16][1024, 16, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n#   %cat : Tensor &quot;f32[4, 64, 8][512, 8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor &quot;f32[2, 64, 8][512, 8, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x2 = xindex // 512\n    x0 = (xindex % 8)\n    x1 = ((xindex // 8) % 64)\n    x3 = xindex\n    tmp0 = x2\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*x1 + 1024*(x2)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*x1 + 1024*((-2) + x2)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 64, 8), (512, 8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 64, 8), (512, 8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 348, in test_fuse_matmul_reduce_scatter
    self.assertIn("fused_matmul_reduce_scatter", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_matmul_reduce_scatter' not found in '# AOT ID: [\'20_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpd8le4hwt/po/cpouspfi54hul3qalnekjvjux2rmijzvciqupffes76iygzetxkq.py\n# Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   chunk =&gt; split\n#   matmul =&gt; view_1\n#   self =&gt; cat\n#   tensor =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %mm : Tensor "f32[128, 16][16, 1]xpu:0" = PlaceHolder[target=mm]\n#   %view_1 : Tensor "f32[2, 64, 16][1024, 16, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%mm, [2, 64, 16]), kwargs = {})\n#   %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n#   %cat : Tensor "f32[4, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[2, 64, 8][512, 8, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n#   return %buf1\ntriton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0 = async_compile.triton(\'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.pointwise(\n    size_hints={\'x\': 2048}, \n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr0\': \'*fp32\', \'xnumel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (2,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': False, \'num_load\': 2, \'num_reduction\': 0, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'x\': 32768}},\n    min_elem_per_thread=0\n)\n@triton.jit\ndef triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 2048\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex &lt; xnumel\n    x2 = xindex // 512\n    x0 = (xindex % 8)\n    x1 = ((xindex // 8) % 64)\n    x3 = xindex\n    tmp0 = x2\n    tmp1 = tl.full([1], 0, tl.int64)\n    tmp2 = tmp0 &gt;= tmp1\n    tmp3 = tl.full([1], 2, tl.int64)\n    tmp4 = tmp0 &lt; tmp3\n    tmp5 = tl.load(in_ptr0 + (x0 + 16*x1 + 1024*(x2)), tmp4 &amp; xmask, other=0.0)\n    tmp6 = tmp0 &gt;= tmp3\n    tmp7 = tl.full([1], 4, tl.int64)\n    tmp8 = tmp0 &lt; tmp7\n    tmp9 = tl.load(in_ptr0 + (8 + x0 + 16*x1 + 1024*((-2) + x2)), tmp6 &amp; xmask, other=0.0)\n    tmp10 = tl.where(tmp4, tmp5, tmp9)\n    tl.store(out_ptr0 + (x3), tmp10, xmask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (2, 64, 32), (2048, 32, 1))\n        assert_size_stride(arg1_1, (32, 16), (16, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            buf0 = empty_strided_xpu((128, 16), (16, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.view, aten.mm]\n            extern_kernels.mm(reinterpret_tensor(arg0_1, (128, 32), (32, 1), 0), arg1_1, out=buf0)\n            del arg0_1\n            del arg1_1\n            buf1 = empty_strided_xpu((4, 64, 8), (512, 8, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_poi_fused__unsafe_view_cat_reduce_scatter_tensor_split_0.run(buf0, buf1, 2048, stream=stream0)\n            del buf0\n            # Topologically Sorted Source Nodes: [matmul, chunk, self, tensor], Original ATen: [aten._unsafe_view, aten.split, aten.cat, _c10d_functional.reduce_scatter_tensor]\n            buf2 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf1, \'avg\', 2, \'0\')\n            assert_size_stride(buf2, (2, 64, 8), (512, 8, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf2, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf2)\n            del buf1\n        return (buf2, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((2, 64, 32), (2048, 32, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((32, 16), (16, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0" time="0.009"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%_scaled_mm, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%_scaled_mm, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1" time="0.009"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%_scaled_mm, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%a_1, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%_scaled_mm, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_2" time="0.001" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0" time="0.008"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_1, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_1, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1" time="0.010"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 32, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2" time="0.010"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 391, in test_fuse_scaled_matmul_reduce_scatter
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [128, 32]), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %a_scale_1, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 64, 16]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_1, 8, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, avg, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0" time="0.012"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_2, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%view_2, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1" time="0.011"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 8, 1), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2" time="0.011"><failure message="AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 32, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 552, in instantiated_test
    test(self, **param_kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 1955, in wrap_fn
    return fn(self, *args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 455, in test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape
    self.assertIn("fused_scaled_matmul_reduce_scatter", str(gm.graph))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_scaled_matmul_reduce_scatter' not found in 'graph():\n    %a_1 : [num_users=1] = placeholder[target=A_1]\n    %b_1 : [num_users=1] = placeholder[target=B_1]\n    %a_scale_1 : [num_users=1] = placeholder[target=A_scale_1]\n    %b_scale_1 : [num_users=1] = placeholder[target=B_scale_1]\n    %out_dtype_1 : [num_users=0] = placeholder[target=out_dtype_1]\n    %view : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_1, [-1, 32]), kwargs = {})\n    %view_1 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%a_scale_1, [-1, 1]), kwargs = {})\n    %reciprocal : [num_users=1] = call_function[target=torch.ops.aten.reciprocal.default](args = (%view_1,), kwargs = {})\n    %_scaled_mm : [num_users=1] = call_function[target=torch.ops.aten._scaled_mm.default](args = (%view, %b_1, %reciprocal, %b_scale_1, None, None, torch.bfloat16), kwargs = {})\n    %view_2 : [num_users=1] = call_function[target=torch.ops.aten.reshape.default](args = (%_scaled_mm, [2, 16, 64]), kwargs = {})\n    %split : [num_users=2] = call_function[target=torch.ops.aten.split.Tensor](args = (%view_2, 32, 2), kwargs = {})\n    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%split, 0), kwargs = {})\n    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%split, 1), kwargs = {})\n    %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_1],), kwargs = {})\n    %reduce_scatter_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%cat, sum, 2, 0), kwargs = {})\n    %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%reduce_scatter_tensor,), kwargs = {})\n    return wait_tensor'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTPTest.test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTPTest" name="test_get_unexposed_collectives" time="0.025" /><testcase classname="test.distributed.tensor.parallel.test_micro_pipeline_tp.MicroPipelineTP4GPUTest" name="test_extra_collectives" time="2.974"><failure message="AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'21_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpqsiv8knn/n2/cn27kropqbqilnc2cqdal3tcjzwjjxrajr7j74lc5qm2en3rmmhi.py\n# Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   full_hidden =&gt; div\n#   pow_1 =&gt; pow_1\n#   sqrt =&gt; sqrt\n#   sum_1 =&gt; sum_1\n#   tensor_2 =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %buf6 : Tensor  = PlaceHolder[target=buf6]\n#   %sum_1 : Tensor &quot;f32[][]xpu:0&quot; = PlaceHolder[target=sum_1]\n#   %pow_1 : Tensor &quot;f32[32, 7][7, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%wait_tensor_1, 2), kwargs = {})\n#   %sum_1 : Tensor &quot;f32[][]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%pow_1,), kwargs = {})\n#   %sqrt : Tensor &quot;f32[][]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})\n#   %div : Tensor &quot;f32[32, 7][7, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%wait_tensor_1, %sqrt), kwargs = {})\n#   %reduce_scatter_tensor : Tensor &quot;f32[16, 7][7, 1]xpu:0&quot;[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%div, avg, 2, 3), kwargs = {})\n#   return %sum_1,%buf8\ntriton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0 = async_compile.triton(\'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.persistent_reduction(\n    size_hints={\'x\': 1, \'r0_\': 256},\n    reduction_hint=ReductionHint.INNER,\n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr1\': \'*fp32\', \'xnumel\': \'constexpr\', \'r0_numel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {\'xnumel\': 1}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (3,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': None, \'num_load\': 1, \'num_reduction\': 1, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'r0_\': 1792}}\n)\n@triton.jit\ndef triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 224\n    R0_BLOCK: tl.constexpr = 256\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index &lt; r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tmp0 * tmp0\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n    tmp4 = tl.where(r0_mask, tmp2, 0)\n    tmp5 = tl.sum(tmp4, 1)[:, None].to(tl.float32)\n    tmp6 = libdevice.sqrt(tmp5)\n    tmp7 = (tmp0 / tmp6)\n    tl.store(out_ptr1 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp7, r0_mask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1, arg2_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (8, 10), (10, 1))\n        assert_size_stride(arg1_1, (7, 10), (10, 1))\n        assert_size_stride(arg2_1, (10, 7), (7, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'1\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((16, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t, hidden], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(arg1_1, (10, 7), (1, 10), 0), out=buf3)\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf4 = torch.ops._c10d_functional.all_gather_into_tensor.default(buf3, 2, \'3\')\n            assert_size_stride(buf4, (32, 7), (7, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf4, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [res_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf4)\n            del buf3\n            buf8 = empty_strided_xpu((32, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0.run(buf4, buf8, 1, 224, stream=stream0)\n            # Topologically Sorted Source Nodes: [sqrt, full_hidden, tensor_2], Original ATen: [aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            buf9 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf8, \'avg\', 2, \'3\')\n            assert_size_stride(buf9, (16, 7), (7, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf9, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf4\n            # Topologically Sorted Source Nodes: [res_2], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf9)\n            del buf8\n            buf12 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t_1, matmul_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf9, reinterpret_tensor(arg2_1, (7, 10), (1, 7), 0), out=buf12)\n            del arg2_1\n            # Topologically Sorted Source Nodes: [tensor_3], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf13 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf12, \'avg\', 2, \'1\')\n            assert_size_stride(buf13, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf13, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf9\n            # Topologically Sorted Source Nodes: [res_3], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf13)\n            del buf12\n        return (buf13, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((7, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg2_1 = rand_strided((10, 7), (7, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1, arg2_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == &quot;__main__&quot;:\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTP4GPUTest.test_extra_collectives&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/parallel/test_micro_pipeline_tp.py", line 547, in test_extra_collectives
    self.assertIn("fused_all_gather_matmul", code)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 1112, in assertIn
    self.fail(self._formatMessage(msg, standardMsg))
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 675, in fail
    raise self.failureException(msg)
AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\'21_inference\']\nfrom ctypes import c_void_p, c_long, c_int\nimport torch\nimport math\nimport random\nimport os\nimport tempfile\nfrom math import inf, nan\nfrom cmath import nanj\nfrom torch._inductor.hooks import run_intermediate_hooks\nfrom torch._inductor.utils import maybe_profile\nfrom torch._inductor.codegen.memory_planning import _align as align\nfrom torch import device, empty_strided\nfrom torch._inductor.async_compile import AsyncCompile\nfrom torch._inductor.select_algorithm import extern_kernels\nimport triton\nimport triton.language as tl\nfrom torch._inductor.runtime.triton_heuristics import start_graph, end_graph\nfrom torch._C import _xpu_getCurrentRawStream as get_raw_stream\n\naten = torch.ops.aten\ninductor_ops = torch.ops.inductor\n_quantized = torch.ops._quantized\nassert_size_stride = torch._C._dynamo.guards.assert_size_stride\nassert_alignment = torch._C._dynamo.guards.assert_alignment\nempty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\nempty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nempty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\nempty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\nreinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\nalloc_from_pool = torch.ops.inductor._alloc_from_pool\nasync_compile = AsyncCompile()\nempty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n\n\n# kernel path: /tmp/tmpqsiv8knn/n2/cn27kropqbqilnc2cqdal3tcjzwjjxrajr7j74lc5qm2en3rmmhi.py\n# Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n# Source node to ATen node mapping:\n#   full_hidden =&gt; div\n#   pow_1 =&gt; pow_1\n#   sqrt =&gt; sqrt\n#   sum_1 =&gt; sum_1\n#   tensor_2 =&gt; reduce_scatter_tensor\n# Graph fragment:\n#   %buf6 : Tensor  = PlaceHolder[target=buf6]\n#   %sum_1 : Tensor "f32[][]xpu:0" = PlaceHolder[target=sum_1]\n#   %pow_1 : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.pow.Tensor_Scalar](args = (%wait_tensor_1, 2), kwargs = {})\n#   %sum_1 : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sum.default](args = (%pow_1,), kwargs = {})\n#   %sqrt : Tensor "f32[][]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.sqrt.default](args = (%sum_1,), kwargs = {})\n#   %div : Tensor "f32[32, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%wait_tensor_1, %sqrt), kwargs = {})\n#   %reduce_scatter_tensor : Tensor "f32[16, 7][7, 1]xpu:0"[num_users=1] = call_function[target=torch.ops._c10d_functional.reduce_scatter_tensor.default](args = (%div, avg, 2, 3), kwargs = {})\n#   return %sum_1,%buf8\ntriton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0 = async_compile.triton(\'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'\'\'\nimport triton\nimport triton.language as tl\n\nfrom torch._inductor.runtime import triton_helpers, triton_heuristics\nfrom torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\nfrom torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\ntriton_helpers.set_driver_to_gpu()\n\n@triton_heuristics.persistent_reduction(\n    size_hints={\'x\': 1, \'r0_\': 256},\n    reduction_hint=ReductionHint.INNER,\n    filename=__file__,\n    triton_meta={\'signature\': {\'in_ptr0\': \'*fp32\', \'out_ptr1\': \'*fp32\', \'xnumel\': \'constexpr\', \'r0_numel\': \'i32\', \'XBLOCK\': \'constexpr\'}, \'device\': DeviceProperties(type=\'xpu\', index=0, multi_processor_count=56, cc={\'architecture\': 13136561920, \'device_id\': 3034, \'driver_version\': \'1.6.33578+15\', \'gpu_eu_count\': 448, \'gpu_subslice_count\': 56, \'has_atomic64\': True, \'has_bfloat16_conversions\': True, \'has_fp16\': True, \'has_fp64\': True, \'has_subgroup_2d_block_io\': True, \'has_subgroup_matrix_multiply_accumulate\': True, \'has_subgroup_matrix_multiply_accumulate_tensor_float32\': False, \'max_compute_units\': 448, \'max_num_sub_groups\': 64, \'max_work_group_size\': 1024, \'name\': \'Intel(R) Data Center GPU Max 1100\', \'platform_name\': \'Intel(R) oneAPI Unified Runtime over Level-Zero\', \'sub_group_sizes\': [16, 32], \'total_memory\': 51522830336, \'type\': \'gpu\', \'vendor\': \'Intel(R) Corporation\', \'version\': \'12.60.7\'}, major=None, regs_per_multiprocessor=None, max_threads_per_multi_processor=None, warp_size=32), \'constants\': {\'xnumel\': 1}, \'configs\': [{(0,): [[\'tt.divisibility\', 16]], (1,): [[\'tt.divisibility\', 16]], (3,): [[\'tt.divisibility\', 16]]}]},\n    inductor_meta={\'grid_type\': \'Grid1D\', \'autotune_hints\': set(), \'kernel_name\': \'triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0\', \'mutated_arg_names\': [], \'optimize_mem\': True, \'no_x_dim\': None, \'num_load\': 1, \'num_reduction\': 1, \'backend_hash\': \'512C1FF87ACE660C3D0B1289C32D3FA06092B9DB45933B3F9A48110AED8B8634\', \'are_deterministic_algorithms_enabled\': False, \'assert_indirect_indexing\': True, \'autotune_local_cache\': True, \'autotune_pointwise\': True, \'autotune_remote_cache\': None, \'force_disable_caches\': False, \'dynamic_scale_rblock\': True, \'max_autotune\': False, \'max_autotune_pointwise\': False, \'min_split_scan_rblock\': 256, \'spill_threshold\': 16, \'store_cubin\': False, \'tiling_scores\': {\'r0_\': 1792}}\n)\n@triton.jit\ndef triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0(in_ptr0, out_ptr1, xnumel, r0_numel, XBLOCK : tl.constexpr):\n    xnumel = 1\n    r0_numel = 224\n    R0_BLOCK: tl.constexpr = 256\n    rnumel = r0_numel\n    RBLOCK: tl.constexpr = R0_BLOCK\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = tl.full([XBLOCK, R0_BLOCK], True, tl.int1)\n    r0_index = tl.arange(0, R0_BLOCK)[None, :]\n    r0_offset = 0\n    r0_mask = r0_index &lt; r0_numel\n    roffset = r0_offset\n    rindex = r0_index\n    r0_0 = r0_index\n    tmp0 = tl.load(in_ptr0 + (r0_0), r0_mask, other=0.0)\n    tmp1 = tmp0 * tmp0\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, R0_BLOCK])\n    tmp4 = tl.where(r0_mask, tmp2, 0)\n    tmp5 = tl.sum(tmp4, 1)[:, None].to(tl.float32)\n    tmp6 = libdevice.sqrt(tmp5)\n    tmp7 = (tmp0 / tmp6)\n    tl.store(out_ptr1 + (tl.broadcast_to(r0_0, [XBLOCK, R0_BLOCK])), tmp7, r0_mask)\n\'\'\', device_str=\'xpu\')\n\n\nasync_compile.wait(globals())\ndel async_compile\n\nclass Runner:\n    def __init__(self, partitions):\n        self.partitions = partitions\n\n    def recursively_apply_fns(self, fns):\n        new_callables = []\n        for fn, c in zip(fns, self.partitions):\n            new_callables.append(fn(c))\n        self.partitions = new_callables\n\n    def call(self, args):\n        arg0_1, arg1_1, arg2_1 = args\n        args.clear()\n        assert_size_stride(arg0_1, (8, 10), (10, 1))\n        assert_size_stride(arg1_1, (7, 10), (10, 1))\n        assert_size_stride(arg2_1, (10, 7), (7, 1))\n        with torch.xpu._DeviceGuard(0):\n            torch.xpu.set_device(0)\n            # Topologically Sorted Source Nodes: [tensor], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf0 = torch.ops._c10d_functional.all_gather_into_tensor.default(arg0_1, 2, \'1\')\n            assert_size_stride(buf0, (16, 10), (10, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf0, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            # Topologically Sorted Source Nodes: [res], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf0)\n            del arg0_1\n            buf3 = empty_strided_xpu((16, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t, hidden], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf0, reinterpret_tensor(arg1_1, (10, 7), (1, 10), 0), out=buf3)\n            del arg1_1\n            # Topologically Sorted Source Nodes: [tensor_1], Original ATen: [_c10d_functional.all_gather_into_tensor]\n            buf4 = torch.ops._c10d_functional.all_gather_into_tensor.default(buf3, 2, \'3\')\n            assert_size_stride(buf4, (32, 7), (7, 1), \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            assert_alignment(buf4, 16, \'torch.ops._c10d_functional.all_gather_into_tensor.default\')\n            del buf0\n            # Topologically Sorted Source Nodes: [res_1], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf4)\n            del buf3\n            buf8 = empty_strided_xpu((32, 7), (7, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [pow_1, sum_1, sqrt, full_hidden, tensor_2], Original ATen: [aten.pow, aten.sum, aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            stream0 = get_raw_stream(0)\n            triton_per_fused_div_pow_reduce_scatter_tensor_sqrt_sum_0.run(buf4, buf8, 1, 224, stream=stream0)\n            # Topologically Sorted Source Nodes: [sqrt, full_hidden, tensor_2], Original ATen: [aten.sqrt, aten.div, _c10d_functional.reduce_scatter_tensor]\n            buf9 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf8, \'avg\', 2, \'3\')\n            assert_size_stride(buf9, (16, 7), (7, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf9, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf4\n            # Topologically Sorted Source Nodes: [res_2], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf9)\n            del buf8\n            buf12 = empty_strided_xpu((16, 10), (10, 1), torch.float32)\n            # Topologically Sorted Source Nodes: [t_1, matmul_1], Original ATen: [aten.t, aten.mm]\n            extern_kernels.mm(buf9, reinterpret_tensor(arg2_1, (7, 10), (1, 7), 0), out=buf12)\n            del arg2_1\n            # Topologically Sorted Source Nodes: [tensor_3], Original ATen: [_c10d_functional.reduce_scatter_tensor]\n            buf13 = torch.ops._c10d_functional.reduce_scatter_tensor.default(buf12, \'avg\', 2, \'1\')\n            assert_size_stride(buf13, (8, 10), (10, 1), \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            assert_alignment(buf13, 16, \'torch.ops._c10d_functional.reduce_scatter_tensor.default\')\n            del buf9\n            # Topologically Sorted Source Nodes: [res_3], Original ATen: [_c10d_functional.wait_tensor]\n            torch.ops._c10d_functional.wait_tensor.default(buf13)\n            del buf12\n        return (buf13, )\n\nrunner = Runner(partitions=[])\ncall = runner.call\nrecursively_apply_fns = runner.recursively_apply_fns\n\n\ndef benchmark_compiled_module(times=10, repeat=10):\n    from torch._dynamo.testing import rand_strided\n    from torch._inductor.utils import print_performance\n    arg0_1 = rand_strided((8, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg1_1 = rand_strided((7, 10), (10, 1), device=\'xpu:0\', dtype=torch.float32)\n    arg2_1 = rand_strided((10, 7), (7, 1), device=\'xpu:0\', dtype=torch.float32)\n    fn = lambda: call([arg0_1, arg1_1, arg2_1])\n    return print_performance(fn, times=times, repeat=repeat)\n\n\nif __name__ == "__main__":\n    from torch._inductor.wrapper_benchmark import compiled_module_main\n    compiled_module_main(\'None\', benchmark_compiled_module)\n'

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_micro_pipeline_tp.py MicroPipelineTP4GPUTest.test_extra_collectives

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase></testsuite></testsuites>