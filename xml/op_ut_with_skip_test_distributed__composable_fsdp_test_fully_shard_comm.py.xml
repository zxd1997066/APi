<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="1" skipped="8" tests="20" time="352.552" timestamp="2025-08-30T12:08:51.419498" hostname="dut7358"><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCollectiveOps" name="test_all_gather_fp32" time="0.001"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:133: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCollectiveOps" name="test_reduce_scatter_fp16" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:237: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCollectiveOps" name="test_reduce_scatter_fp32" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:224: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCommunication" name="test_fully_shard_communication_count" time="33.180" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCommunication" name="test_manual_reshard_with_reshard_after_forward_false" time="31.852" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardCommunication" name="test_set_reshard_after_forward" time="32.239" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_backward_misprefetch" time="32.138" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_fully_shard_backward_prefetch" time="41.762" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_fully_shard_multi_module_backward_prefetch" time="32.252" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_fully_shard_multi_module_unused_module" time="31.852" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_set_modules_to_backward_prefetch" time="32.152" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardPrefetch" name="test_set_modules_to_forward_prefetch" time="31.950" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardUnshardMultiProcess" name="test_unshard_async" time="30.936"><failure message="RuntimeError: Process 0 exited with error code 10 and exception:&#10;Traceback (most recent call last):&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 864, in run_test&#10;    getattr(self, test_name)()&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 718, in wrapper&#10;    fn()&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py&quot;, line 3226, in wrapper&#10;    method(*args, **kwargs)&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 221, in wrapper&#10;    return func(*args, **kwargs)&#10;  File &quot;/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py&quot;, line 1334, in test_unshard_async&#10;    self.assertEqual(losses[0], losses[1])&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py&quot;, line 4181, in assertEqual&#10;    raise error_metas.pop()[0].to_error(  # type: ignore[index]&#10;AssertionError: Scalars are not close!&#10;&#10;Expected 2.766118049621582 but got 2.2723538875579834.&#10;Absolute difference: 0.49376416206359863 (up to 1e-05 allowed)&#10;Relative difference: 0.17850437082074205 (up to 1.3e-06 allowed)&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_comm.py TestFullyShardUnshardMultiProcess.test_unshard_async&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0&#10;&#10;Process 1 exited with error code 10 and exception:&#10;Traceback (most recent call last):&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 864, in run_test&#10;    getattr(self, test_name)()&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 718, in wrapper&#10;    fn()&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py&quot;, line 3226, in wrapper&#10;    method(*args, **kwargs)&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 221, in wrapper&#10;    return func(*args, **kwargs)&#10;  File &quot;/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py&quot;, line 1334, in test_unshard_async&#10;    self.assertEqual(losses[0], losses[1])&#10;  File &quot;/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py&quot;, line 4181, in assertEqual&#10;    raise error_metas.pop()[0].to_error(  # type: ignore[index]&#10;AssertionError: Scalars are not close!&#10;&#10;Expected 2.766118049621582 but got 2.2723538875579834.&#10;Absolute difference: 0.49376416206359863 (up to 1e-05 allowed)&#10;Relative difference: 0.17850437082074205 (up to 1.3e-06 allowed)&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_comm.py TestFullyShardUnshardMultiProcess.test_unshard_async&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1020, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 864, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 718, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3226, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 221, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py", line 1334, in test_unshard_async
    self.assertEqual(losses[0], losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 2.766118049621582 but got 2.2723538875579834.
Absolute difference: 0.49376416206359863 (up to 1e-05 allowed)
Relative difference: 0.17850437082074205 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_comm.py TestFullyShardUnshardMultiProcess.test_unshard_async

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0

Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 864, in run_test
    getattr(self, test_name)()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 718, in wrapper
    fn()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3226, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 221, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py", line 1334, in test_unshard_async
    self.assertEqual(losses[0], losses[1])
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not close!

Expected 2.766118049621582 but got 2.2723538875579834.
Absolute difference: 0.49376416206359863 (up to 1e-05 allowed)
Relative difference: 0.17850437082074205 (up to 1.3e-06 allowed)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/_composable/fsdp/test_fully_shard_comm.py TestFullyShardUnshardMultiProcess.test_unshard_async

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardUnshardMultiThread" name="test_unshard_no_param_group" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:1342: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardUnshardMultiThread" name="test_unshard_without_lazy_init" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:1353: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardAllocFromPG" name="test_exception_when_used_together_with_comm_hooks" time="15.826" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardAllocFromPG" name="test_fully_shard_alloc_from_pg" time="0.001"><skipped type="pytest.skip" message="multicast support is not available">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:1381: multicast support is not available</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardForceSumReduction" name="test_fully_shard_force_sum_both_reductions" time="0.000"><skipped type="pytest.skip" message="Related environment variable is not supported with XCCL">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:1511: Related environment variable is not supported with XCCL</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardForceSumReduction" name="test_fully_shard_force_sum_reduce_scatter" time="0.001"><skipped type="pytest.skip" message="Related environment variable is not supported with XCCL">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_comm.py:1458: Related environment variable is not supported with XCCL</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_comm.TestFullyShardReduceOpWorldSize1" name="test_size1_reduceop" time="4.108" /></testsuite></testsuites>