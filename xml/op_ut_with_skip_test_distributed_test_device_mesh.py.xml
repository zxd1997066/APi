<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="2" skipped="6" tests="52" time="697.500" timestamp="2025-08-22T12:26:09.853268" hostname="dut7358"><testcase classname="test.distributed.test_device_mesh.DeviceMeshSetDeviceTest" name="test_auto_set_device_from_heuristic" time="15.662" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshSetDeviceTest" name="test_auto_set_device_from_local_rank" time="16.334" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshSetDeviceTest" name="test_manual_set_device" time="16.036" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_2d_mesh_eager_init_subgroup" time="15.338" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_2d_mesh_non_eager_init_subgroup" time="16.248" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_assert_invalid_mesh_tensor" time="15.734" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_device_mesh_2d" time="16.235" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_device_mesh_init_backend" time="15.842" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_fake_pg_device_mesh" time="3.921" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_from_group_with_global_pg" time="15.739" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_from_group_with_invalid_mesh" time="15.534" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_get_group_and_get_all_groups" time="14.942" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_get_local_rank" time="15.943" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_get_local_rank_raises_exception" time="15.748" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_init_process_group" time="15.240" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_raises_invalid_device_type" time="3.822" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTest" name="test_set_mesh_dim_group_options" time="14.946" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_device_mesh_hash" time="15.546" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_device_mesh_nd" time="4.017"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:343: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_device_mesh_parent_child_hash" time="15.646" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_from_group_with_mesh_shape_2d" time="15.246" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_from_group_with_mesh_shape_3d" time="14.646" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshTestNDim" name="test_get_local_rank_3d" time="3.821"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:378: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_backend_override_argument_dict_with_idx_and_backend_eager" time="15.146" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_backend_override_argument_dict_with_idx_and_backend_lazy" time="15.951" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_backend_override_argument_dict_with_name_and_options" time="3.916" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_backend_override_argument_errors" time="15.848" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_init_device_mesh" time="15.046" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_raises_duplicate_mesh_dim_names" time="14.738" /><testcase classname="test.distributed.test_device_mesh.InitDeviceMeshTest" name="test_raises_mesh_shape_mesh_dim_names_mismatch" time="15.345" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_cache_and_reuse_submesh_slice_result" time="15.547" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_flatten_mesh_3d" time="15.347" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_flatten_mesh_4d" time="3.920"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:876: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_get_item_1d" time="15.947" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_get_item_2d" time="14.844" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_get_item_3d" time="4.020"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:752: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_get_item_3d_noncontiguous_slicing" time="3.918"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:804: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_raises_invalid_mesh_dim_name" time="15.752" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_raises_no_mesh_dim_found" time="15.748" /><testcase classname="test.distributed.test_device_mesh.TestDeviceMeshGetItem" name="test_reconstruct_mesh_with_flatten_dim" time="3.721"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/test_device_mesh.py:894: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_get_all_submeshes" time="15.646" /><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_get_mesh_dim_by_name" time="15.341" /><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_get_root_mesh" time="14.739" /><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_get_root_mesh_dim_exist" time="16.047" /><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_get_root_mesh_dim_not_exist" time="15.540" /><testcase classname="test.distributed.test_device_mesh.TestMeshEnv" name="test_mesh_slice_fake_tensor_mode" time="15.446" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_all_gather_uneven" time="15.347" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_broadcast_1d" time="15.245" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_broadcast_nd" time="4.718"><failure message="AssertionError: Scalars are not equal!&#10;&#10;Expected 0 but got -11.&#10;Absolute difference: 11&#10;Relative difference: inf&#10;Expected exit code 0 but got -11 for pid: 274680">Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1056, in _check_return_codes
    self.assertEqual(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not equal!

Expected 0 but got -11.
Absolute difference: 11
Relative difference: inf
Expected exit code 0 but got -11 for pid: 274680</failure></testcase><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_reduce_scatter_contiguous" time="16.337" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_reduce_scatter_uneven" time="28.354" /><testcase classname="test.distributed.test_device_mesh.DeviceMeshCollectiveTest" name="test_scatter_nd" time="4.917"><failure message="AssertionError: Scalars are not equal!&#10;&#10;Expected 0 but got -11.&#10;Absolute difference: 11&#10;Relative difference: inf&#10;Expected exit code 0 but got -11 for pid: 275601">Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1056, in _check_return_codes
    self.assertEqual(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 4181, in assertEqual
    raise error_metas.pop()[0].to_error(  # type: ignore[index]
AssertionError: Scalars are not equal!

Expected 0 but got -11.
Absolute difference: 11
Relative difference: inf
Expected exit code 0 but got -11 for pid: 275601</failure></testcase></testsuite></testsuites>