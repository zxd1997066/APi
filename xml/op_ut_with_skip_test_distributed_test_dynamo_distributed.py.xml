<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="2" skipped="50" tests="60" time="27.724" timestamp="2025-09-12T14:37:02.359594+00:00" hostname="dut7358"><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_call_method_forward" time="5.715" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_ddp_optimizer_inductor_strides_dont_specialize" time="1.312" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_hf_bert_ddp_aot_eager" time="2.170"><failure message="AssertionError: Torch not compiled with CUDA enabled&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_dynamo_distributed.py TestFakeDistributedSingleProc.test_hf_bert_ddp_aot_eager&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/mock.py", line 1379, in patched
    return func(*newargs, **newkeywargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py", line 351, in test_hf_bert_ddp_aot_eager
    model, inputs = get_hf_bert(0)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py", line 274, in get_hf_bert
    model = AutoModelForMaskedLM.from_config(config).to(device)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4459, in to
    return super().to(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/cuda/__init__.py", line 403, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_dynamo_distributed.py TestFakeDistributedSingleProc.test_hf_bert_ddp_aot_eager

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_hf_bert_ddp_inductor" time="0.909"><failure message="AssertionError: Torch not compiled with CUDA enabled&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_dynamo_distributed.py TestFakeDistributedSingleProc.test_hf_bert_ddp_inductor&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 59, in testPartExecutor
    yield
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3225, in wrapper
    method(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/unittest/mock.py", line 1379, in patched
    return func(*newargs, **newkeywargs)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py", line 345, in test_hf_bert_ddp_inductor
    model, inputs = get_hf_bert(0)
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py", line 274, in get_hf_bert
    model = AutoModelForMaskedLM.from_config(config).to(device)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4459, in to
    return super().to(*args, **kwargs)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
  File "/tmp/xpu-tool/Python/3.10.18/x64/lib/python3.10/site-packages/torch/cuda/__init__.py", line 403, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/test_dynamo_distributed.py TestFakeDistributedSingleProc.test_hf_bert_ddp_inductor

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_issue90375" time="0.292" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_symbol_splitting" time="1.245" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_unbacked_symbol_splitting_direct" time="1.053" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_unbacked_symbol_splitting_indirect" time="1.517" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_unbacked_symbol_splitting_no_binding" time="1.626" /><testcase classname="test.distributed.test_dynamo_distributed.TestFakeDistributedSingleProc" name="test_unbacked_symbol_splitting_torture_multi" time="0.742" /><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_asymmetric_compilation" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1200: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_asymmetric_compilation_with_fx_cache" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1251: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_automatic_dynamic_scalar" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:958: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_automatic_dynamic_speculation_divergence" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:986: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_automatic_dynamic_tensor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:913: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_dim_mismatch" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1048: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_graph_break_empty_graph_still_collective" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1016: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_missing_source" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1077: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_scalar_missing_source" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1099: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_compiler_collectives_type_mismatch" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1121: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_ddp_activation_checkpointing" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:614: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_ddp_baseline_aot_eager_multiprocess" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:563: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_ddp_optimizer_cudagraph" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:682: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_activation_checkpointing" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:797: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_aot_eager" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:658: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_inductor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:772: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_setattr" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:726: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_unspecialized_forced_getattr_inline" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:758: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_fsdp_unspecialized_forced_getattr_no_inline" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:743: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_get_pg_attr" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1180: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_guard_collective" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1155: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_ddp_aot_eager" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:602: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_ddp_aot_eager_static_graph" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:608: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_ddp_inductor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:580: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_ddp_inductor_static_graph" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:588: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_fsdp" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:819: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestMultiProc" name="test_hf_bert_fsdp_activation_checkpointing" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:864: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_aot_autograd" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1679: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_async_subclass_no_specialize" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:2049: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_compiled_flex_attention_full_model_ddp" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1433: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_compiled_flex_attention_local_ddp" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1489: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_custom_layer" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1696: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_ddp_baseline_aot_eager" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1330: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_ddp_baseline_inductor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1340: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_empty_graph_inductor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1717: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_dup_tensors_diff_source" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1958: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_dup_tensors_same_source" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1928: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_orig_params_assert" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1802: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_skip_guards" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1809: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_skip_register_attr_or_module" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1874: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_fsdp_staticmethod" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:2004: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1351: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split_ctx_manager" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1381: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split_inductor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1545: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split_inductor_layout_optimizations_inference" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1621: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split_inductor_layout_optimizations_training" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1615: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_graph_split_inductor_transpose" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1625: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_higher_order_op" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1762: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_ignored_parameters" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1731: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed.test_dynamo_distributed.TestSingleProc" name="test_no_split" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/test_dynamo_distributed.py:1660: c10d was not compiled with the NCCL backend</skipped></testcase></testsuite></testsuites>