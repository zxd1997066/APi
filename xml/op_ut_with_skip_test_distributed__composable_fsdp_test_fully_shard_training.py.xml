<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="0" skipped="12" tests="24" time="499.065" timestamp="2025-09-19T14:36:07.147860+00:00" hostname="dut7358"><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardForwardInputs" name="test_root_move_forward_input_to_device" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:70: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardRegisteredParams" name="test_param_registration_after_backward" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:164: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardRegisteredParams" name="test_param_registration_after_forward" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:103: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardCastAfterInit" name="test_to_float64_after_init" time="0.000"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:218: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_explicit_prefetching" time="32.853" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_multi_forward_module" time="31.957" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_non_root_forward_backward" time="0.000"><skipped type="pytest.skip" message="Sleep is not supported on XPU">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:499: Sleep is not supported on XPU</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_post_optim_event" time="0.000"><skipped type="pytest.skip" message="Sleep is not supported on HPU/XPU">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:630: Sleep is not supported on HPU/XPU</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_train_parity_multi_group" time="0.000"><skipped type="pytest.skip" message="Sleep kernel not supported for HPU/XPU">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:328: Sleep kernel not supported for HPU/XPU</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_train_parity_multi_group_cpu_offload_eager" time="0.000"><skipped type="pytest.skip" message="sleep kernel not supported on HPU/XPU">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:351: sleep kernel not supported on HPU/XPU</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_train_parity_multi_group_unshard_async_op" time="0.000"><skipped type="pytest.skip" message="sleep kernel not supported on HPU/XPU">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:375: sleep kernel not supported on HPU/XPU</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_train_parity_single_group_shard_dim0" time="31.655" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCore" name="test_train_parity_single_group_shard_largest_dim" time="31.639" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShard1DTrainingCompose" name="test_train_parity_with_activation_checkpointing" time="42.566"><skipped type="pytest.xfail" message="" /></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardShardPlacementFnMultiProcess" name="test_train_parity_shard_placement_fn_shard_largest_dim" time="32.058" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardShardPlacementFnMultiThread" name="test_shard_placement_fn_contiguous_params_grads" time="0.001"><skipped type="pytest.skip" message="not-support-multithread">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:845: not-support-multithread</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardSharedParams" name="test_train_parity_with_shared_params" time="33.058" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardGradientAccumulation" name="test_1f1b_microbatching" time="31.857" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardGradientAccumulation" name="test_gradient_accumulation" time="72.318" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardNDTraining" name="test_2d_mlp_with_nd_mesh" time="46.580" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardHSDP3DTraining" name="test_3d_mlp_with_nd_mesh" time="15.530"><skipped type="pytest.skip" message="Need at least 8 CUDA devices">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_composable/fsdp/test_fully_shard_training.py:1278: Need at least 8 CUDA devices</skipped></testcase><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardHSDPTraining" name="test_train_parity_hsdp" time="49.481" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardCustomForwardMethod" name="test_register_fsdp_forward_method" time="41.866" /><testcase classname="test.distributed._composable.fsdp.test_fully_shard_training.TestFullyShardWorldSize1" name="test_train_parity_single_worldsize1" time="3.707" /></testsuite></testsuites>