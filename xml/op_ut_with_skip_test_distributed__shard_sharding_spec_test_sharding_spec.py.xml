<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="0" skipped="5" tests="11" time="5.108" timestamp="2025-09-05T17:36:14.426471" hostname="dut7358"><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_check_overlapping" time="0.009" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_chunked_sharding_spec" time="0.000"><skipped type="pytest.skip" message="2 CUDA GPUs are needed">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharding_spec/test_sharding_spec.py:65: 2 CUDA GPUs are needed</skipped></testcase><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_device_placement" time="0.000"><skipped type="pytest.skip" message="2 CUDA GPUs are needed">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharding_spec/test_sharding_spec.py:45: 2 CUDA GPUs are needed</skipped></testcase><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_enumerable_sharding_spec" time="0.000"><skipped type="pytest.skip" message="2 CUDA GPUs are needed">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharding_spec/test_sharding_spec.py:98: 2 CUDA GPUs are needed</skipped></testcase><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_get_chunk_sharding_params" time="0.001" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_get_chunked_dim_size" time="0.001" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_get_split_size" time="0.001" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestShardingSpec" name="test_infer_sharding_spec_from_shards_metadata" time="0.005" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestCustomShardingSpec" name="test_custom_sharding_spec" time="3.009" /><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestCustomShardingSpec" name="test_custom_sharding_spec_shard_tensor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharding_spec/test_sharding_spec.py:595: c10d was not compiled with the NCCL backend</skipped></testcase><testcase classname="test.distributed._shard.sharding_spec.test_sharding_spec.TestCustomShardingSpec" name="test_custom_sharding_spec_tensor_ctor" time="0.000"><skipped type="pytest.skip" message="c10d was not compiled with the NCCL backend">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/_shard/sharding_spec/test_sharding_spec.py:568: c10d was not compiled with the NCCL backend</skipped></testcase></testsuite></testsuites>