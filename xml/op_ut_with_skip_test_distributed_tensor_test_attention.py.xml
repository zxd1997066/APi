<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="0" skipped="5" tests="6" time="2.861" timestamp="2025-08-30T11:15:34.831846" hostname="dut7358"><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_is_causal_behavior" time="0.010" /><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_custom_transformer" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:371: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_native_transformer" time="0.001"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:275: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_sdpa" time="0.000"><skipped type="pytest.skip" message="Does not support flash nor efficient attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:74: Does not support flash nor efficient attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingFlexAttentionTest" name="test_ring_flex_attention" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:679: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingFlexAttentionTest" name="test_ring_flex_attention_document_mask" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:698: Does not support flash attention</skipped></testcase></testsuite></testsuites>