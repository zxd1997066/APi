<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="0" failures="0" skipped="3" tests="4" time="2.217" timestamp="2025-09-19T13:20:24.879835+00:00" hostname="dut7358"><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_is_causal_behavior" time="0.009" /><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_sdpa" time="0.000"><skipped type="pytest.skip" message="Does not support flash nor efficient attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:71: Does not support flash nor efficient attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.CPFlexAttentionTest" name="test_cp_flex_attention" time="0.001"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:495: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.CPFlexAttentionTest" name="test_cp_flex_attention_document_mask" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:514: Does not support flash attention</skipped></testcase></testsuite></testsuites>