<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="1" skipped="4" tests="6" time="14.831" timestamp="2025-08-22T13:17:01.530737" hostname="dut7358"><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_is_causal_behavior" time="0.009" /><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_custom_transformer" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:371: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_native_transformer" time="0.001"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:275: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingAttentionTest" name="test_ring_attention_sdpa" time="0.000"><skipped type="pytest.skip" message="Does not support flash nor efficient attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:74: Does not support flash nor efficient attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingFlexAttentionTest" name="test_ring_flex_attention" time="0.000"><skipped type="pytest.skip" message="Does not support flash attention">/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py:679: Does not support flash attention</skipped></testcase><testcase classname="test.distributed.tensor.test_attention.RingFlexAttentionTest" name="test_ring_flex_attention_document_mask" time="11.424"><failure message="RuntimeError: Process 0 exited with error code 10 and exception:&#10;Traceback (most recent call last):&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 864, in run_test&#10;    getattr(self, test_name)()&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 718, in wrapper&#10;    fn()&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py&quot;, line 3226, in wrapper&#10;    method(*args, **kwargs)&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py&quot;, line 221, in wrapper&#10;    return func(*args, **kwargs)&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py&quot;, line 491, in wrapper&#10;    raise e&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py&quot;, line 488, in wrapper&#10;    func(self, *args, **kwargs)  # type: ignore[misc]&#10;  File &quot;/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py&quot;, line 742, in test_ring_flex_attention_document_mask&#10;    test_func()&#10;  File &quot;/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py&quot;, line 577, in _test_ring_flex_attention&#10;    expect_out, expect_lse = compiled_flex_attention(&#10;  File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py&quot;, line 813, in compile_wrapper&#10;    raise e.with_traceback(None) from e.__cause__  # User compiler error&#10;torch._dynamo.exc.Unsupported: Observed exception&#10;  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.&#10;  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.&#10;  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.&#10;&#10;  Developer debug context: raised exception ValueError([ConstantVariable(str: 'FlexAttention is only supported on CUDA, CPU or HPU devices. Found input tensors on xpu device.')])&#10;&#10; For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html&#10;&#10;from user code:&#10;   File &quot;/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/nn/attention/flex_attention.py&quot;, line 1466, in flex_attention&#10;    _validate_device(query, key, value)&#10;&#10;Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=&quot;+dynamo&quot;&#10;&#10;&#10;To execute this test, run the following from the base repo dir:&#10;    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/test_attention.py RingFlexAttentionTest.test_ring_flex_attention_document_mask&#10;&#10;This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0">Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 716, in wrapper
    self._join_processes(fn)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 980, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 1020, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 864, in run_test
    getattr(self, test_name)()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 718, in wrapper
    fn()
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py", line 3226, in wrapper
    method(*args, **kwargs)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py", line 221, in wrapper
    return func(*args, **kwargs)
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 491, in wrapper
    raise e
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/distributed/_tensor/common_dtensor.py", line 488, in wrapper
    func(self, *args, **kwargs)  # type: ignore[misc]
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py", line 742, in test_ring_flex_attention_document_mask
    test_func()
  File "/home/jenkins/actions-runner/_work/torch-xpu-ops/pytorch/test/distributed/tensor/test_attention.py", line 577, in _test_ring_flex_attention
    expect_out, expect_lse = compiled_flex_attention(
  File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 813, in compile_wrapper
    raise e.with_traceback(None) from e.__cause__  # User compiler error
torch._dynamo.exc.Unsupported: Observed exception
  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.
  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.
  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.

  Developer debug context: raised exception ValueError([ConstantVariable(str: 'FlexAttention is only supported on CUDA, CPU or HPU devices. Found input tensors on xpu device.')])

 For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html

from user code:
   File "/home/jenkins/.conda/envs/xpu_op_/lib/python3.10/site-packages/torch/nn/attention/flex_attention.py", line 1466, in flex_attention
    _validate_device(query, key, value)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/test_attention.py RingFlexAttentionTest.test_ring_flex_attention_document_mask

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0</failure></testcase></testsuite></testsuites>